{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE WITH NEW PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944cf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import initializers\n",
    "from keras.models import Model\n",
    "from sklearn.utils import resample\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046b0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniverseFull(df, asset_universe_dict, train_or_test):\n",
    "    # determine the asset universe to use for whether train or test data\n",
    "    if train_or_test == 'train':\n",
    "        index_start = 0\n",
    "        index_end   = len(asset_universe_dict)-4\n",
    "    elif train_or_test=='test':\n",
    "        index_start = len(asset_universe_dict)-4\n",
    "        index_end   = len(asset_universe_dict)\n",
    "    else:\n",
    "        assert(False),('get wit zee program')\n",
    "        \n",
    "    # subset to included assets\n",
    "    for i in range(index_start, index_end):\n",
    "        # extract this quarter and its included assets\n",
    "        date = list(asset_universe_dict.keys())[i]\n",
    "        assets = asset_universe_dict[date]\n",
    "\n",
    "        # form start and end date for this window\n",
    "        start_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "        end_date   = datetime.strptime(date, '%Y-%m-%d') + relativedelta(months=3)\n",
    "\n",
    "        # drop rows in this time period that are not the included assets\n",
    "        df = df[~(((df.index>=start_date) & (df.index<end_date)) & (~df.asset.isin(assets)))]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aeec626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week):\n",
    "    # Determine what quarter the oos_week is in\n",
    "    oos_mnth = pd.to_datetime(oos_week).month\n",
    "    oos_yr   = pd.to_datetime(oos_week).year\n",
    "    mnth_qtr = int(np.floor((oos_mnth-1)/3)*3+1)\n",
    "    if mnth_qtr == 10:\n",
    "        oos_qtr = str(oos_yr)+'-'+str(mnth_qtr)+'-01'\n",
    "    else:\n",
    "        oos_qtr = str(oos_yr)+'-0'+str(mnth_qtr)+'-01'\n",
    "\n",
    "    # Determine the asset universe\n",
    "    asset_universe = asset_universe_dict[oos_qtr]\n",
    "    \n",
    "    # Subset the training data to the asset universe\n",
    "    temp_df = temp_df[temp_df.asset.isin(asset_universe)]\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60956e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitFFNN(train_df, hps_yhats_dict, val_df=None, early_stopping=True):\n",
    "    # Obtain the covariates\n",
    "    column_names = list(df.columns.values)\n",
    "    column_names.remove('asset')\n",
    "    column_names.remove('r_tplus7')\n",
    "    covariates = column_names\n",
    "\n",
    "    # Extract the hyperparameters\n",
    "    number_hidden_layer = hps_yhats_dict['number_hidden_layer']\n",
    "    learning_rate       = hps_yhats_dict['learning_rate']\n",
    "    l1_penalty          = hps_yhats_dict['l1_penalty']\n",
    "    batch_size          = hps_yhats_dict['batch_size']\n",
    "    number_ensemble     = hps_yhats_dict['number_ensemble']\n",
    "    boot_strap_pct      = hps_yhats_dict['boot_strap_pct']\n",
    "\n",
    "    # Initialize the models\n",
    "    models = []\n",
    "\n",
    "    # Loop over the ensembles to build models for each\n",
    "    assert(number_ensemble <= 5)\n",
    "    n=round(boot_strap_pct*len(train_df))\n",
    "    for i in range(0, number_ensemble):\n",
    "        # Bootstrap the rows so different models in the ensemble are less correlated\n",
    "        train_df = resample(train_df, replace=True, n_samples=n, random_state=i)\n",
    "        \n",
    "        # Obtain the training input and output data and, if passed, validation data\n",
    "        train_rhs   = train_df[covariates]  \n",
    "        train_y     = train_df[['r_tplus7']]\n",
    "        if val_df is not None:\n",
    "            val_rhs     = val_df[covariates]  \n",
    "            val_y       = val_df[['r_tplus7']]\n",
    "        \n",
    "        # According to which model in the ensemble it is, initialize parameters.\n",
    "        if i==0:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.Constant(0.1)\n",
    "        elif i==1:\n",
    "            weight_initializer=initializers.GlorotUniform(seed=i)\n",
    "            bias_initializer=initializers.Constant(0.1)\n",
    "        elif i==2:\n",
    "            weight_initializer=initializers.RandomUniform(seed=i)\n",
    "            bias_initializer=initializers.Constant(0.1)\n",
    "        elif i==3:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.HeNormal(seed=i)\n",
    "        elif i==4:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.GlorotUniform(seed=i)\n",
    "        else:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.RandomUniform(seed=i)\n",
    "        \n",
    "        # Build the model\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.Input(shape=(train_rhs.shape[1],)))\n",
    "        model.add(Dense(6, activation='relu',\n",
    "                        kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                        kernel_initializer=weight_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "        model.add(BatchNormalization())\n",
    "        if number_hidden_layer >= 2:\n",
    "            model.add(Dense(5, activation='relu',\n",
    "                            kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                            kernel_initializer=weight_initializer,\n",
    "                            bias_initializer=bias_initializer))\n",
    "            model.add(BatchNormalization())\n",
    "        if number_hidden_layer == 3:\n",
    "            model.add(Dense(4, activation='relu',\n",
    "                            kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                            kernel_initializer=weight_initializer,\n",
    "                            bias_initializer=bias_initializer))\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='mean_squared_error',\n",
    "                      metrics=['mse'])\n",
    "\n",
    "        # Prepare early stopping object \n",
    "        es = EarlyStopping(monitor='val_mse', mode='min', verbose=2, patience=2)\n",
    "\n",
    "        # Fit the model\n",
    "        if early_stopping == True:\n",
    "            model.fit(x=train_rhs, y=train_y, \n",
    "                      batch_size=batch_size,\n",
    "                      validation_data=(val_rhs, val_y), \n",
    "                      epochs=5, verbose=1,\n",
    "                      workers=24, callbacks=[es])\n",
    "        else:\n",
    "            model.fit(x=train_rhs, y=train_y, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=5, verbose=1,\n",
    "                      workers=24)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983b742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genYhats(val_df, models):\n",
    "    # Build RHS \n",
    "    column_names = list(val_df.columns.values)\n",
    "    column_names.remove('asset')\n",
    "    column_names.remove('r_tplus7')\n",
    "    covariates   = column_names\n",
    "    oos_rhs      = val_df[covariates]  \n",
    "    \n",
    "    #Loop over each model and fit to RHS for all rows\n",
    "    yhats = []\n",
    "    for model in models:\n",
    "        yhats.append(model.predict(oos_rhs))\n",
    "        \n",
    "    # return the average of the predicted yhats\n",
    "    return(np.mean(yhats, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472f1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df, asset_universe_dict, last_train_year=2017, val_end_year=2020):\n",
    "    # Initialize hp result objects\n",
    "    hps_yhats_dict_list = []\n",
    "    hps_mse_df_list     = []\n",
    "    \n",
    "    # Initialize the hyperparameter grid  \n",
    "    batch_sizes          = [64, 128, 256] # note: ensure powers of 2 for eff\n",
    "    number_hidden_layers = [1, 3]\n",
    "    learning_rates       = [1e-3, 1e-2, 1e-1] \n",
    "    l1_penalties         = [1, 0.1, 0.01]\n",
    "    number_ensembles     = [5]\n",
    "    boot_strap_pcts      = [0.99]   \n",
    "\n",
    "    # Determine the weeks in the validation window  \n",
    "    val_weeks = np.unique(df[(df.index.year > last_train_year)  \n",
    "                             & (df.index.year <= val_end_year)].index.values)  \n",
    "\n",
    "    # Generate yhats for every hyperparameter grid point\n",
    "    for hps in itertools.product(batch_sizes,\n",
    "                                 number_hidden_layers,\n",
    "                                 learning_rates,\n",
    "                                 l1_penalties,\n",
    "                                 number_ensembles,\n",
    "                                 boot_strap_pcts):\n",
    "        hps_yhats_dict = {'batch_size': hps[0],\n",
    "                          'number_hidden_layer': hps[1],\n",
    "                          'learning_rate': hps[2],\n",
    "                          'l1_penalty': hps[3],\n",
    "                          'number_ensemble': hps[4],\n",
    "                          'boot_strap_pct':hps[5],\n",
    "                          'yhats': np.array([]),\n",
    "                          'ys': np.array([])}\n",
    "        print(hps_yhats_dict, '\\n')\n",
    "        tic = time.perf_counter()\n",
    "        for val_week in val_weeks:\n",
    "            print(val_week, '\\n')\n",
    "            train_df = df[df.index < val_week].copy()\n",
    "            train_df = subsetToAssetUniversePerWeek(train_df, asset_universe_dict, oos_week=val_week)\n",
    "            val_df   = df[df.index == val_week].copy()\n",
    "            val_df   = subsetToAssetUniversePerWeek(val_df, asset_universe_dict, oos_week=val_week)\n",
    "            \n",
    "            models = fitFFNN(train_df, hps_yhats_dict, val_df=val_df, early_stopping=True)\n",
    "            yhats  = genYhats(val_df, models)\n",
    "            ys     = val_df.r_tplus7.values\n",
    "\n",
    "            hps_yhats_dict['yhats'] = np.append(hps_yhats_dict['yhats'], yhats)\n",
    "            hps_yhats_dict['ys']    = np.append(hps_yhats_dict['ys'], ys)\n",
    "\n",
    "            val_ys_todate = hps_yhats_dict['ys'] \n",
    "            rw_val_mse    = np.mean(np.square(val_ys_todate))\n",
    "            model_val_mse = np.mean(np.square(val_ys_todate-hps_yhats_dict['yhats']))\n",
    "            print('\\n val random walk mse: ' + str(rw_val_mse))\n",
    "            print('\\n val model mse: ' + str(model_val_mse))\n",
    "            print('\\n val model mse winning?: ' + str(model_val_mse < rw_val_mse))\n",
    "            print('\\n\\n')\n",
    "        # Save run time and space out result print out\n",
    "        toc = time.perf_counter()\n",
    "        print('\\n\\n\\n')\n",
    "\n",
    "        # Update lists of results for each HP point\n",
    "        hps_yhats_dict_list.append(hps_yhats_dict)\n",
    "        \n",
    "        # Set up the CV results to save\n",
    "        cv_results_dict = hps_yhats_dict.copy()\n",
    "        del cv_results_dict['yhats']\n",
    "        del cv_results_dict['ys']\n",
    "        cv_results_dict['runtime_mins'] = round((toc - tic)/60, 0)\n",
    "        cv_results_dict['mse'] = model_val_mse\n",
    "        hps_mse_df_list.append(pd.DataFrame(cv_results_dict, index=[0]))\n",
    "\n",
    "        # Save to a CSV\n",
    "        cv_df = pd.concat(hps_mse_df_list, ignore_index=True)\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = '../4-output/cv-results-ffn-' + timestr + '.csv'\n",
    "        cv_df.to_csv(fp, index=False)\n",
    "\n",
    "    return hps_yhats_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f26af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelPortfolioWeights(df):\n",
    "    # assign tertiles\n",
    "    np.random.seed(42)\n",
    "    df['rand']    = np.random.uniform(size=df.shape[0])\n",
    "    df = df.sort_values(by=['date', 'yhat'])\n",
    "    df['ranking'] = df.groupby(['date']).cumcount()\n",
    "    df['counts']  = 1\n",
    "    df['total_assets_per_week'] = df.groupby('date').counts.transform('sum')\n",
    "    df['ranking']               = df.ranking/df.total_assets_per_week\n",
    "    df.loc[df.ranking < 1/3, 'prtfl_wght'] = 0\n",
    "    df.loc[(df.ranking>=1/3) & \n",
    "           (df.ranking<2/3), 'prtfl_wght'] = 1/6\n",
    "    df.loc[df.ranking>=2/3,  'prtfl_wght'] = 5/6\n",
    "    df['prtfl_wght'] = 3*df.prtfl_wght/df.total_assets_per_week\n",
    "    \n",
    "    # clean up\n",
    "    df = df.drop(['rand', 'ranking', 'counts',\n",
    "                  'total_assets_per_week'], axis=1)\n",
    "    \n",
    "    # confirm portfolio weights roughly sum to 1 for each week\n",
    "    assert(len(np.unique(df.index)) == \n",
    "           np.sum(np.isclose(df.groupby(['date']).prtfl_wght.sum(), 1,\n",
    "                             rtol=1e-2, atol=1e-2)))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96cf8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAnnualTransactionCosts(df):\n",
    "    # merge on the previous week's holdings to the new holdings\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df.week_idx<np.max(temp_df.week_idx)]\n",
    "    temp_df['week_idx'] = temp_df.week_idx+1\n",
    "    temp_df = temp_df[['week_idx', 'asset', 'prtfl_wght']]\n",
    "    temp_df = temp_df.rename(columns={'prtfl_wght': 'prtfl_wght_tm7'})\n",
    "    df = df.merge(temp_df,\n",
    "                  on=['week_idx', 'asset'],\n",
    "                  how='outer',\n",
    "                  validate='one_to_one')\n",
    "\n",
    "    # calc weekly turnover and ensure it has the appropriate range\n",
    "    df['asset_to'] = np.abs(df.prtfl_wght - df.prtfl_wght_tm7)\n",
    "    to_df = df.groupby('week_idx')[['asset_to']].sum().reset_index()\n",
    "    assert((np.min(to_df.asset_to)>=0) & (np.max(to_df.asset_to<=2)))\n",
    "\n",
    "    # correct the first and last week valid for buying the initial port and liquidating\n",
    "    to_df.loc[to_df.week_idx==106, 'asset_to'] = 1\n",
    "    to_df = pd.concat((to_df, pd.DataFrame(data={'week_idx': [262],\n",
    "                                                 'asset_to': 1})))\n",
    "    to_df = to_df.reset_index(drop=True)\n",
    "\n",
    "    # add transaction costs assuming maker and taker fee of 20 bps each\n",
    "    to_df['tc'] = to_df.asset_to*0.002\n",
    "\n",
    "    # return annualize transaction cost\n",
    "    return -np.sum(to_df.tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104893ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioReturn(r_df):\n",
    "    num_wks  = r_df.shape[0]\n",
    "    if np.sum(r_df.r.values <= -1)>=1:\n",
    "        return -1\n",
    "    else:\n",
    "        tot_ret  = np.product(r_df.r.values+1)-1\n",
    "        wkly_ret = (tot_ret+1)**(1/num_wks)-1\n",
    "        annl_ret = (wkly_ret+1)**(52.18)-1\n",
    "        return annl_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6275a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioSharpe(df):\n",
    "    wkly_sharpe = np.mean(df.r.values)/np.std(df.r.values)\n",
    "    annl_sharpe = wkly_sharpe*np.sqrt(52.18)\n",
    "    return annl_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc2464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_draw_down(df):\n",
    "    cumulative_ret=(df.r+1).cumprod()\n",
    "    roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "    daily_drawdown=cumulative_ret/roll_max\n",
    "    max_daily_drawdown=daily_drawdown.min() - 1\n",
    "    return max_daily_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d599db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_1_month_loss(df):\n",
    "    max_loss=(df['r']+1).rolling(4).apply(np.prod)\n",
    "    max_loss_minus=max_loss.min()-1\n",
    "    return max_loss_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b118f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenTestYhats(df, opt_hps, test_year=2021): \n",
    "    test_weeks = np.unique(df[df.index.year == test_year].index.values)\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "    for test_week in test_weeks:\n",
    "        print(test_week, '\\n')\n",
    "        temp_df  = df[df.index <= test_week].copy()\n",
    "        temp_df  = subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week=test_week)\n",
    "        train_df = temp_df[temp_df.index < test_week].copy()\n",
    "        oos_df   = temp_df[temp_df.index == test_week].copy()\n",
    "           \n",
    "        models = fitFFNN(train_df, opt_hps, val_df=None, early_stopping=False)\n",
    "        yhats  = genYhats(oos_df, models)\n",
    "        \n",
    "        oos_df = oos_df[['asset', 'r_tplus7']]\n",
    "        oos_df['yhat'] = yhats\n",
    "        test_df = pd.concat((test_df, oos_df))\n",
    "        rw_mse = np.mean(np.square(test_df.r_tplus7.values))\n",
    "        model_mse = np.mean(np.square(test_df.r_tplus7.values - test_df.yhat.values))\n",
    "        print('\\n test random walk mse: ' + str(rw_mse))\n",
    "        print('\\n test model mse: ' + str(model_mse))\n",
    "        print('winning?: ' + str(model_mse < rw_mse))\n",
    "        print('\\n')\n",
    "    \n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6e54502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean up data\n",
    "with open('../3-data/clean/asset_universe_dates_and_lists.pkl', 'rb') as handle:\n",
    "    asset_universe_dict = pickle.load(handle)\n",
    "input_fp = '../3-data/clean/panel_train.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "#df = df.drop('week_idx', axis=1)\n",
    "df = df.sort_values(by=['date', 'asset'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e810df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run CV\n",
    "hps_yhats_list = runCV(df, asset_universe_dict, last_train_year=2017, val_end_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form validation period results\n",
    "df = df[['asset', 'week_idx', 'r_tplus7']]\n",
    "df = subsetToAssetUniverseFull(df, asset_universe_dict, train_or_test='train')\n",
    "df = df[df.index.year >= 2018]\n",
    "df['yhat'] = hps_yhats_list[0]['yhats']\n",
    "df      = labelPortfolioWeights(df)\n",
    "annl_tc = calcAnnualTransactionCosts(df)\n",
    "df['r'] = df.prtfl_wght*df.r_tplus7\n",
    "r_df    = df.groupby(['date'])[['r']].sum()\n",
    "annl_ret = calcPortfolioReturn(r_df)\n",
    "annl_sharpe = calcPortfolioSharpe(r_df)\n",
    "max_dd = max_draw_down(r_df)\n",
    "max_1mo_loss = max_1_month_loss(r_df)\n",
    "print('annual transaction costs in simple return terms: ' + str(np.round(annl_tc, 4)))\n",
    "print('annual simple return before trans costs: ' + str(np.round(annl_ret, 4)))\n",
    "print('annual sharpe: '+str(np.round(annl_sharpe, 2)))\n",
    "print('max drawdown : '+str(np.round(max_dd, 2)))\n",
    "print('max one month loss : '+str(np.round(max_1mo_loss, 2)))\n",
    "\n",
    "# TODO SCOPE OLD NOTEBOOK FOR LATEST NUMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean up data for test fitting\n",
    "input_fp = '../3-data/clean/panel_oos.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "df = df.sort_values(by=['date', 'asset'])\n",
    "opt_hps = hps_yhats_list[0].copy()\n",
    "del opt_hps['yhats']\n",
    "del opt_hps['ys']\n",
    "test_df = GenTestYhats(df, opt_hps) \n",
    "test_df = test_df.merge(df[['asset', 'week_idx']],\n",
    "                        on=['date', 'asset'],\n",
    "                        how='inner', \n",
    "                        validate='one_to_one')\n",
    "\n",
    "# form test period results\n",
    "test_df = labelPortfolioWeights(test_df)\n",
    "annl_tc = calcAnnualTransactionCosts(test_df)\n",
    "test_df['r'] = test_df.prtfl_wght*test_df.r_tplus7\n",
    "r_df    = test_df.groupby(['date'])[['r']].sum()\n",
    "annl_ret = calcPortfolioReturn(r_df)\n",
    "annl_sharpe = calcPortfolioSharpe(r_df)\n",
    "max_dd = max_draw_down(r_df)\n",
    "max_1mo_loss = max_1_month_loss(r_df)\n",
    "print('annual transaction costs in simple return terms: ' + str(np.round(annl_tc, 4)))\n",
    "print('annual simple return before trans costs: ' + str(np.round(annl_ret, 4)))\n",
    "print('annual sharpe: '+str(np.round(annl_sharpe, 2)))\n",
    "print('max drawdown : '+str(np.round(max_dd, 2)))\n",
    "print('max one month loss : '+str(np.round(max_1mo_loss, 2)))\n",
    "\n",
    "# TODO SCOPE OLD NOTEBOOK FOR LATEST NUMBERS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
