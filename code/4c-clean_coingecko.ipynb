{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helper_functions import Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCrosswalk(cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Cleans a crosswalk DataFrame by ensuring that both asset IDs are unique, \n",
    "    sorting the columns, sorting by the asset name, and resetting the index.\n",
    "\n",
    "    Args:\n",
    "        cw_df: A pandas DataFrame representing the crosswalk.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the cleaned crosswalk.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If either of the asset IDs are not unique.\n",
    "    \"\"\"\n",
    "    # Ensure both asset ids are unique\n",
    "    assert cw_df['asset_cg'].is_unique, \"asset_cg must be unique\"\n",
    "\n",
    "    # Sort columns\n",
    "    cw_df = cw_df[['asset_cm', 'asset_cg']]\n",
    "\n",
    "    # Sort by cm asset name and reset index\n",
    "    cw_df = cw_df.sort_values(by='asset_cm').reset_index(drop=True)\n",
    "\n",
    "    return cw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPanel(df: pd.DataFrame, cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Fix columns with known issues\n",
    "    df = df.drop(columns='alexa_rank', axis=1)\n",
    "    df = df.rename(columns={'usd_volume_cg': 'usd_volume_24h_cg',\n",
    "                            'forks': 'github_forks',\n",
    "                            'stars': 'github_stars',\n",
    "                            'subscribers': 'github_subscribers',\n",
    "                            'total_issues': 'github_total_issues',\n",
    "                            'closed_issues': 'github_closed_issues',\n",
    "                            'pull_requests_merged': 'github_pull_requests_merged',\n",
    "                            'pull_requests_contributors': 'github_pull_requests_contributors',\n",
    "                            'code_additions_4_weeks': 'github_code_additions_4_weeks',\n",
    "                            'code_deletions_4_weeks': 'github_code_deletions_4_weeks',\n",
    "                            'commit_count_4_weeks': 'github_commit_count_4_weeks'})\n",
    "    df['github_code_deletions_4_weeks'] = np.abs(df['github_code_deletions_4_weeks'])\n",
    "    df['usd_mcap_cg'] = df.groupby('asset_cg')['usd_mcap_cg'].ffill()\n",
    "\n",
    "    # Convert all columns to float32\n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    cols.remove('asset_cg')\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # Convert date column to datetime format and remove timezone information\n",
    "    df['date'] = pd.to_datetime(df['date'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # Confirm no mising dates nor asset ids\n",
    "    assert 0 == df.date.isnull().sum()\n",
    "    assert 0 == df.asset_cg.isnull().sum()\n",
    "\n",
    "    # Confirm all asset ids are in cw\n",
    "    assert cw_df.asset_cg.is_unique\n",
    "    asset_ids = list(cw_df.asset_cg.values)\n",
    "    assert df.shape[0] == df[df.asset_cg.isin(asset_ids)].shape[0]\n",
    "\n",
    "    # Cut down to relevant dates between July 1, 2016 and January 2, 2023\n",
    "    df = df[(df['date'] >= '2016-07-01') & (df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # For price, volume, and mcap columns, ensure no missing obs and below thresholds\n",
    "    thresholds = {'usd_per_token_cg': 1e9, 'usd_volume_24h_cg': 1e11, 'usd_mcap_cg': 1e13}\n",
    "    for col, thresh in thresholds.items():\n",
    "        assert 0 == df[col].isnull().sum()\n",
    "        df = df[df[col] < thresh]\n",
    "\n",
    "    # For each asset_cg, replace missing values with zero if there's at least one non-missing observation in the column\n",
    "    for col in cols:\n",
    "        has_nonmissing = df.groupby('asset_cg')[col].transform(lambda x: x.notnull().any())\n",
    "        df.loc[has_nonmissing, col] = df.loc[has_nonmissing, col].fillna(0)\n",
    "\n",
    "    # Ensure no negative numbers\n",
    "    assert 0 == (df[cols]<0).sum().sum()\n",
    "\n",
    "    # Loop over all assets to add any missing days and fill missing price, volume, and mcap data\n",
    "    final_df = pd.DataFrame()\n",
    "    assets = list(np.unique(df.asset_cg.values))\n",
    "    for asset in assets:\n",
    "        # subset to asset of interest\n",
    "        asset_df = df[df.asset_cg==asset].copy()\n",
    "\n",
    "        # determine the date gaps\n",
    "        date_gaps = []\n",
    "        dates = asset_df.date.values\n",
    "        for i in range(1, len(dates)):\n",
    "            date_gaps.append(np.timedelta64(dates[i]-dates[i-1], 'D').astype(int))\n",
    "\n",
    "        # determine new days to add\n",
    "        indices_to_expand = [i for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32)]\n",
    "        num_datetime_to_add = [date_gaps[i] for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32)]\n",
    "        start_datetimes = dates[indices_to_expand]\n",
    "        new_datetimes = []\n",
    "        for i in range(len(start_datetimes)):\n",
    "            start_datetime = start_datetimes[i]\n",
    "            datetime_to_add = num_datetime_to_add[i]\n",
    "            for j in range(1, datetime_to_add):\n",
    "                new_datetimes.append(start_datetime+np.timedelta64(24*(j), 'h'))\n",
    "\n",
    "        # add the new days to the asset df\n",
    "        new_asset_df = pd.DataFrame(data={'date': new_datetimes})\n",
    "        new_asset_df['asset_cg'] = asset\n",
    "        asset_df = pd.concat((asset_df, new_asset_df))\n",
    "        asset_df = asset_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # forward fill \n",
    "        asset_df = asset_df.ffill()\n",
    "\n",
    "        # add data to master df\n",
    "        final_df = pd.concat((final_df, asset_df))\n",
    "\n",
    "    # reset df name\n",
    "    del df\n",
    "    df = final_df.copy()\n",
    "\n",
    "    # Set column order\n",
    "    cols.remove('usd_per_token_cg')\n",
    "    cols.remove('usd_volume_24h_cg')\n",
    "    cols.remove('usd_mcap_cg')\n",
    "    cols.sort()\n",
    "    df = df[['date', 'asset_cg', 'usd_per_token_cg', 'usd_mcap_cg', 'usd_volume_24h_cg']+cols]\n",
    "\n",
    "    # Ensure all columns have cg in name\n",
    "    df.columns = [col if col == 'date' or col.endswith('_cg') else col + '_cg' for col in df.columns]\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not df.duplicated(subset=['date', 'asset_cg']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    df = df.sort_values(by=['date', 'asset_cg']).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsePanel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a pandas DataFrame `df` and collapses multiple columns representing \n",
    "    GitHub and Reddit activity into single columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the columns to be collapsed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The modified DataFrame with the collapsed columns.\n",
    "    \"\"\"\n",
    "    # fill missing with zero\n",
    "    df = df.fillna(0) \n",
    "\n",
    "    # create single github activity column\n",
    "    cols_to_norm = ['github_closed_issues_cg',\n",
    "        'github_code_additions_4_weeks_cg', 'github_code_deletions_4_weeks_cg',\n",
    "        'github_commit_count_4_weeks_cg', 'github_forks_cg',\n",
    "        'github_pull_requests_merged_cg', 'github_stars_cg',\n",
    "        'github_subscribers_cg', 'github_total_issues_cg',\n",
    "        'pull_request_contributors_cg']\n",
    "    for col in cols_to_norm:\n",
    "        df = Helper.xsecNormalizeToMinusOneOne(df, col, 'asset_cg')\n",
    "    df['github_activity_cg'] = df[cols_to_norm].mean(axis=1)\n",
    "    df = Helper.xsecNormalizeToMinusOneOne(df, 'github_activity_cg', 'asset_cg')\n",
    "    df = df.drop(cols_to_norm, axis=1)\n",
    "\n",
    "    # create single reddit activity column\n",
    "    cols_to_norm = ['reddit_accounts_active_48h_cg', 'reddit_average_comments_48h_cg', \n",
    "        'reddit_average_posts_48h_cg', 'reddit_subscribers_cg']\n",
    "    for col in cols_to_norm:\n",
    "        df = Helper.xsecNormalizeToMinusOneOne(df, col, 'asset_cg')\n",
    "    df['reddit_activity_cg'] = df[cols_to_norm].mean(axis=1)\n",
    "    df = Helper.xsecNormalizeToMinusOneOne(df, 'reddit_activity_cg', 'asset_cg')\n",
    "    df = df.drop(cols_to_norm, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    CW_IN_FP = '../data/raw/coingecko_coinmetrics_cw.pkl'\n",
    "    PANEL_IN_FP = '../data/raw/coingecko_panel.pkl'\n",
    "    CW_OUT_FP = '../data/derived/cg_cm_cw.pkl'\n",
    "    PANEL_OUT_FP = '../data/derived/cg_panel.pkl'\n",
    "\n",
    "    # import data\n",
    "    cw_df = pd.read_pickle(CW_IN_FP)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # clean data\n",
    "    cw_df = cleanCrosswalk(cw_df)\n",
    "    df = cleanPanel(df, cw_df)\n",
    "\n",
    "    # collapse columns of panel\n",
    "    df = collapsePanel(df)\n",
    "\n",
    "    # save data\n",
    "    cw_df.to_pickle(CW_OUT_FP)\n",
    "    df.to_pickle(PANEL_OUT_FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
