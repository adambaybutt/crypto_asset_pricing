{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE WITH NEW PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab026fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 11:47:26.797746: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba13f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week):\n",
    "    # Determine what quarter the oos_week is in\n",
    "    oos_mnth = pd.to_datetime(oos_week).month\n",
    "    oos_yr   = pd.to_datetime(oos_week).year\n",
    "    mnth_qtr = int(np.floor((oos_mnth-1)/3)*3+1)\n",
    "    if mnth_qtr == 10:\n",
    "        oos_qtr = str(oos_yr)+'-'+str(mnth_qtr)+'-01'\n",
    "    else:\n",
    "        oos_qtr = str(oos_yr)+'-0'+str(mnth_qtr)+'-01'\n",
    "\n",
    "    # Determine the asset universe\n",
    "    asset_universe = asset_universe_dict[oos_qtr]\n",
    "    \n",
    "    # Subset the training data to the asset universe\n",
    "    temp_df = temp_df[temp_df.asset.isin(asset_universe)]\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec103cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniverseFull(df, asset_universe_dict, train_or_test):\n",
    "    # determine the asset universe to use for whether train or test data\n",
    "    if train_or_test == 'train':\n",
    "        index_start = 0\n",
    "        index_end   = len(asset_universe_dict)-4\n",
    "    elif train_or_test=='test':\n",
    "        index_start = len(asset_universe_dict)-4\n",
    "        index_end   = len(asset_universe_dict)\n",
    "    else:\n",
    "        assert(False),('get wit zee program')\n",
    "        \n",
    "    # subset to included assets\n",
    "    for i in range(index_start, index_end):\n",
    "        # extract this quarter and its included assets\n",
    "        date = list(asset_universe_dict.keys())[i]\n",
    "        assets = asset_universe_dict[date]\n",
    "\n",
    "        # form start and end date for this window\n",
    "        start_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "        end_date   = datetime.strptime(date, '%Y-%m-%d') + relativedelta(months=3)\n",
    "\n",
    "        # drop rows in this time period that are not the included assets\n",
    "        df = df[~(((df.index>=start_date) & (df.index<end_date)) & (~df.asset.isin(assets)))]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094ff685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPlatonicPortfolioWeights(df, min_weight, lhs_col):\n",
    "    # sorts assets by return within date and rank\n",
    "    df = df.sort_values(by=['date', lhs_col])\n",
    "    df['ranking'] = df.groupby(['date']).cumcount()\n",
    "    df['counts']  = 1\n",
    "    df['assets_per_date'] = df.groupby(['date']).counts.sum()\n",
    "    df['ranking'] = df.ranking/df.assets_per_date\n",
    "\n",
    "    # determine number of assets per date\n",
    "    N = df[df.index==np.min(df.index)].shape[0]\n",
    "\n",
    "    # create min (difference) portfolio weights\n",
    "    # -about 51% in top half and 49% in bottom half earns \n",
    "    #  roughly a 10 basis prem in val data over the naive weights\n",
    "#     df.loc[df.ranking<0.5, 'y_min']  = 0.49/(N/2)\n",
    "#     df.loc[df.ranking>=0.5, 'y_min'] = 0.51/(N/2)\n",
    "\n",
    "    # create lin portfolio weights\n",
    "    # -determine delta in portfolio weights to increase linearly from min weight, s.t. sum to 1 each week\n",
    "    # --solves for weight_delta==w_d where min_weight==m_w and N=assets_per_week,\n",
    "    #   in 1=m_w+(min_w+w_d)+...+(m_w+(N-1)*w_d)\n",
    "#     df['weight_delta'] = 2*(1-df.assets_per_date*min_weight)/(df.assets_per_date*(df.assets_per_date-1))\n",
    "#     df['y_lin'] = df.groupby(['date'])['weight_delta'].cumsum()\n",
    "#     df['y_lin'] = df.y_lin - df.weight_delta + min_weight\n",
    "\n",
    "    # create exp portfolio weights\n",
    "#     def softmax(x):\n",
    "#         e = np.exp(x)\n",
    "#         return e / np.sum(e)\n",
    "#     # keep trying a set of weights such that we work\n",
    "#     #  down to the set where the min weight is just above min_weight\n",
    "#     for i in range(1,N):\n",
    "#         y_exp = softmax(np.array(range(N))/N*i)\n",
    "#         if y_exp[0]<min_weight:\n",
    "#             break\n",
    "#     y_exp = softmax(np.array(range(N))/N*(i-1))\n",
    "#     shift_amount = y_exp[0]-min_weight\n",
    "#     half_index   = int(len(y_exp)/2)\n",
    "#     y_exp[:half_index] = y_exp[:half_index]-shift_amount\n",
    "#     y_exp[half_index:] = y_exp[half_index:]+shift_amount\n",
    "#     assert(y_exp[0]>=min_weight)\n",
    "#     num_weeks = len(np.unique(df.index.values))\n",
    "#     df['y_exp'] = np.tile(y_exp, num_weeks)\n",
    "\n",
    "    # assign max (difference) portfolio weights\n",
    "#     df.loc[df.ranking<0.5, 'y_max'] = min_weight\n",
    "#     df.loc[df.ranking>=0.5, 'y_max'] = (1-min_weight*(N/2))/(N/2)\n",
    "    \n",
    "    # create agg(ressive) portfolio weights\n",
    "#     df['y_agg'] = 0\n",
    "#     df.loc[df.ranking==np.max(df.ranking), 'y_agg'] = 1\n",
    "    \n",
    "    # assign portfolio weights with shorting\n",
    "    df.loc[df[lhs_col] < 0, 'assets_neg_per_date'] = 1\n",
    "    df['assets_neg_per_date'] = df.groupby('date').assets_neg_per_date.transform('sum')\n",
    "    df.loc[(df.assets_neg_per_date == 1) & (df.ranking==np.min(df.ranking)), 'y_short'] = -1\n",
    "    df.loc[(df.assets_neg_per_date == 1) & (df.ranking==np.min(df.ranking)), 'y_short'] = -1\n",
    "    df.loc[(df.assets_neg_per_date == 1) & (df.ranking==np.max(df.ranking)), 'y_short'] = 1\n",
    "    df.loc[(df.assets_neg_per_date == 1) & (df.ranking==np.min(df.ranking)) \n",
    "                                         & (df.r_tplus1>-0.005), 'y_short'] = 0\n",
    "    df.loc[(df.assets_neg_per_date == 1) & (df.ranking==np.max(df.ranking)) \n",
    "                                         & (df.r_tplus1<0.005), 'y_short'] = 0\n",
    "    df.loc[(df.assets_neg_per_date==2), 'y_short'] = -1\n",
    "    df.loc[(df.assets_neg_per_date==2)&(df.r_tplus1>-.01)&(df.y_short==-1), 'y_short'] = 0\n",
    "    df.loc[(df.assets_neg_per_date == 0), 'y_short'] = 1\n",
    "    df.loc[(df.assets_neg_per_date == 0) & (df.r_tplus1<0.005), 'y_short'] = 0\n",
    "    df = df.drop('assets_neg_per_date', axis=1)\n",
    "    \n",
    "    # confirm weights are equal to one each date for each type of LHS var\n",
    "#     for lhs_y in ['y_min', 'y_lin', 'y_exp', 'y_max', 'y_agg']:\n",
    "#         assert(len(np.unique(df.index)) == \n",
    "#                np.sum(np.isclose(df.groupby(['date'])[lhs_y].sum(), 1,\n",
    "#                                  rtol=1e-2, atol=1e-2)))\n",
    "    \n",
    "    # clean up columns\n",
    "    #df = df.drop(['ranking', 'counts', 'assets_per_week', 'weight_delta'],\n",
    "    #              axis=1)\n",
    "    df = df.drop(['ranking', 'counts', 'assets_per_date'], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9cafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenToOneObPerDate(df, lhs_col):\n",
    "    # initialize df to return\n",
    "    final_df = pd.DataFrame(data={'date': np.unique(df.index)})\n",
    "    final_df = final_df.set_index('date')\n",
    "\n",
    "    # for each asset, update column names\n",
    "    assets = list(np.unique(df.asset))\n",
    "    for asset in assets:\n",
    "        # subset to relavant rows\n",
    "        temp_df = df[df.asset == asset]\n",
    "\n",
    "        # drop asset column and macro columns\n",
    "        temp_df = temp_df.drop('asset', axis=1)\n",
    "        macro_cols = list(temp_df.filter(regex='macro_', axis=1).columns.values)\n",
    "        temp_df = temp_df.drop(macro_cols, axis=1)\n",
    "\n",
    "        # form lhs variables\n",
    "        temp_df = temp_df.rename(columns={'y_short': 'y_short_'+asset,\n",
    "                                          lhs_col: lhs_col+'_'+asset})\n",
    "\n",
    "        # clean up covar columns\n",
    "        covar_cols = list(temp_df.filter(regex='covar_', axis=1).columns.values)\n",
    "        for covar_col in covar_cols:\n",
    "            temp_df = temp_df.rename(columns={covar_col: covar_col+'_'+asset})\n",
    "        final_df = final_df.merge(temp_df,\n",
    "                                  on='date',\n",
    "                                  how='inner',\n",
    "                                  validate='one_to_one')\n",
    "\n",
    "    # merge on macro columns to final df\n",
    "    macro_cols = list(df.filter(regex='macro_', axis=1).columns.values)\n",
    "    macro_df = df[macro_cols]\n",
    "    macro_df = macro_df.drop_duplicates()\n",
    "    final_df = final_df.merge(macro_df,\n",
    "                              on='date',\n",
    "                              how='inner',\n",
    "                              validate='one_to_one')\n",
    "\n",
    "    # reset column order\n",
    "    all_cols      = list(final_df.columns.values)\n",
    "    y_cols        = [col for col in all_cols if col[:2] == 'y_']\n",
    "    r_tplus_cols  = [col for col in all_cols if col[:8] == lhs_col]\n",
    "    covar_cols    = [col for col in all_cols if col[:6] == 'covar_']\n",
    "    final_df      = final_df[y_cols+r_tplus_cols+macro_cols+covar_cols]\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da7ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(lmbd):\n",
    "    def loss(y_true, y_pred):\n",
    "        naive_weights    = tf.repeat(0.05, 20) # note: manually programmed 20 assets\n",
    "        weeks_neg_return = -tf.tensordot(y_true, tf.transpose(y_pred), axes=1)\n",
    "        dist_from_naive  = tf.math.reduce_sum(tf.square(naive_weights - y_pred))\n",
    "        return float(weeks_neg_return) + lmbd*float(dist_from_naive)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e2febb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFFNN(i, train_rhs, train_lhs,\n",
    "              first_layer_width, leaky_relu, \n",
    "              l1_penalty, apply_dropout, dropout_rate,\n",
    "              number_hidden_layer, learning_rate,\n",
    "              loss, lmbd,\n",
    "              weight_initializer, bias_initializer):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(train_rhs.shape[1],)))\n",
    "    model.add(Dense(first_layer_width, activation='gelu',\n",
    "                    kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                    kernel_initializer=weight_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "    model.add(Dropout(dropout_rate, seed=i)) \n",
    "    model.add(BatchNormalization())\n",
    "        \n",
    "    if number_hidden_layer >= 2:\n",
    "        model.add(Dense(64, activation='gelu',\n",
    "                        kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                        kernel_initializer=weight_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "        model.add(Dropout(dropout_rate, seed=i))\n",
    "        model.add(BatchNormalization())\n",
    "         \n",
    "            \n",
    "    if number_hidden_layer >= 3:\n",
    "        model.add(Dense(32, activation='gelu',\n",
    "                        kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                        kernel_initializer=weight_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "        model.add(Dropout(dropout_rate, seed=i)) \n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "            \n",
    "    if number_hidden_layer >= 4:\n",
    "        model.add(Dense(16, activation='gelu',\n",
    "                        kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                        kernel_initializer=weight_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "        model.add(Dropout(dropout_rate, seed=i)) \n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "            \n",
    "    if number_hidden_layer == 5:\n",
    "        model.add(Dense(8, activation='gelu',\n",
    "                        kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                        kernel_initializer=weight_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "        model.add(Dropout(dropout_rate, seed=i)) \n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "            \n",
    "    model.add(Dense(train_lhs.shape[1], activation=\"linear\"))\n",
    "    #model.add(Softmax())\n",
    "    \n",
    "    # Compile the model\n",
    "    if loss=='mse':\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='mse', metrics=['mse', 'mae'])\n",
    "    if loss=='custom':\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss=customLoss(lmbd=lmbd))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4134e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: FINISH THIS SKETCHED OUT CODE\n",
    "# TODO: CHECK I USE ALL ARGUMENTS\n",
    "def buildTransformer(i, train_rhs, train_lhs,\n",
    "                     first_layer_width, l1_penalty, dropout_rate,\n",
    "                     number_hidden_layer, learning_rate, loss, lmbd,\n",
    "                     weight_initializer, bias_initializer,\n",
    "                     dim_k, num_heads):\n",
    "\n",
    "    # set up input\n",
    "    input_dim  = train_rhs.shape[1]\n",
    "    output_dim = train_lhs.shape[1]\n",
    "    input_vec  = tf.keras.Input(shape=input_dim)\n",
    "\n",
    "    # build Q, K, V\n",
    "    q   = Dense(dim_k, activation='linear',\n",
    "                kernel_initializer=weight_initializer,\n",
    "                use_bias=False)(input_vec)\n",
    "    k   = Dense(dim_k, activation='linear',\n",
    "                kernel_initializer=weight_initializer,\n",
    "                use_bias=False)(input_vec)\n",
    "    v   = Dense(dim_k, activation='linear',\n",
    "                kernel_initializer=weight_initializer,\n",
    "                use_bias=False)(input_vec)\n",
    "\n",
    "    # build multi-head attention\n",
    "    attn = MultiHeadAttention(num_heads=num_heads,\n",
    "                              dropout=dropout_rate,\n",
    "                              output_shape=input_dim,\n",
    "                              kernel_initializer=weight_initializer,\n",
    "                              bias_initializer=bias_initializer,\n",
    "                              kernel_regularizer=regularizers.l1(l1=l1_penalty))(q=q, k=k, v=v)\n",
    "    attn = Dropout(dropout_rate, seed=i)(attn)\n",
    "    attn = input_vec + attn\n",
    "    attn = BatchNormalization()(attn)\n",
    "\n",
    "    # build ffnn\n",
    "    x  = Dense(first_layer_width, activation='gelu',\n",
    "              kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "              kernel_initializer=weight_initializer,\n",
    "              bias_initializer=bias_initializer)(x)\n",
    "    x  = Dropout(dropout_rate, seed=i)(x)\n",
    "    x  = BatchNormalization()(x)\n",
    "\n",
    "    if number_hidden_layer >= 2:\n",
    "        x = Dense(64, activation='gelu',\n",
    "                  kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                  kernel_initializer=weight_initializer,\n",
    "                  bias_initializer=bias_initializer)(x)\n",
    "        x = Dropout(dropout_rate, seed=i)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    if number_hidden_layer >= 3:\n",
    "        x = Dense(32, activation='gelu',\n",
    "                  kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                  kernel_initializer=weight_initializer,\n",
    "                  bias_initializer=bias_initializer)(x)\n",
    "        x = Dropout(dropout_rate, seed=i)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    if number_hidden_layer >= 4:\n",
    "        x = Dense(16, activation='gelu',\n",
    "                  kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                  kernel_initializer=weight_initializer,\n",
    "                  bias_initializer=bias_initializer)(x)\n",
    "        x = Dropout(dropout_rate, seed=i)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    if number_hidden_layer == 5:\n",
    "        x = Dense(8, activation='gelu',\n",
    "                  kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                  kernel_initializer=weight_initializer,\n",
    "                  bias_initializer=bias_initializer)(x)\n",
    "        x = Dropout(dropout_rate, seed=i)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "    # form final layer to map to weights\n",
    "    x = Dense(output_dim, activation='linear')(x)\n",
    "    output_vec = Softmax()(x)        \n",
    "    model = Model(input_vec, output_vec)\n",
    "\n",
    "    # Compile the model\n",
    "    if loss=='mse':\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='mse', metrics=['mse', 'mae'])\n",
    "    if loss=='custom':\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss=customLoss(lmbd=lmbd))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb6eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitNN(train_df, hps_yhats_dict, lhs_col, val_df=None):\n",
    "    # Extract the hyperparameters\n",
    "    first_layer_width   = hps_yhats_dict['first_layer_width']\n",
    "    number_hidden_layer = hps_yhats_dict['number_hidden_layer']\n",
    "    learning_rate       = hps_yhats_dict['learning_rate']\n",
    "    batch_size          = hps_yhats_dict['batch_size']\n",
    "    l1_penalty          = hps_yhats_dict['l1_penalty']\n",
    "    number_ensemble     = hps_yhats_dict['number_ensemble']\n",
    "    epoch               = hps_yhats_dict['epoch']\n",
    "    early_stopping      = hps_yhats_dict['early_stopping']\n",
    "    bootstrap_pct       = hps_yhats_dict['bootstrap_pct']\n",
    "    dropout_rate        = hps_yhats_dict['dropout_rate']\n",
    "    apply_dropout       = hps_yhats_dict['apply_dropout']\n",
    "    leaky_relu          = hps_yhats_dict['leaky_relu']\n",
    "    patience            = hps_yhats_dict['patience']\n",
    "    loss                = hps_yhats_dict['loss']\n",
    "    lmbd                = hps_yhats_dict['lmbd']\n",
    "    bias_initial        = hps_yhats_dict['bias_initial']\n",
    "    weight_initial      = hps_yhats_dict['weight_initial']\n",
    "    target_mse          = hps_yhats_dict['target_mse']\n",
    "\n",
    "    # Obtain the lhs and rhs column names\n",
    "    all_cols      = list(train_df.columns.values)\n",
    "    y_cols        = [col for col in all_cols if col[:2] == 'y_']\n",
    "    r_tplus_cols  = [col for col in all_cols if col[:8] == lhs_col]\n",
    "    covar_cols    = [col for col in all_cols if col[:6] == 'covar_']\n",
    "    macro_cols    = [col for col in all_cols if col[:6] == 'macro_']\n",
    "    rhs_cols      = covar_cols+macro_cols\n",
    "    if loss=='mse':\n",
    "        lhs_cols = y_cols\n",
    "    if loss=='custom':\n",
    "        lhs_cols = r_tplus_cols\n",
    "        \n",
    "    # Loop over the ensembles to a build model for each\n",
    "    assert(number_ensemble <= 5)\n",
    "    models = []\n",
    "    for i in range(number_ensemble):\n",
    "        # Bootstrap the rows so different models in the ensemble are less correlated\n",
    "        model_train_df = resample(train_df, replace=False, \n",
    "                                  n_samples=int(train_df.shape[0]*bootstrap_pct),\n",
    "                                  random_state=i)\n",
    "\n",
    "        # Obtain the training input and output data and, if passed, validation data\n",
    "        train_rhs   = model_train_df[rhs_cols].values\n",
    "        train_lhs   = model_train_df[lhs_cols].values\n",
    "        if val_df is not None:\n",
    "            val_rhs = val_df[rhs_cols].values  \n",
    "            val_lhs = val_df[lhs_cols].values\n",
    "\n",
    "        # Initialize parameters, but with different values across ensembled models\n",
    "        if bias_initial=='constant':\n",
    "            bias_initializer=initializers.Constant(0.01)\n",
    "        if bias_initial=='he':\n",
    "            bias_initializer=initializers.HeNormal(seed=i)\n",
    "        if bias_initial=='glorot':\n",
    "            bias_initializer=initializers.GlorotUniform(seed=i)\n",
    "        if bias_initial=='uniform':\n",
    "            bias_initializer=initializers.RandomUniform(seed=i)\n",
    "        if weight_initial=='he':\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "        if weight_initial=='glorot':\n",
    "            weight_initializer=initializers.GlorotUniform(seed=i)\n",
    "        if weight_initial=='uniform':\n",
    "            weight_initializer=initializers.RandomUniform(seed=i)\n",
    "        \n",
    "        # Build the model\n",
    "        model = buildFFNN(i, train_rhs, train_lhs,\n",
    "                          first_layer_width, leaky_relu, \n",
    "                          l1_penalty, apply_dropout, dropout_rate,\n",
    "                          number_hidden_layer, learning_rate,\n",
    "                          loss, lmbd,\n",
    "                          weight_initializer, bias_initializer)\n",
    "        \n",
    "        # Set up early stopping\n",
    "        es = EarlyStopping(monitor='mse', mode='min', verbose=1, \n",
    "                           patience=patience, min_delta=target_mse*0.001)\n",
    "\n",
    "        # Fit the model\n",
    "        with tf.device('/CPU:0'):\n",
    "            model.fit(x=train_rhs, y=train_lhs, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epoch, verbose=1,\n",
    "                      workers=10, callbacks=[es])\n",
    "           \n",
    "        # Save the model\n",
    "        models.append(model)\n",
    "        print('\\n')\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb888556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genYhats(oos_df, models, lhs_col):\n",
    "    # Obtain the lhs and rhs column names\n",
    "    all_cols      = list(oos_df.columns.values)\n",
    "    y_cols        = [col for col in all_cols if col[:2] == 'y_']\n",
    "    r_tplus_cols  = [col for col in all_cols if col[:8] == lhs_col]\n",
    "    covar_cols    = [col for col in all_cols if col[:6] == 'covar_']\n",
    "    macro_cols    = [col for col in all_cols if col[:6] == 'macro_']\n",
    "    rhs_cols      = covar_cols+macro_cols\n",
    "    \n",
    "    # Obtain the RHS data\n",
    "    oos_rhs   = oos_df[rhs_cols].values  \n",
    "    oos_lhs   = oos_df[y_cols].values\n",
    "    \n",
    "    # Form the yhats\n",
    "    yhats = []\n",
    "    for model in models:\n",
    "        with tf.device('/CPU:0'):\n",
    "            yhats.append(model.predict(oos_rhs))\n",
    "    yhats = np.mean(yhats, axis=0)\n",
    "\n",
    "    # Return results\n",
    "    return pd.DataFrame(data={'date': oos_df.index.values[0],\n",
    "                              lhs_col: list(oos_df[r_tplus_cols].values),\n",
    "                              'ys':    list(oos_lhs),\n",
    "                              'yhats': list(yhats)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44720f45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runCV(df, asset_universe_dict, lhs_col,\n",
    "          last_train_year=2018, val_end_year=2020, arch_name=None):\n",
    "    # Initialize hp result objects\n",
    "    results_list    = []\n",
    "    hps_mse_df_list = []\n",
    "    \n",
    "    # Initialize the hyperparameter grid  \n",
    "    first_layer_widths   = [256] \n",
    "    number_hidden_layers = [5]\n",
    "    learning_rates       = [1e-4] # TODO NARROW TO CLOSER RANGE\n",
    "    batch_sizes          = [128]  # explore once arch is dialed in [64, 128, 256]\n",
    "    l1_penalties         = [0.001] # Doesnt seem to do much; maybe explore once dialed in  [0.001, 0.01, 0.1, 1]\n",
    "    number_ensembles     = [3] # Explore cranking up once dialed in, 7, 9 or way more\n",
    "    epochs               = [200] # explore cranking it up once more dialed in [100, 200, 500]\n",
    "    # Explore cutting down once dialed in as never seem to use more than 100\n",
    "    # or figure out a way to turn learning way down so we can turn epoch up\n",
    "    early_stoppings      = [True] \n",
    "    bootstrap_pcts       = [0.99] # explore more once arch is dialed in [0.9, 0.95, 0.99]\n",
    "    dropout_rates        = [0.1] # explore more once arch is dialed in [0.01, 0.1, 0.2, 0.3, 0.4]\n",
    "    apply_dropouts       = [True] \n",
    "    leaky_relus          = [0]          # explore once arch is dialed in [0, 0.01, 0.1]\n",
    "    patiences            = [16]          # explore more once  arch is dialed in [20, 50, 100]\n",
    "    losses               = ['mse']      # explore custom once arch is dialed in\n",
    "    lambdas              = [0]          # explore custom once arch is dialed in [0, 0.01, 0.1]\n",
    "    bias_initials        = ['he']       # once architecture is dialed, circle back to playing with these\n",
    "    weight_initials      = ['uniform']  # once architecture is dialed, circle back to playing with these\n",
    "    lhs_vars             = ['short']      # once architecture is dialed, circle back to playing with these\n",
    "    \n",
    "    # Determine the dates in the validation window  \n",
    "    val_dates = np.unique(df[(df.index.year > last_train_year)  \n",
    "                             & (df.index.year <= val_end_year)].index.values)  \n",
    "\n",
    "    # Generate yhats for every hyperparameter grid point\n",
    "    for hps in itertools.product(first_layer_widths,\n",
    "                                 number_hidden_layers,\n",
    "                                 learning_rates,\n",
    "                                 batch_sizes,\n",
    "                                 l1_penalties,\n",
    "                                 number_ensembles,\n",
    "                                 epochs,\n",
    "                                 early_stoppings,\n",
    "                                 bootstrap_pcts,\n",
    "                                 dropout_rates,\n",
    "                                 apply_dropouts,\n",
    "                                 leaky_relus,\n",
    "                                 patiences,\n",
    "                                 losses,\n",
    "                                 lambdas,\n",
    "                                 bias_initials,\n",
    "                                 weight_initials,\n",
    "                                 lhs_vars):\n",
    "\n",
    "        # don't repeat all weighted custom loss functions when loss function is mse\n",
    "        if (hps[13] == 'mse') & (hps[14] > 0):\n",
    "            continue\n",
    "            \n",
    "        # build the hp and yhats dictionary\n",
    "        hps_yhats_dict = {'first_layer_width': hps[0],\n",
    "                          'number_hidden_layer': hps[1],\n",
    "                          'learning_rate': hps[2],\n",
    "                          'batch_size': hps[3],\n",
    "                          'l1_penalty': hps[4],\n",
    "                          'number_ensemble': hps[5],\n",
    "                          'epoch': hps[6],\n",
    "                          'early_stopping': hps[7],\n",
    "                          'bootstrap_pct': hps[8],\n",
    "                          'dropout_rate': hps[9],\n",
    "                          'apply_dropout': hps[10],\n",
    "                          'leaky_relu': hps[11],\n",
    "                          'patience': hps[12],\n",
    "                          'loss': hps[13],\n",
    "                          'lmbd': hps[14],\n",
    "                          'bias_initial': hps[15],\n",
    "                          'weight_initial': hps[16],\n",
    "                          'lhs_var': hps[17],\n",
    "                          'results_df': pd.DataFrame()}\n",
    "        \n",
    "        # determine relevant lhs variables and other column names to use\n",
    "        all_cols      = list(df.columns.values)\n",
    "        y_cols        = [col for col in all_cols if col[:(len(hps_yhats_dict['lhs_var'])+3)] == \n",
    "                         ('y_'+hps_yhats_dict['lhs_var']+'_')]\n",
    "        r_tplus_cols  = [col for col in all_cols if col[:8] == lhs_col]\n",
    "        covar_cols    = [col for col in all_cols if col[:6] == 'covar_']\n",
    "        macro_cols    = [col for col in all_cols if col[:6] == 'macro_']\n",
    "        \n",
    "        # add target mse to validation results\n",
    "        naive_forecast = np.mean(df[df.index.year>last_train_year][y_cols].values)\n",
    "        target_mse = np.mean(np.square(df[df.index.year>last_train_year][y_cols].values-naive_forecast))\n",
    "        hps_yhats_dict['target_mse'] = target_mse\n",
    "        \n",
    "        # fit on all val dates\n",
    "        print(hps_yhats_dict, '\\n')\n",
    "        tic = time.perf_counter()\n",
    "        training_model_mses = np.array([])\n",
    "        val_model_mses      = np.array([])\n",
    "        for val_date in val_dates:\n",
    "            # remove irrelevant lhs varibles\n",
    "            temp_df  = df[y_cols+r_tplus_cols+covar_cols+macro_cols].copy()\n",
    "            \n",
    "            # Form train and validation data frames\n",
    "            temp_df  = temp_df[df.index <= val_date]\n",
    "            train_df = temp_df[temp_df.index < val_date]\n",
    "            val_df   = temp_df[temp_df.index == val_date]\n",
    "            \n",
    "            # Fit model and generate yhats for val week\n",
    "            models         = fitNN(train_df, hps_yhats_dict, lhs_col)\n",
    "            val_results_df = genYhats(val_df, models, lhs_col)\n",
    "\n",
    "            # Append this date's results\n",
    "            hps_yhats_dict['results_df'] = pd.concat((hps_yhats_dict['results_df'], val_results_df))\n",
    "\n",
    "            # Display Val Date results\n",
    "            # train data results just to check\n",
    "            train_results_df = genYhats(train_df, models, lhs_col)\n",
    "            train_ys         = train_results_df.ys.values[0]\n",
    "            train_yhats      = train_results_df.yhats.values[0]\n",
    "            train_naive      = np.repeat(np.mean(train_ys), len(train_ys))\n",
    "            train_mse_naive  = np.mean(np.square(train_ys-train_naive))\n",
    "            train_mse_model  = np.mean(np.square(train_ys-train_yhats))\n",
    "            print('Train mse Naive: '+str(np.round(train_mse_naive, 6)))\n",
    "            print('Train mse Model: '+str(np.round(train_mse_model, 6)))\n",
    "            training_model_mses = np.concatenate((training_model_mses, \n",
    "                                                  np.array([train_mse_model]))) # save to report after all val weeks predicted\n",
    "            \n",
    "            # mse results\n",
    "            print('\\n')\n",
    "            print(val_date)\n",
    "            val_ys        = val_results_df.ys.values[0]\n",
    "            val_yhats     = val_results_df.yhats.values[0]\n",
    "            \n",
    "            print(val_ys)\n",
    "            print(val_yhats)\n",
    "            val_naive     = np.repeat(np.mean(val_ys), len(val_ys))\n",
    "            val_mse_naive = np.mean(np.square(val_ys-val_naive))\n",
    "            val_mse_model = np.mean(np.square(val_ys-val_yhats))\n",
    "            val_model_mses = np.concatenate((val_model_mses,\n",
    "                                             np.array([val_mse_model]))) # save to report after all val weeks predicted\n",
    "            print('Val date mse Naive: '+str(np.round(val_mse_naive, 6)))\n",
    "            print('Val date mse Model: '+str(np.round(val_mse_model, 6)))\n",
    "            \n",
    "            #yhat distribution results\n",
    "            print('Distribution of val week y hats:')\n",
    "            print('Max : '+str(np.round(np.max(val_yhats), 2)))\n",
    "            print('Median: '+str(np.round(np.median(val_yhats), 2)))\n",
    "            print('Min : '+str(np.round(np.min(val_yhats), 2)))\n",
    "\n",
    "            # actual return results\n",
    "            val_rets      = val_df[r_tplus_cols].values\n",
    "            val_ret_naive = np.mean(val_rets)\n",
    "            val_ret_model = np.sum(val_yhats*val_rets)\n",
    "            print('Naive val return: '+str(np.round(val_ret_naive, 8)))\n",
    "            print('Model val return: '+str(np.round(val_ret_model, 8)))\n",
    "\n",
    "            # Display to date results\n",
    "            # mse results\n",
    "            print('\\nEntire val period results:')\n",
    "            oos_ys        = np.vstack(hps_yhats_dict['results_df'].ys.values)\n",
    "            oos_yhats     = np.vstack(hps_yhats_dict['results_df'].yhats.values)\n",
    "            oos_naive     = np.repeat(np.mean(oos_ys), oos_ys.shape[1]*oos_ys.shape[0]).reshape(-1, oos_ys.shape[1])\n",
    "            oos_mse_naive = np.mean(np.square(oos_ys-oos_naive))\n",
    "            oos_mse_model = np.mean(np.square(oos_ys-oos_yhats))\n",
    "            print('MSE Naive: '+str(np.round(oos_mse_naive, 6)))\n",
    "            print('MSE Model: '+str(np.round(oos_mse_model, 6)))\n",
    "            if oos_mse_model < oos_mse_naive:\n",
    "                print('Winning MSE?: Yes! ' + str(np.round(oos_mse_naive-oos_mse_model, 4)))\n",
    "            else:\n",
    "                print('Winning MSE?: No! ' + str(np.round(oos_mse_naive-oos_mse_model, 4)))\n",
    "                \n",
    "            #yhat distribution results\n",
    "            print('Distribution of val period y hats:')\n",
    "            print('Max : '+str(np.round(np.max(oos_yhats), 2)))\n",
    "            print('Median: '+str(np.round(np.median(oos_yhats), 2)))\n",
    "            print('Min : '+str(np.round(np.min(oos_yhats), 2)))\n",
    "\n",
    "            # actual return results\n",
    "            num_dates     = hps_yhats_dict['results_df'].shape[0]\n",
    "            oos_rets      = np.vstack(hps_yhats_dict['results_df'][lhs_col].values)\n",
    "            oos_ret_naive = np.prod(1+np.mean(oos_rets, axis=1))**(1/num_dates)-1\n",
    "            oos_ret_model = np.prod(1+np.sum(oos_rets*oos_yhats, axis=1))**(1/num_dates)-1\n",
    "            print('Naive avg per date return to date: '+str(np.round(oos_ret_naive, 4)))\n",
    "            print('Model avg per date return to date: '+str(np.round(oos_ret_model, 4)))\n",
    "            if oos_ret_model > oos_ret_naive:\n",
    "                print('Winning return?: Yes! ' + str(np.round(oos_ret_model-oos_ret_naive, 8)))\n",
    "            else:\n",
    "                print('Winning return?: No! ' + str(np.round(oos_ret_model-oos_ret_naive, 8)))\n",
    "            \n",
    "            # Stop running if results are not good enough\n",
    "#             month = val_date.astype('datetime64[M]').astype(int)%12+1\n",
    "#             if (month >= 2) & ((oos_ret_model-oos_ret_naive)<-0.003):\n",
    "#                 break   \n",
    "#             if (month >= 3) & ((oos_ret_model-oos_ret_naive)<-0.001):\n",
    "#                 break  \n",
    "#             if (month >= 6) & ((oos_ret_model-oos_ret_naive)<0):\n",
    "#                 break \n",
    "#             year = val_date.astype('datetime64[Y]').astype(int)+1970\n",
    "#             if (year >= 2019) & ((oos_ret_model-oos_ret_naive)<0):\n",
    "#                 break\n",
    "                \n",
    "            # space out the results\n",
    "            print('\\n')\n",
    "\n",
    "        # Save run time and space out result print out\n",
    "        toc = time.perf_counter()\n",
    "        print('\\n\\n\\n')\n",
    "\n",
    "        # Update this HP point's results and append to list of results\n",
    "        hps_yhats_dict['end_date']      = val_date\n",
    "        hps_yhats_dict['mse']           = oos_mse_model\n",
    "        hps_yhats_dict['val_ret_delta'] = oos_ret_model-oos_ret_naive\n",
    "        results_list.append(hps_yhats_dict)\n",
    "\n",
    "        # Save the ongoing results as a csv to be able to watch\n",
    "        cv_results_dict = hps_yhats_dict.copy()\n",
    "        del cv_results_dict['results_df']\n",
    "        cv_results_dict['runtime_mins']     = round((toc - tic)/60, 0)  \n",
    "        cv_results_dict['num_lhs_vars']     = len(r_tplus_cols)\n",
    "        cv_results_dict['num_rhs_vars']     = len(covar_cols+macro_cols)\n",
    "        cv_results_dict['arch_name']        = arch_name\n",
    "        cv_results_dict['first_val_year']   = str(last_train_year+1) \n",
    "        cv_results_dict['first_train_year'] = str(np.min(df.index.year))\n",
    "        cv_results_dict['models_avg_training_mses'] = np.mean(training_model_mses)\n",
    "        cv_results_dict['models_avg_diff_btwn_train_and_val_mse'] = np.mean(training_model_mses-val_model_mses)\n",
    "        # report metrics by year if relevant\n",
    "        if last_train_year == 2017:\n",
    "            temp_df   = hps_yhats_dict['results_df'].copy()\n",
    "            rets      = np.vstack(temp_df[temp_df.date.dt.year==2018][lhs_col].values)\n",
    "            ys        = np.vstack(temp_df[temp_df.date.dt.year==2018].ys.values)\n",
    "            yhats     = np.vstack(temp_df[temp_df.date.dt.year==2018].yhats.values)\n",
    "            num_weeks = temp_df.shape[0]\n",
    "            cv_results_dict['ret_naive_2018'] = np.prod(1+np.mean(rets, axis=1))**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_model_2018'] = np.prod(1+np.sum(rets*yhats, axis=1))**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_diff_2018']  = np.prod((1+np.sum(rets*yhats, axis=1))-(1+np.mean(rets, axis=1)))**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_btc_2018']   = np.prod(1+df[df.index.year==2018][lhs_col+'_bitcoin'].values)**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_eth_2018']   = np.prod(1+df[df.index.year==2018][lhs_col+'_ethereum'].values)**(1/num_weeks)-1\n",
    "            cv_results_dict['mse_model_2018'] = np.mean(np.square(ys-yhats))\n",
    "            cv_results_dict['mse_model_divided_naive_2018'] = np.mean(np.square(ys-yhats))/target_mse\n",
    "        \n",
    "        if last_train_year <= 2018:\n",
    "            temp_df   = hps_yhats_dict['results_df'].copy()\n",
    "            rets      = np.vstack(temp_df[temp_df.date.dt.year==2019][lhs_col].values)\n",
    "            ys        = np.vstack(temp_df[temp_df.date.dt.year==2019].ys.values)\n",
    "            yhats     = np.vstack(temp_df[temp_df.date.dt.year==2019].yhats.values)\n",
    "            num_weeks = temp_df.shape[0]\n",
    "            cv_results_dict['ret_naive_2019'] = np.prod(1+np.mean(rets, axis=1))**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_model_2019'] = np.prod(1+np.sum(rets*yhats, axis=1))**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_btc_2019']   = np.prod(1+df[df.index.year==2019][lhs_col+'_bitcoin'].values)**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_eth_2019']   = np.prod(1+df[df.index.year==2019][lhs_col+'_ethereum'].values)**(1/num_weeks)-1\n",
    "            cv_results_dict['mse_model_2019'] = np.mean(np.square(ys-yhats))\n",
    "            cv_results_dict['mse_model_divided_naive_2019'] = np.mean(np.square(ys-yhats))/target_mse\n",
    "        \n",
    "        if last_train_year <= 2019:\n",
    "            temp_df   = hps_yhats_dict['results_df'].copy()\n",
    "            rets      = np.vstack(temp_df[temp_df.date.dt.year==2020][lhs_col].values)\n",
    "            ys        = np.vstack(temp_df[temp_df.date.dt.year==2020].ys.values)\n",
    "            yhats     = np.vstack(temp_df[temp_df.date.dt.year==2020].yhats.values)\n",
    "            num_weeks = temp_df.shape[0]\n",
    "            cv_results_dict['ret_naive_2020'] = np.prod(1+np.mean(rets, axis=1))**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_model_2020'] = np.prod(1+np.sum(rets*yhats, axis=1))**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_btc_2020']   = np.prod(1+df[df.index.year==2020][lhs_col+'_bitcoin'].values)**(1/num_weeks)-1\n",
    "            cv_results_dict['ret_eth_2020']   = np.prod(1+df[df.index.year==2020][lhs_col+'_ethereum'].values)**(1/num_weeks)-1\n",
    "            cv_results_dict['mse_model_2020'] = np.mean(np.square(ys-yhats))\n",
    "            cv_results_dict['mse_model_divided_naive_2020'] = np.mean(np.square(ys-yhats))/target_mse\n",
    "        \n",
    "        hps_mse_df_list.append(pd.DataFrame(cv_results_dict, index=[0]))\n",
    "        cv_df = pd.concat(hps_mse_df_list, ignore_index=True)\n",
    "        \n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = '../4-output/cv-results-custom-' + timestr + '.csv'\n",
    "        cv_df.to_csv(fp, index=False)\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a80cd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelPortfolioWeights(df):\n",
    "    # set parameters for tertile weights\n",
    "    # note: can't go to zero on any as there may be week where that is only yhat\n",
    "    bottom_tertile = 1/6\n",
    "    mid_tertile = 1/3\n",
    "    top_tertile = (1-bottom_tertile-mid_tertile)\n",
    "\n",
    "    # create portfolio weights\n",
    "    df = df.sort_values(by=['date', 'yhat'])\n",
    "    df['counts']  = 1\n",
    "    df['total_assets_per_week_tertile'] = df.groupby(['date', 'yhat']).counts.transform('sum')\n",
    "    df.loc[df.yhat == -1, 'prtfl_wght'] = bottom_tertile / df[df.yhat == -1].total_assets_per_week_tertile\n",
    "    df.loc[df.yhat == 0,  'prtfl_wght'] = mid_tertile / df[df.yhat == 0].total_assets_per_week_tertile\n",
    "    df.loc[df.yhat == 1,  'prtfl_wght'] = top_tertile / df[df.yhat == 1].total_assets_per_week_tertile\n",
    "\n",
    "    # clean up\n",
    "    df = df.drop(['counts', 'total_assets_per_week_tertile'], axis=1)\n",
    "\n",
    "    # fix weeks where portfolio weights add up to less than 1\n",
    "    temp_df = df.groupby(['date'])[['prtfl_wght']].sum()\n",
    "    temp_df = temp_df[~np.isclose(temp_df.prtfl_wght, 1)]\n",
    "    if 0<temp_df.shape[0]:\n",
    "        for i in range(temp_df.shape[0]):\n",
    "            date = temp_df.index.values[i]\n",
    "            total_weight = temp_df.prtfl_wght.values[i]\n",
    "            df.loc[df.index == date, 'prtfl_wght'] = df[df.index==date].prtfl_wght * (1/total_weight)\n",
    "            \n",
    "    # confirm portfolio weights roughly sum to 1 for each week\n",
    "    assert(len(np.unique(df.index)) == \n",
    "           np.sum(np.isclose(df.groupby(['date']).prtfl_wght.sum(), 1,\n",
    "                             rtol=1e-2, atol=1e-2)))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "626e997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAnnualTransactionCosts(df):\n",
    "    # merge on the previous week's holdings to the new holdings\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df.index<np.max(temp_df.index)]\n",
    "    temp_df.index = temp_df.index+np.timedelta64(1, 'D')\n",
    "    temp_df = temp_df[['asset', 'prtfl_wght']]\n",
    "    temp_df = temp_df.rename(columns={'prtfl_wght': 'prtfl_wght_tm7'})\n",
    "    df      = df.merge(temp_df,\n",
    "                       on=['date', 'asset'],\n",
    "                       how='outer',\n",
    "                       validate='one_to_one')\n",
    "\n",
    "    # calc weekly turnover and ensure it has the appropriate range\n",
    "    df['asset_to'] = np.abs(df.prtfl_wght - df.prtfl_wght_tm7)\n",
    "    to_df = df.groupby('date')[['asset_to']].sum().reset_index()\n",
    "    assert((np.min(to_df.asset_to)>=0) & (np.max(to_df.asset_to<=2)))\n",
    "\n",
    "    # correct the first and last week valid for buying the initial port and liquidating\n",
    "    to_df.loc[0, 'asset_to'] = 1\n",
    "    to_df = pd.concat((to_df, pd.DataFrame(data={'date': np.max(temp_df.index.values)+np.timedelta64(1, 'D'),\n",
    "                                                 'asset_to': 1}, index=[0])))\n",
    "    to_df = to_df.reset_index(drop=True)\n",
    "\n",
    "    # add transaction costs assuming maker and taker fee of 20 bps each\n",
    "    to_df['tc'] = to_df.asset_to*0.002\n",
    "\n",
    "    # return annualize transaction cost\n",
    "    return -np.sum(to_df.tc)/(to_df.shape[0]/52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6664dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioReturn(r_df):\n",
    "    num_days = r_df.shape[0]\n",
    "    if np.sum(r_df.r.values <= -1)>=1:\n",
    "        return -1\n",
    "    else:\n",
    "        tot_ret   = np.product(r_df.r.values+1)-1\n",
    "        daily_ret = (tot_ret+1)**(1/num_days)-1\n",
    "        annl_ret  = (daily_ret+1)**(365.25)-1\n",
    "        return annl_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d85e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioSharpe(df):\n",
    "    daily_sharpe = np.mean(df.r.values)/np.std(df.r.values)\n",
    "    annl_sharpe  = daily_sharpe*np.sqrt(365.25)\n",
    "    return annl_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142f862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_draw_down(df):\n",
    "    cumulative_ret=(df.r+1).cumprod()\n",
    "    roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "    daily_drawdown=cumulative_ret/roll_max\n",
    "    max_daily_drawdown=daily_drawdown.min() - 1\n",
    "    return max_daily_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23e023c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_1_week_loss(df):\n",
    "    max_loss=(df['r']+1).rolling(7).apply(np.prod)\n",
    "    max_loss_minus=max_loss.min()-1\n",
    "    return max_loss_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5809a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPsuedoOOSWeek(train_df, oos_df):\n",
    "    # obtain rhs values for oos week and all training data\n",
    "    all_cols         = list(oos_df.columns.values)\n",
    "    rhs_cols         = [col for col in all_cols if col[:6] == 'covar_']\n",
    "    oos_rhs_values   = oos_df[rhs_cols].values\n",
    "    train_rhs_values = train_df[rhs_cols].values\n",
    "    \n",
    "    # find the index of training data that most resembles the oos week's rhs\n",
    "    # TO DO: Make this better by taking it within cycle week\n",
    "    weekly_l2_norm   = np.sqrt(np.sum(np.square(train_rhs_values - oos_rhs_values), axis=1))\n",
    "    matching_week    = np.argmin(weekly_l2_norm)\n",
    "\n",
    "    # return that week of training data as pusedo oos week\n",
    "    return train_df[matching_week:(matching_week+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f487f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenTestYhats(df, lhs_col, opt_hps, test_year=2022): \n",
    "    test_dates = np.unique(df[df.index.year == test_year].index.values)\n",
    "\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "    for test_date in test_dates:\n",
    "        print(test_date, '\\n')\n",
    "        temp_df       = df[df.index <= test_date].copy()\n",
    "        train_df      = temp_df[temp_df.index < test_date].copy()\n",
    "        oos_df        = temp_df[temp_df.index == test_date].copy()\n",
    "\n",
    "        models = fitNN(train_df, opt_hps, lhs_col)\n",
    "\n",
    "        oos_results_df = genYhats(oos_df, models, lhs_col)\n",
    "\n",
    "        test_df = pd.concat((test_df, oos_results_df))\n",
    "\n",
    "        # oos date results\n",
    "        oos_ys        = oos_results_df.ys.values[0]\n",
    "        oos_yhats     = oos_results_df.yhats.values[0]\n",
    "        oos_rets      = np.vstack(oos_results_df[lhs_col].values)\n",
    "        oos_ret_naive = np.mean(oos_rets)\n",
    "        oos_ret_model = np.prod(1+np.sum(oos_yhats*oos_rets, axis=1))**(1/test_df.shape[0])-1\n",
    "        print('OOS week results:')\n",
    "        print('Naive oos return: '+str(np.round(oos_ret_naive, 8)))\n",
    "        print('Model oos return: '+str(np.round(oos_ret_model, 8)))\n",
    "        print('\\n')\n",
    "\n",
    "        # oos results\n",
    "        oos_ys    = np.vstack(test_df.ys.values)\n",
    "        oos_yhats = np.vstack(test_df.yhats.values)\n",
    "        oos_naive = np.repeat(np.mean(oos_ys), oos_ys.shape[1]*oos_ys.shape[0]).reshape(-1, oos_ys.shape[1])\n",
    "        oos_mse_naive = np.mean(np.square(oos_ys-oos_naive))\n",
    "        oos_mse_model = np.mean(np.square(oos_ys-oos_yhats))\n",
    "        print('full oos period results:')\n",
    "        print('oos mse naive: '+str(np.round(oos_mse_naive, 6)))\n",
    "        print('oos mse model: '+str(np.round(oos_mse_model, 6)))\n",
    "        print('Distribution of oos y hats:')\n",
    "        print('Max : '+str(np.round(np.max(oos_yhats), 2)))\n",
    "        print('Median: '+str(np.round(np.median(oos_yhats), 2)))\n",
    "        print('Min : '+str(np.round(np.min(oos_yhats), 2)))\n",
    "        oos_rets      = np.vstack(test_df[lhs_col].values)\n",
    "        oos_ret_naive = np.prod(np.mean(oos_rets,axis=1)+1)**(1/test_df.shape[0])-1\n",
    "        oos_ret_model = np.prod(1+np.sum(oos_yhats*oos_rets, axis=1))**(1/test_df.shape[0])-1\n",
    "        print('Naive oos return: '+str(np.round(oos_ret_naive, 8)))\n",
    "        print('Model oos return: '+str(np.round(oos_ret_model, 8)))\n",
    "        if oos_ret_model > oos_ret_naive:\n",
    "            print('Winning return?: Yes! ' + str(np.round(oos_ret_model-oos_ret_naive, 8)))\n",
    "        else:\n",
    "            print('Winning return?: No! ' + str(np.round(oos_ret_model-oos_ret_naive, 8)))\n",
    "        print('\\n')\n",
    "    \n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE CODE\n",
    "\n",
    "# Load in the data\n",
    "#input_fp = '../3-data/clean/panel_train.pkl'\n",
    "input_fp = '../3-data/clean/panel_btceth.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "lhs_col = 'r_tplus1'\n",
    "\n",
    "# Clean up the data\n",
    "df = df.set_index('date')\n",
    "df = df.drop(['covar_mcap_t', 'covar_supply_adr_bal_100k_usd_t',\n",
    "              'covar_supply_outside_ex_t'], axis=1)\n",
    "\n",
    "# create new lhs variables\n",
    "df = createPlatonicPortfolioWeights(df, min_weight=0.01, lhs_col=lhs_col)\n",
    "\n",
    "# convert to single obseration per date\n",
    "df = df.sort_values(by=['date', 'asset'])   \n",
    "df = flattenToOneObPerDate(df, lhs_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY WITH AND WITHOUT NORMALIZATION\n",
    "rhs_cols = list(df.columns.values)[4:]\n",
    "for col in rhs_cols:\n",
    "    df[col] = (df[col]-np.mean(df[col]))/np.std(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "566d9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "174fffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index<np.max(df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = runCV(df, asset_universe_dict=None, lhs_col=lhs_col,\n",
    "                     last_train_year=2019, val_end_year=2021,\n",
    "                     arch_name='ffnn_one_ob_per_day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "4c23efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_master = results_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db1b6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE RESULTS\n",
    "with open('../3-data/derived/validation_2020_2021_results_btceth_normalized.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18fb7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../3-data/derived/validation_2020_2021_results_btceth_normalized.pickle', 'rb') as handle:\n",
    "    results_list = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "488664e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WAS NOT NORMALIZED\n",
    "# with open('../3-data/derived/validation_2020_2021_results_btceth.pickle', 'wb') as handle:\n",
    "#     pickle.dump(results_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# THIS WAS WITH OLD VARIABLES\n",
    "# with open('../3-data/derived/validation_2018_2020_results_btceth.pickle', 'rb') as handle:\n",
    "#     results_list = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form validation period data\n",
    "all_cols      = list(df.columns.values)\n",
    "r_tplus_cols = [col for col in all_cols if col[:8] == lhs_col]\n",
    "df = df[r_tplus_cols]\n",
    "df = df[(df.index.year ==2020) | (df.index.year ==2021)]\n",
    "\n",
    "# form validation period benchmark results\n",
    "actual_simple_rets        = np.mean(df.values, axis=1)\n",
    "actual_daily_simple_ret  = np.prod(1+actual_simple_rets)**(1/len(actual_simple_rets))-1\n",
    "actual_annual_simple_ret  = (actual_daily_simple_ret+1)**(365.25)-1\n",
    "actual_annual_sharpe      = np.mean(actual_simple_rets)/np.std(actual_simple_rets)*np.sqrt(365.25)\n",
    "print('annual equal-weighted simple return: ' + \n",
    "      str(np.round(actual_annual_simple_ret, 4)))\n",
    "print('annual equal-weighted sharpe: ' + \n",
    "      str(np.round(actual_annual_sharpe, 2)))\n",
    "print('\\n')\n",
    "actual_simple_rets        = df['r_tplus1_bitcoin'].values\n",
    "actual_daily_simple_ret   = np.prod(1+actual_simple_rets)**(1/len(actual_simple_rets))-1\n",
    "actual_annual_simple_ret  = (actual_daily_simple_ret+1)**(365.25)-1\n",
    "actual_annual_sharpe      = np.mean(actual_simple_rets)/np.std(actual_simple_rets)*np.sqrt(365.25)\n",
    "print('BTC annual simple return: ' + \n",
    "      str(np.round(actual_annual_simple_ret, 4)))\n",
    "print('BTC annual sharpe: ' + \n",
    "      str(np.round(actual_annual_sharpe, 2)))\n",
    "print('\\n')\n",
    "actual_simple_rets        = df['r_tplus1_ethereum'].values\n",
    "actual_daily_simple_ret   = np.prod(1+actual_simple_rets)**(1/len(actual_simple_rets))-1\n",
    "actual_annual_simple_ret  = (actual_daily_simple_ret+1)**(365.25)-1\n",
    "actual_annual_sharpe      = np.mean(actual_simple_rets)/np.std(actual_simple_rets)*np.sqrt(365.25)\n",
    "print('ETH annual simple return: ' + \n",
    "      str(np.round(actual_annual_simple_ret, 4)))\n",
    "print('ETH annual sharpe: ' + \n",
    "      str(np.round(actual_annual_sharpe, 2)))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2bbd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form validation period results\n",
    "df = pd.DataFrame(data={'date': np.repeat(df.index.values, df.shape[1]),\n",
    "                        'asset': np.tile([col[9:] for col in df.columns], df.shape[0]),\n",
    "                        lhs_col: df.values.reshape(-1),\n",
    "                        'prtfl_wght': np.vstack(results_list[0]['results_df'].yhats.values).reshape(-1)})\n",
    "df = df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "090a386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "long = 0.9\n",
    "short = -0.1\n",
    "long_threshold = 0\n",
    "short_threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "250e6861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annual transaction costs in simple return terms: -0.0513\n",
      "annual simple return before trans costs: 6.2743\n",
      "annual sharpe: 2.24\n",
      "max drawdown : -0.6\n",
      "max one week loss : -0.46\n",
      "LFG!!!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.loc[df.prtfl_wght>long_threshold, 'prtfl_wght'] = long\n",
    "df.loc[df.prtfl_wght<short_threshold, 'prtfl_wght'] = short\n",
    "\n",
    "annl_tc = calcAnnualTransactionCosts(df)\n",
    "df['r'] = df.prtfl_wght*df[lhs_col]\n",
    "r_df    = df.groupby(['date'])[['r']].sum()\n",
    "annl_ret = calcPortfolioReturn(r_df)\n",
    "annl_sharpe = calcPortfolioSharpe(r_df)\n",
    "max_dd = max_draw_down(r_df)\n",
    "max_1week_loss = max_1_week_loss(r_df)\n",
    "print('annual transaction costs in simple return terms: ' + str(np.round(annl_tc, 4)))\n",
    "print('annual simple return before trans costs: ' + str(np.round(annl_ret, 4)))\n",
    "print('annual sharpe: '+str(np.round(annl_sharpe, 2)))\n",
    "print('max drawdown : '+str(np.round(max_dd, 2)))\n",
    "print('max one week loss : '+str(np.round(max_1week_loss, 2)))\n",
    "print('LFG!!!!!')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4301ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean up data for test fitting\n",
    "input_fp = '../3-data/clean/panel_btceth.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "df = df.drop(['covar_mcap_t', 'covar_supply_adr_bal_100k_usd_t',\n",
    "              'covar_supply_outside_ex_t'], axis=1)\n",
    "df = createPlatonicPortfolioWeights(df, min_weight=0.01, lhs_col=lhs_col)\n",
    "df = df.sort_values(by=['date', 'asset'])   \n",
    "df = flattenToOneObPerDate(df, lhs_col)\n",
    "rhs_cols = list(df.columns.values)[4:]\n",
    "for col in rhs_cols:\n",
    "    df[col] = (df[col]-np.mean(df[col]))/np.std(df[col])\n",
    "df = df.astype('float32')\n",
    "opt_hps = results_list[0].copy()\n",
    "del opt_hps['results_df']\n",
    "del opt_hps['end_date']\n",
    "del opt_hps['mse']\n",
    "del opt_hps['val_ret_delta']\n",
    "\n",
    "# TEMP DROP OF LAST DATE\n",
    "df = df[df.index < np.max(df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit test period yhats\n",
    "results_test_df = GenTestYhats(df, lhs_col, opt_hps) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a05a2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form test period data\n",
    "all_cols      = list(df.columns.values)\n",
    "r_tplus_cols = [col for col in all_cols if col[:8] == lhs_col]\n",
    "df = df[r_tplus_cols]\n",
    "df            = df[df.index.year == 2022]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# form test period benchmark results\n",
    "actual_simple_rets        = np.mean(df.values, axis=1)\n",
    "actual_weekly_simple_ret  = np.prod(1+actual_simple_rets)**(1/len(actual_simple_rets))-1\n",
    "actual_annual_simple_ret  = (actual_weekly_simple_ret+1)**(365.25)-1\n",
    "actual_annual_sharpe      = np.mean(actual_simple_rets)/np.std(actual_simple_rets)*np.sqrt(365.25)\n",
    "print('annual equal-weighted simple return: ' + \n",
    "      str(np.round(actual_annual_simple_ret, 4)))\n",
    "print('annual equal-weighted sharpe: ' + \n",
    "      str(np.round(actual_annual_sharpe, 2)))\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# form test period results\n",
    "test_df      = pd.DataFrame(data={'date': np.repeat(df.index.values, df.shape[1]),\n",
    "                                  'asset': np.tile([col[9:] for col in df.columns], df.shape[0]),\n",
    "                                  lhs_col: df.values.reshape(-1),\n",
    "                                  'prtfl_wght': np.vstack(results_test_df.yhats.values).reshape(-1)})\n",
    "test_df      = test_df.set_index('date')\n",
    "long = 0.5\n",
    "short = -0.4\n",
    "long_threshold = 0.5\n",
    "short_threshold = -0.4\n",
    "test_df.loc[test_df.prtfl_wght>long_threshold, 'prtfl_wght'] = long\n",
    "test_df.loc[test_df.prtfl_wght<short_threshold, 'prtfl_wght'] = short\n",
    "\n",
    "annl_tc      = calcAnnualTransactionCosts(test_df)\n",
    "test_df['r'] = test_df.prtfl_wght*test_df[lhs_col]\n",
    "r_df         = test_df.groupby(['date'])[['r']].sum()\n",
    "annl_ret     = calcPortfolioReturn(r_df)\n",
    "annl_sharpe  = calcPortfolioSharpe(r_df)\n",
    "max_dd       = max_draw_down(r_df)\n",
    "max_1mo_loss = max_1_week_loss(r_df)\n",
    "print('annual transaction costs in simple return terms: ' + str(np.round(annl_tc, 4)))\n",
    "print('annual simple return before trans costs: ' + str(np.round(annl_ret, 4)))\n",
    "print('annual sharpe: '+str(np.round(annl_sharpe, 2)))\n",
    "print('max drawdown : '+str(np.round(max_dd, 2)))\n",
    "print('max one week loss : '+str(np.round(max_1mo_loss, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "fa40f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SAVE THESE RESULTS TO SOME FILEAS BEAUT!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a640d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP FILE FOR MODEL HPs BUT I SHOULD USE THE FINAL TEST ONE\n",
    "with open('../3-data/derived/validation_2020_2021_results_btceth_normalized.pickle', 'rb') as handle:\n",
    "    results_list = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96b5e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean up data for quant strat\n",
    "lhs_col = 'r_tplus1'\n",
    "input_fp = '../3-data/clean/panel_btceth.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "\n",
    "df = createPlatonicPortfolioWeights(df, min_weight=0.01, lhs_col=lhs_col)\n",
    "df = df.sort_values(by=['date', 'asset'])   \n",
    "df = flattenToOneObPerDate(df, lhs_col)\n",
    "rhs_cols = list(df.columns.values)[4:]\n",
    "\n",
    "for col in rhs_cols:\n",
    "    df[col] = (df[col]-np.mean(df[col]))/np.std(df[col])\n",
    "df = df.astype('float32')\n",
    "opt_hps = results_list[0].copy()\n",
    "del opt_hps['results_df']\n",
    "del opt_hps['end_date']\n",
    "del opt_hps['mse']\n",
    "del opt_hps['val_ret_delta']\n",
    "\n",
    "train_df = df[df.index < np.max(df.index)]\n",
    "oos_df = df[df.index == np.max(df.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f14f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = fitNN(train_df, opt_hps, lhs_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_results_df = genYhats(oos_df, models, lhs_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04588eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN MASTER DATA\n",
    "master_oos_results_df = pd.read_pickle('../3-data/clean/oos_yhats_2022.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fae51700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append data\n",
    "oos_results_df = pd.concat((master_oos_results_df, oos_results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a654c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE NEW MASTER DATA\n",
    "oos_results_df.to_pickle('../3-data/clean/oos_yhats_2022.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe779ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22887671",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=np.vstack(results_list[0]['results_df'].yhats.values)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dfa80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
