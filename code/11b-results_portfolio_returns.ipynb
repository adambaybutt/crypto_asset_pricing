{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE WITH NEW PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "959a7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGeomAvg(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the geometric average of a vector of simple returns.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar geometric average.\n",
    "    \"\"\"\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        raise TypeError(\"Input 'returns' must be a NumPy array\")\n",
    "    if annualized and periods_in_year is None:\n",
    "        raise ValueError(\"Input 'periods_in_year' must be provided if 'annualized' is True\")\n",
    "    geom_avg_at_given_freq = np.prod(1 + returns) ** (1 / np.size(returns)) - 1\n",
    "    return (geom_avg_at_given_freq + 1) ** periods_in_year - 1 if annualized else geom_avg_at_given_freq\n",
    "\n",
    "def calcTSAvgReturn(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float:\n",
    "    \"\"\" Calculate the time series mean return of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar time series mean return.\n",
    "    \"\"\"\n",
    "    mean_ret_at_given_freq = np.mean(returns)\n",
    "    if annualized == False:\n",
    "        return mean_ret_at_given_freq\n",
    "    else:\n",
    "        mean_ret = periods_in_year*mean_ret_at_given_freq\n",
    "        if mean_ret < -1:\n",
    "            return -1.\n",
    "        else:\n",
    "            return mean_ret\n",
    "\n",
    "def calcTotalReturn(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float:\n",
    "    \"\"\" Calculate the total return of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar total return.\n",
    "    \"\"\"\n",
    "    total_return = np.prod(1+returns)-1\n",
    "    if annualized==False:\n",
    "        return total_return\n",
    "    else:\n",
    "        return (total_return+1)**(periods_in_year/len(returns))-1\n",
    "\n",
    "def calcSD(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the standard deviation of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar standard deviation.\n",
    "    \"\"\"\n",
    "    sd_at_given_freq = np.std(returns)\n",
    "    if annualized==False:\n",
    "        return sd_at_given_freq\n",
    "    else:\n",
    "        return np.sqrt(periods_in_year)*sd_at_given_freq\n",
    "\n",
    "def calcSharpe(returns: np.array,\n",
    "    periods_in_year: int,\n",
    "    risk_free_returns: np.array=None) -> float:\n",
    "    \"\"\" Calculate the annual Sharpe Ratio of a vector of simple returns. \n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "        risk_free_returns (np.array): vector of simple returns of the risk free rate.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar standard deviation.\n",
    "    \"\"\"\n",
    "    if risk_free_returns is not None:\n",
    "        returns = returns - risk_free_returns\n",
    "    \n",
    "    return (calcTSAvgReturn(returns, annualized=True, periods_in_year=periods_in_year) /\n",
    "            calcSD(returns, annualized=True, periods_in_year=periods_in_year))\n",
    "\n",
    "def calcMaxDrawdown(returns: np.array) -> float:\n",
    "    ''' calculate maximum drawdown for a vector of returns of any frequency.\n",
    "    \n",
    "    Args:\n",
    "        returns (np.array): vector of simple returns.\n",
    "    \n",
    "    Returns:\n",
    "        max_drawdown (float): maximum drawdown in simple return units over this period.\n",
    "    '''\n",
    "    # calculate the cumulative return as a new vector of the same length\n",
    "    cumulative_ret=(returns+1).cumprod()\n",
    "\n",
    "    # for every period, calc the historic maximum value of the portfolio \n",
    "    roll_max=pd.Series(cumulative_ret).rolling(len(cumulative_ret), min_periods=1).max()\n",
    "\n",
    "    # calc drawdown as the current portfolio value divided by the historic max value\n",
    "    dd=np.min(cumulative_ret/roll_max)\n",
    "    \n",
    "    # return simple return of max drawdown\n",
    "    return dd-1\n",
    "\n",
    "def calcMaxOneWeekLoss(returns: np.array, periods_in_week: int) -> float:\n",
    "    ''' Calculate the maximum loss for a one week period given how many obs are in week for input\n",
    "        returns.\n",
    "    \n",
    "    Args:\n",
    "        returns (np.array): vector of simple returns of any frequency.\n",
    "        periods_in_week (int): number of observations in a week.\n",
    "    \n",
    "    Returns:\n",
    "        max_loss (float): maximum loss over any one week period in simple returns.\n",
    "    '''\n",
    "    weekly_returns = (pd.Series(returns)+1).rolling(periods_in_week).apply(np.prod)\n",
    "    max_loss = weekly_returns.min()-1\n",
    "    return max_loss\n",
    "\n",
    "def calcTransactionCosts(positions: np.array) -> np.array:\n",
    "    ''' Calculate a vector of transaction costs which are positive numbers in return units.\n",
    "    \n",
    "    Args:\n",
    "        positions (np.array): vector of positions, where positive is long and above 1, in absolute\n",
    "                              value terms, is a leveraged position.\n",
    "\n",
    "    Returns:\n",
    "        tc (np.array): vector of transaction costs in return terms.\n",
    "    '''\n",
    "    # transaction costs, in return terms, from kraken for trading two spots paris, on margin\n",
    "    tc_to_open        = 0.0005\n",
    "    tc_to_close       = 0.0005\n",
    "    tc_to_open_margin = 0.00004\n",
    "    tc_margin_per_hr  = 0.00001\n",
    "\n",
    "    # initial tc array\n",
    "    tc = np.zeros(len(positions))\n",
    "\n",
    "    # set first tc\n",
    "    first_position = positions[0]\n",
    "    if first_position == 0:\n",
    "        tc[0] = 0\n",
    "    elif (-1 <= first_position) & (first_position <= 1):\n",
    "        tc[0] = tc_to_open\n",
    "    elif (-5 <= first_position) & (first_position <= 5):\n",
    "        tc[0] = tc_to_open+tc_to_open_margin+tc_margin_per_hr\n",
    "    else:\n",
    "        raise ValueError('first position is not a valid position.')\n",
    "\n",
    "    # set remaining tc's\n",
    "    for i in range(1,len(tc)):\n",
    "        prev_position = positions[i-1]\n",
    "        current_position = positions[i]\n",
    "        if current_position == prev_position:\n",
    "            if np.abs(current_position)>1:\n",
    "                tc[i] = tc_margin_per_hr\n",
    "        else:\n",
    "            if current_position==0:\n",
    "                tc[i] = tc_to_close\n",
    "            elif (-1 <= current_position) & (current_position <= 1):\n",
    "                tc[i] = tc_to_close+tc_to_open\n",
    "            elif (-5 <= current_position) & (current_position <= 5):\n",
    "                tc[i] = tc_to_close+tc_to_open+tc_to_open_margin+tc_margin_per_hr\n",
    "            else: \n",
    "                raise ValueError('position '+str(i)+' is not a valid position.')\n",
    "\n",
    "    # adjust last tc element for closing position\n",
    "    last_position = positions[-1]\n",
    "    if np.abs(last_position)>0:\n",
    "        tc[-1] += tc_to_close\n",
    "\n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a29cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelPortfolioWeights(df):\n",
    "    # assign tertiles\n",
    "    np.random.seed(42)\n",
    "    df['rand']    = np.random.uniform(size=df.shape[0])\n",
    "    df = df.sort_values(by=['date', 'yhat'])\n",
    "    df['ranking'] = df.groupby(['date']).cumcount()\n",
    "    df['counts']  = 1\n",
    "    df['total_assets_per_week'] = df.groupby('date').counts.transform('sum')\n",
    "    df['ranking']               = df.ranking/df.total_assets_per_week\n",
    "    df.loc[df.ranking < 1/3, 'prtfl_wght'] = 0\n",
    "    df.loc[(df.ranking>=1/3) & \n",
    "           (df.ranking<2/3), 'prtfl_wght'] = 1/6\n",
    "    df.loc[df.ranking>=2/3,  'prtfl_wght'] = 5/6\n",
    "    df['prtfl_wght'] = 3*df.prtfl_wght/df.total_assets_per_week\n",
    "    \n",
    "    # clean up\n",
    "    df = df.drop(['rand', 'ranking', 'counts',\n",
    "                  'total_assets_per_week'], axis=1)\n",
    "    \n",
    "    # confirm portfolio weights roughly sum to 1 for each week\n",
    "    assert(len(np.unique(df.index)) == \n",
    "           np.sum(np.isclose(df.groupby(['date']).prtfl_wght.sum(), 1,\n",
    "                             rtol=1e-2, atol=1e-2)))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4568121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO MOVE ALL STANDARD ASSET PRICING FUNCS TO A PY SCRIPT THAT I IMPORT AND KEEP IN MY ASSET PRICING TOOLS FOLDER / REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f13a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "28d653fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formFFQuintiles(test_df):\n",
    "    # Build quintiles\n",
    "    # -mcap is low to high preferable\n",
    "    # -while r_t_2 is high to low is preferable\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 3) &\n",
    "                (test_df.tertile_mcap_t_1 == 1), 'quintile'] = 5\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 3) &\n",
    "                (test_df.tertile_mcap_t_1 == 2), 'quintile'] = 4\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 2) &\n",
    "                (test_df.tertile_mcap_t_1 == 1), 'quintile'] = 4\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 3) &\n",
    "                (test_df.tertile_mcap_t_1 == 3), 'quintile'] = 3\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 2) &\n",
    "                (test_df.tertile_mcap_t_1 == 2), 'quintile'] = 3\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 1) &\n",
    "                (test_df.tertile_mcap_t_1 == 1), 'quintile'] = 3\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 2) &\n",
    "                (test_df.tertile_mcap_t_1 == 3), 'quintile'] = 2\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 1) &\n",
    "                (test_df.tertile_mcap_t_1 == 2), 'quintile'] = 2\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 1) &\n",
    "                (test_df.tertile_mcap_t_1 == 3), 'quintile'] = 1\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "08afcf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formFFPortfolioResults(quintile_df, sheet_name):\n",
    "    # Calculate value-weighted average returns for each quintile\n",
    "    quintile_df['mcap_sum']     = quintile_df.groupby(['date', 'quintile'])['mcap_t_1'].transform('sum')\n",
    "    quintile_df['weight']       = quintile_df.mcap_t_1 / quintile_df.mcap_sum\n",
    "    quintile_df['quintile_r_t'] = quintile_df.weight * quintile_df.r_t\n",
    "    quintile_df['quintile_r_t'] = quintile_df.groupby(['date', 'quintile'])['quintile_r_t'].transform('sum')\n",
    "    results_df = quintile_df[['quintile', 'quintile_r_t']].drop_duplicates()\n",
    "\n",
    "    # Form the output table\n",
    "    output_df = pd.DataFrame(data = {'quintile': [1, 2, 3, 4, 5]})\n",
    "    for quintile in [1, 2, 3, 4, 5]:\n",
    "        weekly_returns = results_df[results_df.quintile == quintile].quintile_r_t.values\n",
    "        output_df.loc[output_df.quintile == quintile, 'Real'] = np.product(weekly_returns+1)**(1/52)-1\n",
    "        output_df.loc[output_df.quintile == quintile, 'Std'] = np.std(weekly_returns)\n",
    "        output_df.loc[output_df.quintile == quintile, 'SR'] = np.sqrt(52)*np.mean(weekly_returns)/np.std(weekly_returns)\n",
    "\n",
    "    # Output to Excel without overwriting the file\n",
    "    book          = load_workbook('../4-output/portfolio_results.xlsx')\n",
    "    writer        = pd.ExcelWriter('../4-output/portfolio_results.xlsx', engine='openpyxl') \n",
    "    writer.book   = book\n",
    "    writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "    output_df.to_excel(writer, sheet_name=sheet_name)\n",
    "    writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a9fa9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formQuintiles(test_df): \n",
    "    test_df = test_df.sort_values(by=['date', 'y_hat_t'])\n",
    "    test_df['ranking'] = test_df.groupby(['date']).cumcount()+1\n",
    "    test_df['counts'] = 1\n",
    "    test_df['coins_per_week'] = test_df.groupby(['date']).counts.sum()\n",
    "    test_df['ranking'] = test_df.ranking / test_df.coins_per_week\n",
    "    test_df.loc[test_df.ranking <= 0.2, 'quintile'] = 1\n",
    "    test_df.loc[(test_df.ranking > 0.2) & (test_df.ranking <= 0.4), 'quintile'] = 2\n",
    "    test_df.loc[(test_df.ranking > 0.4) & (test_df.ranking <= 0.6), 'quintile'] = 3\n",
    "    test_df.loc[(test_df.ranking > 0.6) & (test_df.ranking <= 0.8), 'quintile'] = 4\n",
    "    test_df.loc[(test_df.ranking > 0.8), 'quintile'] = 5\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3c56ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPortfolioResults(quintile_df, sheet_name):\n",
    "    # Calculate equal-weighted average returns for each quintile\n",
    "    quintile_df['quintile_y_hat_t'] = quintile_df.groupby(['date', 'quintile'])['y_hat_t'].transform('mean')\n",
    "    quintile_df['quintile_r_t'] = quintile_df.groupby(['date', 'quintile'])['r_t'].transform('mean')\n",
    "    results_df = quintile_df[['quintile', 'quintile_y_hat_t', 'quintile_r_t']].drop_duplicates()\n",
    "\n",
    "    # Form the output table\n",
    "    output_df = pd.DataFrame(data = {'quintile': [1, 2, 3, 4, 5]})\n",
    "    for quintile in [1, 2, 3, 4, 5]:\n",
    "        pred_returns = results_df[results_df.quintile == quintile].quintile_y_hat_t.values\n",
    "        weekly_returns = results_df[results_df.quintile == quintile].quintile_r_t.values\n",
    "        output_df.loc[output_df.quintile == quintile, 'Pred'] = np.product(pred_returns+1)**(1/52)-1\n",
    "        output_df.loc[output_df.quintile == quintile, 'Real'] = np.product(weekly_returns+1)**(1/52)-1\n",
    "        output_df.loc[output_df.quintile == quintile, 'Std'] = np.std(weekly_returns)\n",
    "        output_df.loc[output_df.quintile == quintile, 'SR'] = np.sqrt(52)*np.mean(weekly_returns)/np.std(weekly_returns)\n",
    "\n",
    "    # Output to Excel without overwriting the file\n",
    "    book          = load_workbook('../4-output/portfolio_results.xlsx')\n",
    "    writer        = pd.ExcelWriter('../4-output/portfolio_results.xlsx', engine='openpyxl') \n",
    "    writer.book   = book\n",
    "    writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "    output_df.to_excel(writer, sheet_name=sheet_name)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4be58a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputBenchmarks(test_df):\n",
    "    # Calculate equal weighted return statistics\n",
    "    eql_wght_weekly_returns = test_df.groupby('date').r_t.mean().values\n",
    "    eql_wght_real = np.product(eql_wght_weekly_returns+1)**(1/52)-1\n",
    "    eql_wght_std = np.std(eql_wght_weekly_returns)\n",
    "    eql_wght_sr = np.sqrt(52)*np.mean(eql_wght_weekly_returns)/eql_wght_std\n",
    "\n",
    "    # Calculate mcap weighted return statistics\n",
    "    test_df['mcap_sum'] = test_df.groupby(['date'])['mcap_t_1'].transform('sum')\n",
    "    test_df['weight']   = test_df.mcap_t_1 / test_df.mcap_sum\n",
    "    test_df['mcap_r_t'] = test_df.weight * test_df.r_t\n",
    "    test_df['mcap_r_t'] = test_df.groupby(['date'])['mcap_r_t'].transform('sum')\n",
    "    mcap_wght_weekly_returns = test_df[['mcap_r_t']].drop_duplicates().mcap_r_t.values\n",
    "    mcap_wght_real = np.product(mcap_wght_weekly_returns+1)**(1/52)-1\n",
    "    mcap_wght_std = np.std(mcap_wght_weekly_returns)\n",
    "    mcap_wght_sr = np.sqrt(52)*np.mean(mcap_wght_weekly_returns)/mcap_wght_std\n",
    "\n",
    "    # Form output dataframe\n",
    "    output_df = pd.DataFrame(data={'weights': ['equal', 'mcap']})\n",
    "    output_df.loc[output_df.weights == 'equal', 'Real'] = eql_wght_real \n",
    "    output_df.loc[output_df.weights == 'equal', 'Std'] = eql_wght_std\n",
    "    output_df.loc[output_df.weights == 'equal', 'SR'] = eql_wght_sr\n",
    "    output_df.loc[output_df.weights == 'mcap', 'Real'] = mcap_wght_real \n",
    "    output_df.loc[output_df.weights == 'mcap', 'Std'] = mcap_wght_std\n",
    "    output_df.loc[output_df.weights == 'mcap', 'SR'] = mcap_wght_sr\n",
    "\n",
    "    # Output to Excel without overwriting the file\n",
    "    book          = load_workbook('../4-output/portfolio_results.xlsx')\n",
    "    writer        = pd.ExcelWriter('../4-output/portfolio_results.xlsx', engine='openpyxl') \n",
    "    writer.book   = book\n",
    "    writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "    output_df.to_excel(writer, sheet_name='benchmarks')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "b80ec612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FF and CA yhats\n",
    "pik_ff = '../3-data/clean/ff-rankings-returns.pkl' \n",
    "with open(pik_ff, \"rb\") as f:\n",
    "    ff_data_in = pickle.load(f)\n",
    "    \n",
    "ff_yhats_df, ff_test_df, ff_return_df = ff_data_in\n",
    "\n",
    "pik_ca = '../3-data/clean/autoencoders-yhats-returns.pkl' \n",
    "with open(pik_ca, \"rb\") as f:\n",
    "    ca_data_in = pickle.load(f)\n",
    "    \n",
    "opt_hps_list, test_dfs_list, returns_dfs_list = ca_data_in\n",
    "\n",
    "# Import other benchmarks\n",
    "\n",
    "# TODO: CMC 200\n",
    "# TODO: BTC\n",
    "# TODO: ETH\n",
    "# TODO: S&P 500\n",
    "\n",
    "# Output FF results\n",
    "test_df = ff_test_df.copy()\n",
    "quintile_df = formFFQuintiles(test_df)\n",
    "formFFPortfolioResults(quintile_df, sheet_name = 'raw_ff')\n",
    "\n",
    "# Output autoencoder results\n",
    "for i in range(len(test_dfs_list)):\n",
    "    test_df = test_dfs_list[i]\n",
    "    opt_hps = opt_hps_list[i]\n",
    "    num_hidden_layer = opt_hps['number_hidden_layer']\n",
    "    num_factor = opt_hps['number_factor']\n",
    "    sheet_name = 'raw_autoencoder-hl_' + str(num_hidden_layer) + '-fac_' + str(num_factor)\n",
    "    \n",
    "    quintile_df = formQuintiles(test_df)\n",
    "    formPortfolioResults(quintile_df, sheet_name)\n",
    "\n",
    "# Output benchmarks\n",
    "outputBenchmarks(ff_test_df)\n",
    "\n",
    "# TODO: ENSURE I REPORT THE RETURN AND SHARPE AND OTHER METRICS FOR ALL BENCHMARKS:\n",
    "# -FF, CA, CMC 200, EQUAL WEIGHTS FROM SAME UNIVERSE, MCAP WEIGHTS FROM SAME UNIVERSE, BTC, ETH, S&P 500\n",
    "\n",
    "# Report out for OOS: (maybe do some of these in a separate script?)\n",
    "# --return weighted by mcap and equal weights\n",
    "# --sharpe for both equal and mcap weights\n",
    "# --source of excess return e.g. distri of each week-asset holding return with naming top returns asset-weeks\n",
    "# --max DD\n",
    "# --fees\n",
    "# --min/Q1-Q3/max portfolio weight each week plotted\n",
    "# --number of transactions per week\n",
    "# --portfolio turnover\n",
    "# --portfolio return per month\n",
    "# --fees per week/month/overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
