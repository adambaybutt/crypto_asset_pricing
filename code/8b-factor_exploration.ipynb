{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE WITH NEW PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c5e8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "import statsmodels.formula.api as smf\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c20002b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def data_preprocessing_table2(df, factors, ast, var = ['week_idx'], index ='week_idx', max_index = 262):\n",
    "    '''\n",
    "    Data preprocessing of table 2: \n",
    "        keep the specified asset, factors, other variables (index by default);\n",
    "        calculate delta of factors;\n",
    "        keep the data within a date range.\n",
    "    \n",
    "    Keyword arguments:\n",
    "        df -- raw dataframe\n",
    "        factors -- list of factors to inspect\n",
    "        ast -- asset to explore\n",
    "        var -- other variables to keep\n",
    "        index -- index column\n",
    "        max_index -- cutoff of the specified index (week_idx of the last datapoint of 2020 is 262)\n",
    "    \n",
    "    Return value:\n",
    "        df_ast -- preprocessed dataframe\n",
    "        factors_pct -- list of strings of the new columns that are pct change in factors \n",
    "    '''\n",
    "    \n",
    "    # only keep the specified asset's rows in the panel data\n",
    "    # i.e. it is now just timeseries data\n",
    "    df_ast = df[df['asset'] == ast]\n",
    "\n",
    "    # only keep specified factors and variables\n",
    "    colns = var + factors\n",
    "    df_ast = df_ast[colns]\n",
    "\n",
    "    # calculate factor pct change\n",
    "    factors_pct = []\n",
    "    for factor in factors:\n",
    "        factor_pct = 'pct_'+factor\n",
    "        df_ast[factor_pct] = df_ast[factor].pct_change()\n",
    "        factors_pct.append(factor_pct)\n",
    "    \n",
    "    # drop the first row (NaN diff)\n",
    "    df_ast = df_ast.dropna()\n",
    "    \n",
    "    # drop original factor colns\n",
    "    df_ast = df_ast.drop(factors, axis=1)\n",
    "\n",
    "    # only keep data from 2016 to 2020 (drop week_idx >= 262)\n",
    "    df_ast = df_ast[df_ast[index] < max_index]\n",
    "\n",
    "    return df_ast, factors_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb8fb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_principal_component(df, var, ncomponents = 1, pc_col_name = 'pca'):\n",
    "    \n",
    "    '''\n",
    "    Generate a column of the principle component among var and add the column to the dataframe\n",
    "    \n",
    "    Keyword arguments:\n",
    "        df -- input dataframe\n",
    "        var -- list of variables to inspect\n",
    "        ncomponents -- number of components of PCA\n",
    "        pc_col_name -- name of the PC column\n",
    "        \n",
    "    Return value:\n",
    "        df -- dataframe with the PC column\n",
    "    '''\n",
    "    \n",
    "    # pca\n",
    "    X = df[var]\n",
    "    pca = decomposition.PCA(n_components=ncomponents)\n",
    "    res = pca.fit_transform(X)\n",
    "\n",
    "    # add pc to dataframe\n",
    "    df[pc_col_name] = res\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "418a7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_2_panel_a(df, factors, ast, index = 'week_idx', max_index = 262):\n",
    "    \n",
    "    '''\n",
    "    Generate Panel A of Table 2\n",
    "    \n",
    "    Keyword arguments: \n",
    "        df -- raw dataframe\n",
    "        factors -- list of factors to explore\n",
    "        ast -- asset of interest\n",
    "        index -- index column\n",
    "        max_index -- cutoff of the specified index (week_idx of the last datapoint of 2020 is 262)\n",
    "    \n",
    "    Return value:\n",
    "        df_corr -- correlation matrix of delta_factors and their PC\n",
    "    '''\n",
    "    \n",
    "    # DATA PREPROCESSING\n",
    "    df_ast, factors_pct = data_preprocessing_table2(df, factors, ast)\n",
    "    \n",
    "    # GENERATE PRINCIPAL COMPONENTS\n",
    "    df_ast = generate_principal_component(df_ast, factors_pct)\n",
    "    \n",
    "    # GENERATE CORRELATION MATRIX\n",
    "    # calculate correlation matrix without the index col\n",
    "    df_corr = df_ast.drop(columns = index).corr()\n",
    "\n",
    "    # drop redundant values in the matrix\n",
    "    df_corr = df_corr.drop(columns = ['pca'])\n",
    "    r = 0\n",
    "    for ind in df_corr.index:\n",
    "        if r == 0 or r == len(df_corr.index)-1:\n",
    "            r += 1\n",
    "            continue\n",
    "        for c in range(r):\n",
    "            if c < r:\n",
    "                df_corr.iloc[r,c] = float(\"NAN\")\n",
    "        r += 1\n",
    "\n",
    "    return df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc2e55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_2_panel_b(df, predictors, response, ast, index = 'week_idx', max_index = 262):\n",
    "    '''\n",
    "    Generate Panel B of Table 2\n",
    "    \n",
    "    Keyword arguments: \n",
    "        df -- raw dataframe\n",
    "        predictors -- list of predictors to inspect; not really predicting, it is same time period\n",
    "        response -- response variable\n",
    "        ast -- asset to inspect\n",
    "        index -- index column\n",
    "        max_index -- cutoff of the specified index (week_idx of the last datapoint of 2020 is 262)\n",
    "    \n",
    "    Return value:\n",
    "        df_res -- regression results\n",
    "    '''\n",
    "    \n",
    "    # DATA PREPROCESSING\n",
    "    var = [index, response]\n",
    "    df_ast, predictors = data_preprocessing_table2(df, predictors, ast, var)\n",
    "        \n",
    "    # GENERATE PRINCIPAL COMPONENTS\n",
    "    df_ast = generate_principal_component(df_ast, predictors)\n",
    "    predictors.append('pca')\n",
    "    \n",
    "    # GENERATE RESULT DF\n",
    "    # rows\n",
    "    res_index = []\n",
    "    for predictor in predictors:\n",
    "        res_index.append(predictor+\"_coef\")\n",
    "        res_index.append(predictor+\"_t_stat\")\n",
    "    res_index.append(\"R_squared\")\n",
    "    # columns\n",
    "    res_col = range(1, 1+len(predictors))\n",
    "    # create empty df\n",
    "    df_res = pd.DataFrame(np.nan, index=res_index, columns=res_col)\n",
    "    \n",
    "    # REGRESSIONS\n",
    "    # run regression for each predictor and fill in the panel\n",
    "    r = 0\n",
    "    for predictor in predictors:\n",
    "        fml = \"\" + response + \" ~ \" + predictor\n",
    "        model = smf.ols(formula = fml, data = df_ast)\n",
    "        results = model.fit(cov_type = 'HC1')\n",
    "        \n",
    "        # TODO: CONFIRM THIS IS CORRECT STANDARD ERROR TO USE.\n",
    "\n",
    "        coef     = results.params[1]\n",
    "        rsquared = results.rsquared\n",
    "        tstat    = results.tvalues[1]\n",
    "        pval     = results.pvalues[1]\n",
    "        \n",
    "        # formatting\n",
    "        if(pval<=0.01):\n",
    "            coef = str(coef)+\"***\"\n",
    "        elif(pval <= 0.05):\n",
    "            coef = str(coef)+\"**\"\n",
    "        elif(pval <= 0.1):\n",
    "            coef = str(coef)+\"*\"\n",
    "        else:\n",
    "            coef = str(coef)\n",
    "            \n",
    "        tstat = \"(\"+str(tstat)+\")\"\n",
    "        \n",
    "        # fill results into df_res\n",
    "        df_res.iloc[r, int(r/2)] = coef\n",
    "        df_res.iloc[r+1, int(r/2)] = tstat\n",
    "        df_res.iloc[len(df_res.index)-1, int(r/2)] = rsquared\n",
    "        \n",
    "        r += 2\n",
    "        \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69610184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_3(df, predictor, responses, ast, maxlag = 8, index = 'week_idx', max_index = 262):\n",
    "    '''\n",
    "    Generate Panel 3\n",
    "    \n",
    "    Keyword arguments: \n",
    "        df -- raw dataframe\n",
    "        predictor -- string of predictor variable is the return of the asset class\n",
    "        responses -- list of LHS response variables, which are the covariates\n",
    "        ast -- asset to inspect\n",
    "        maxlag -- the maximum lag \n",
    "        index -- index column\n",
    "        max_index -- cutoff of the specified index (week_idx of the last datapoint of 2020 is 262)\n",
    "    \n",
    "    Return value:\n",
    "        df_res -- regression results\n",
    "    '''\n",
    "    \n",
    "    # DATA PREPROCESSING\n",
    "    # only keep the specified asset and needed columns\n",
    "    df_ast  = df[df['asset'] == ast]\n",
    "    columns = [index, predictor]+responses\n",
    "    df_ast  = df_ast[columns]\n",
    "\n",
    "    # create the shifted response variables\n",
    "    responses_shifted = []\n",
    "    for response in responses:\n",
    "        for shift in range(1, maxlag+1):\n",
    "            newcol = 'pct_'+response+'plus'+str(shift)\n",
    "            responses_shifted.append(newcol)\n",
    "            df_ast[newcol] = df_ast[response].pct_change().shift(-shift)\n",
    "\n",
    "    # Drop unshifted cols, rows with missing, data beyond index, and reindex\n",
    "    df_ast = df_ast.drop(responses, axis=1)\n",
    "    df_ast = df_ast.dropna()\n",
    "    df_ast = df_ast[df_ast[index] < max_index]\n",
    "    df_ast = df_ast.reset_index(drop=True)\n",
    "\n",
    "    # INITIALIZE RESULT DF\n",
    "    responses_col = []\n",
    "    for response in responses:\n",
    "        responses_col += list(np.repeat(response, 5))\n",
    "    stat_col     = []\n",
    "    stats_list   = ['cmkt_coef', 'cmkt_t_stat',\n",
    "                    'cons_coef', 'cons_t_stat',\n",
    "                    'r_squared']\n",
    "    stats_col    = stats_list*len(responses)\n",
    "    df_res       = pd.DataFrame(data={'response': responses_col,\n",
    "                                      'stat': stats_col})\n",
    "    for i in range(1,maxlag+1):\n",
    "        df_res[i] = np.nan\n",
    "    \n",
    "    # ADD STATS TO RESULTS DATA FRAME\n",
    "    for response in responses: \n",
    "        for shift in range(1,maxlag+1):\n",
    "            # fit regression\n",
    "            response_shifted = 'pct_'+response+'plus'+str(shift)\n",
    "            fml = \"\" + response_shifted + \" ~ \" + predictor\n",
    "            model = smf.ols(formula = fml, data = df_ast)\n",
    "            results = model.fit(cov_type = 'HC1')\n",
    "\n",
    "            # extract statistics\n",
    "            for i in [1,0]:\n",
    "                if i==1:\n",
    "                    param = 'cmkt'\n",
    "                elif i==0:\n",
    "                    param = 'cons'\n",
    "                coef  = results.params[i]\n",
    "                tstat = results.tvalues[i]\n",
    "                pval  = results.pvalues[i]\n",
    "\n",
    "                # formatting\n",
    "                if(pval<=0.01):\n",
    "                    coef = str(coef)+\"***\"\n",
    "                elif(pval <= 0.05):\n",
    "                    coef = str(coef)+\"**\"\n",
    "                elif(pval <= 0.1):\n",
    "                    coef = str(coef)+\"*\"\n",
    "                else:\n",
    "                    coef = str(coef)\n",
    "\n",
    "                tstat = \"(\"+str(tstat)+\")\"\n",
    "\n",
    "                # fill results into df_res\n",
    "                df_res.loc[(df_res.response==response) & \n",
    "                           (df_res.stat==(param+'_coef')), shift]   = coef\n",
    "                df_res.loc[(df_res.response==response) & \n",
    "                           (df_res.stat==(param+'_t_stat')), shift] = tstat\n",
    "\n",
    "            # extract r^2 stat\n",
    "            df_res.loc[(df_res.response==response) & \n",
    "                       (df_res.stat=='r_squared'), shift] = results.rsquared\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ae61183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE FOR FRANCESCO TO DELETE ONCE SCRIPT IS COMPLETE\n",
    "# -identifying columns are date/week_idx + asset\n",
    "# -any column that starts with \"macro_\" is a variable that is constant across assets WITHIN week\n",
    "# -macro_mcap_t is the total market cap of crypto at that date\n",
    "# -macro_mcap_ret_t is the return on the total market cap of crypto from last week to that date\n",
    "# -r_tplus7 is the return for that asset over the SUBSEQUENT week\n",
    "# -all columns that start with \"covar_\" are our covariates or RHS variables that we are interested in \n",
    "#  exploring if they have explanatory power for the cross-section of returns, i.e. r_tplus7\n",
    "\n",
    "# -we do not want to look at 2021 data as later on this project we are using that as\n",
    "#  out of sample data to check the performance of models; so we dont want to know\n",
    "#  about relations in that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f3d0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # READ IN THE DATA\n",
    "    df = pd.read_csv('liu_panel.csv')\n",
    "    \n",
    "    # FORM GROUPS OF FEATURES\n",
    "\n",
    "    # drop non feature columns\n",
    "    all_cols = list(df.columns.values)\n",
    "    for col in ['Unnamed: 0',\n",
    "                'week_idx',\n",
    "                'date',\n",
    "                'asset',\n",
    "                'macro_mcap_t',\n",
    "                'macro_mcap_ret_t',\n",
    "                'r_tplus7']:\n",
    "        all_cols.remove(col)\n",
    "\n",
    "    # remove columns we are not interested in\n",
    "    for col in all_cols:\n",
    "        if 'covar_mcap' in col:\n",
    "            all_cols.remove(col)\n",
    "        if 'macro_med' in col:\n",
    "            all_cols.remove(col)\n",
    "\n",
    "\n",
    "    # FRANCESCO TODO\n",
    "    # form groups of column names by finding the below strings in the column names\n",
    "    # e.g. list(df.filter(regex=REPLACE_THIS_WITH_STRING_TO_FIND, axis=1).columns.values)\n",
    "    # e.g.:\n",
    "    # token_age_feats = list(df.filter(regex='age_', axis=1).columns.values)\n",
    "    # token_age_feats += list(df.filter(regex='dormant_circulation', axis=1).columns.values)\n",
    "\n",
    "    # group1: token_age_feats\n",
    "    # age_\n",
    "    # dormant_circulation\n",
    "\n",
    "    # group2: token_activity_feats\n",
    "    # payments_\n",
    "    # active_addresses\n",
    "    # _transaction_\n",
    "    # velocity\n",
    "    # circulation\n",
    "\n",
    "    # group3: dev_feats\n",
    "    # github\n",
    "    # _dev_activ_\n",
    "\n",
    "    # group4: rank_feats\n",
    "    # _alexa_rank_\n",
    "    # _rank_\n",
    "\n",
    "    # group5: sentiment_feats\n",
    "    # _sentiment_\n",
    "    # _san_sent_\n",
    "\n",
    "    # group6: social_feats\n",
    "    # _social_\n",
    "    #_twitter_\n",
    "\n",
    "    # group7: price_feats\n",
    "    # price_\n",
    "    # _p_\n",
    "\n",
    "    # group8: return_feats\n",
    "    # kurt\n",
    "    # _r_\n",
    "    # _vol_\n",
    "\n",
    "    # group9: volume_feats\n",
    "    # _volume_\n",
    "\n",
    "    # group10: rv_feats\n",
    "    # _realized_value_\n",
    "    # mvrv_\n",
    "    # nvt_\n",
    "\n",
    "    # group11: exchange_feats\n",
    "    # num_market_pairs\n",
    "    # _active_cryptos_\n",
    "    # _active_ex_\n",
    "    # _ex_pairs_\n",
    "    # _ex_volume_\n",
    "\n",
    "    # group11: defi_feats\n",
    "    # _cex_to_dex_flow_\n",
    "    # _dex_to_defi_\n",
    "    # _ex_to_defi_flow_\n",
    "    # _traders_to_defi_\n",
    "    # _whale_defi_\n",
    "    # _mcd_collat_ratio_\n",
    "    # _whale_to_defi_\n",
    "\n",
    "    # group12: eth_feats\n",
    "    # _eth_\n",
    "\n",
    "    # group13: btc_feats\n",
    "    # _btc_\n",
    "\n",
    "    # group14: supply_feats\n",
    "    # _supply_\n",
    "\n",
    "    # group15: fed_feats\n",
    "    # _fed_\n",
    "\n",
    "    # group16: nft_feats\n",
    "    # _nft_\n",
    "\n",
    "    # group17: stf_feats\n",
    "    # stock_to_flow_\n",
    "\n",
    "    # group18: usdt_feats\n",
    "    # _usdt_\n",
    "\n",
    "    # TODO: MAKE SURE THESE GROUPS COVER ALL THE COLUMNS AND ARE MUTUALLY EXCLUSIVE\n",
    "    \n",
    "    # TODO: INSERT FOR LOOP OVER THE FEATURE GROUP TO SET FACTORS EQUAL TO EACH OF THE FEATURE LISTS\n",
    "    #       AND THEN RUN THE BELOW 6 LINES\n",
    "    generate_table_2_panel_a(df, factors, 'bitcoin').to_csv(r'./btc_t2pa.csv')\n",
    "    generate_table_2_panel_b(df, factors, 'macro_mcap_ret_t', 'bitcoin').to_csv(r'./btc_t2pb.csv') \n",
    "    generate_table_3(df, 'macro_mcap_ret_t', factors, 'bitcoin').to_csv(r'./btc_t3.csv')\n",
    "\n",
    "    generate_table_2_panel_a(df, factors, 'ethereum').to_csv(r'./eth_t2pa.csv')\n",
    "    generate_table_2_panel_b(df, factors, 'macro_mcap_ret_t', 'ethereum').to_csv(r'./eth_t2pb.csv') \n",
    "    generate_table_3(df, 'macro_mcap_ret_t', factors, 'ethereum').to_csv(r'./eth_t3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
