{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b6551be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0bad2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to ensure one-to-one mapping between two ID columns\n",
    "def isOneToOne(temp_df, col1, col2):\n",
    "    first = temp_df.groupby(col1)[col2].size().sort_values().values\n",
    "    second = temp_df.groupby(col2)[col1].size().sort_values().values\n",
    "    assert(len(first) == len(second))\n",
    "    return len(first) == np.sum(first == second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1b4466d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    cw_in_fp = \"../data/derived/cmc_token_universe.pkl\"\n",
    "    panel_in_fp = \"../data/derived/cmc_price_volume_mcap_panel.pkl\"\n",
    "\n",
    "    # import data\n",
    "    cw_df = pd.read_pickle(cw_in_fp)\n",
    "    panel_df = pd.read_pickle(panel_in_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c425502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove tokens from panel\n",
    "tokens_to_remove = [770, 776, 3787, 8644, 9103]\n",
    "panel_df = panel_df[~panel_df.cmc_id.isin(tokens_to_remove)]\n",
    "\n",
    "# merge on cmc slug and drop the cmc id\n",
    "df = panel_df.merge(cw_df[['cmc_id', 'slug_cmc']],\n",
    "                    on='cmc_id',\n",
    "                    how='inner',\n",
    "                    validate='many_to_one')\n",
    "df = df.drop('cmc_id', axis=1)\n",
    "df = df[['date', 'slug_cmc', 'usd_per_token_cmc', 'usd_mcap_cmc', 'usd_volume_24h_cmc']]\n",
    "df = df.sort_values(by=['date', 'slug_cmc'], ignore_index=True)\n",
    "\n",
    "# adjust particular values\n",
    "df.loc[(df.slug_cmc=='uquid-coin') & \n",
    "       df.usd_volume_24h_cmc.isnull(), 'usd_volume_24h_cmc'] = 0\n",
    "\n",
    "# ensure no missing in the df\n",
    "assert(0==df.isnull().sum().sum())\n",
    "\n",
    "# ensure unique on key columns\n",
    "dups = df.duplicated(subset=['date', 'slug_cmc'])\n",
    "assert(~dups.any()),('there are duplicates in the data on keys date and slug_cmc')\n",
    "\n",
    "# drop more tokens manually\n",
    "# NOTES: ampleforth is a stablecoin, pax gold is a gold stablecoin, index, and wrapped tokens\n",
    "wrapped_tokens_to_drop = ['ampleforth', 'cryptoindex-com-100', 'pax-gold',\n",
    "                          'wrapped-centrifuge', 'wrapped-luna-token', 'wrapped-ncg', 'wrapped-nxm']\n",
    "df = df[~df.slug_cmc.isin(wrapped_tokens_to_drop)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4a7cd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the dates to obtain\n",
    "start_date = date(2015, 1, 1)\n",
    "end_date   = date(2022, 12, 1)\n",
    "dates = [start_date.strftime('%Y-%m-%d')]\n",
    "current_date = start_date+relativedelta(months=1)\n",
    "while current_date <= end_date:\n",
    "    dates.append(current_date.strftime('%Y-%m-%d'))\n",
    "    current_date += relativedelta(months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2780b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY VOLUME AND MCAP FILTERS\n",
    "token_universe_per_quarter = []\n",
    "\n",
    "for i in range(1,len(dates)):\n",
    "    # determine start and end dates for window\n",
    "    start_date = dates[i-1]\n",
    "    end_date   = dates[i]\n",
    "\n",
    "    # build temporary dataframe for this time period\n",
    "    temp_df = df[(df.date >= start_date) & (df.date <= end_date)].copy()\n",
    "\n",
    "    # obtain list of tokens to consider\n",
    "    assets_included = list(np.unique(temp_df[temp_df.date == start_date].slug_cmc.values))\n",
    "\n",
    "    # figure out tokens removed due to insuff data\n",
    "    # note: 28 days ensures at least 4 weeks of data \n",
    "    asset_ns_df = temp_df.groupby('slug_cmc').size()\n",
    "    assets_lost_given_insuff_data = list(asset_ns_df[asset_ns_df < 28].index.values)\n",
    "    print('Coins lost given less than 28 days of data:')\n",
    "    print(np.unique(assets_lost_given_insuff_data))\n",
    "    print('\\n')\n",
    "    for asset in assets_lost_given_insuff_data:\n",
    "        if asset in assets_included:\n",
    "            assets_included.remove(asset)   \n",
    "\n",
    "    # Figure out tokens removed due to volume threshold\n",
    "    temp_vol_df = temp_df.groupby('slug_cmc').usd_volume_24h.min()\n",
    "    assets_lost_given_insuff_vol = list(temp_vol_df[temp_vol_df < 10000].index.values)\n",
    "    print('Coins lost given less than \\$10,000 on some day:')\n",
    "    print(np.unique(assets_lost_given_insuff_vol))\n",
    "    print('\\n')\n",
    "    for asset in assets_lost_given_insuff_vol:\n",
    "        if asset in assets_included:\n",
    "            assets_included.remove(asset)\n",
    "\n",
    "    # Figure out tokens removed due to mcap threshold\n",
    "    mcap_threshold = 0.00001*temp_df[temp_df.date == date].total_market_cap_cmc.values[0]\n",
    "    print('MCAP threshold is: ' + str(round(mcap_threshold, 0)))\n",
    "    temp_mcap_df = temp_df.groupby('slug_cmc').usd_mcap.mean()\n",
    "    assets_lost_given_mcap_threshold = list(temp_mcap_df[temp_mcap_df < mcap_threshold].index.values)\n",
    "    print('Coins lost given less than mcap threshold:')\n",
    "    print(np.unique(assets_lost_given_mcap_threshold))\n",
    "    print('\\n')\n",
    "    for asset in assets_lost_given_mcap_threshold:\n",
    "        if asset in assets_included:\n",
    "            assets_included.remove(asset)\n",
    "            \n",
    "    # Figure out tokens removed due to cmc rank threshold\n",
    "    assets_below_rank_threshold = np.unique(temp_df[temp_df.rank_cmc > 1000].slug_cmc.values)\n",
    "    print('Coins removed due to being below CMC rank of 1000:')\n",
    "    for asset in assets_below_rank_threshold:\n",
    "        if asset in assets_included:\n",
    "            print(asset)\n",
    "            assets_included.remove(asset)\n",
    "    print('\\n')\n",
    "\n",
    "    # Report out new tokens ever\n",
    "    print('New tokens that we have never had are ')\n",
    "    if (date != '2016-01-01'):\n",
    "        all_tokens = []\n",
    "        for j in range(i-1,-1,-1):\n",
    "            all_tokens += token_universe_per_quarter[j]\n",
    "        print(np.unique(set(assets_included).difference(set(all_tokens))))\n",
    "    else:\n",
    "        print(np.unique(assets_included))\n",
    "    print('\\n')\n",
    "\n",
    "    # Report out tokens lost from last quarter\n",
    "    if date != '2016-01-01':\n",
    "        print('Tokens lost from last quarter are ')\n",
    "        print(set(token_universe_per_quarter[i-1]).difference(set(assets_included)))\n",
    "        print('\\n')\n",
    "\n",
    "    # Report out tokens for this quarter\n",
    "    print('This quarter\\'s tokens are:')\n",
    "    print(np.unique(assets_included))\n",
    "    print(len(assets_included))\n",
    "    print('\\n\\n\\n')\n",
    "\n",
    "    # Add tokens to list\n",
    "    token_universe_per_quarter.append(list(np.unique(assets_included)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure each asset does not appear before first date nor after last date\n",
    "cmc_ids = np.unique(panel_df.cmc_id.values)\n",
    "for cmc_id in cmc_ids:\n",
    "    print(cmc_id)\n",
    "    first_date = cw_df[cw_df.cmc_id==cmc_id].first_date_cmc.values[0]\n",
    "    last_date  = cw_df[cw_df.cmc_id==cmc_id].last_date_cmc.values[0]\n",
    "    assert(0==panel_df[(panel_df.cmc_id==cmc_id)&(panel_df.date<first_date)].shape[0])\n",
    "    assert(0==panel_df[(panel_df.cmc_id==cmc_id)&(panel_df.date>last_date)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9372272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD TOKEN UNIVERSE DICTIONARY\n",
    "asset_universe_dict = {}\n",
    "for i in range(len(dates)):\n",
    "    asset_universe_dict[dates[i]] = token_universe_per_quarter[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69475e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORE RETURNS OF EQUAL- AND MCAP- WEIGHTED PORTFOLIOS\n",
    "\n",
    "# Cut the panel down to just the assets of interest\n",
    "asset_universe_unique = list(np.unique([asset \n",
    "                                        for sublist in token_universe_per_quarter \n",
    "                                        for asset in sublist]))\n",
    "df = df[df.slug_cmc.isin(asset_universe_unique)]\n",
    "\n",
    "# Keep columns of interest\n",
    "df = df[['date', 'slug_cmc', 'usd_per_token', 'usd_mcap', 'usd_volume_24h']]\n",
    "\n",
    "# Drop rows that do not have previous day information\n",
    "df = df.sort_values(by=['slug_cmc', 'date'], ignore_index=True)\n",
    "df.loc[1:, 'day_diff'] = (df.date[1:].values - df.date[:-1]).values.astype('timedelta64[D]').astype(int)\n",
    "df['day_diff2'] = df.day_diff.shift(-1)\n",
    "num_rows = df[df.day_diff == 1].shape[0]\n",
    "df = df[(df.day_diff == 1) | (df.day_diff2 == 1)]\n",
    "assert(num_rows <= df.shape[0])\n",
    "df = df.drop(['day_diff2'], axis=1)\n",
    "\n",
    "# Calculate day over day return\n",
    "df['r_t'] = df.groupby('slug_cmc')['usd_per_token'].apply(pd.Series.pct_change)\n",
    "df = df[df.day_diff == 1]\n",
    "tokens_to_drop = np.unique(df[df.r_t.isnull()].slug_cmc.values)\n",
    "df = df[~df.slug_cmc.isin(tokens_to_drop)]\n",
    "df = df.drop('day_diff', axis=1)\n",
    "\n",
    "# Cut down to time period of interest\n",
    "df = df[df.date.dt.year >= 2016]\n",
    "df = df[df.date.dt.year <= 2021]\n",
    "\n",
    "# Ensure no missings\n",
    "assert(0 == df.isnull().sum().sum())\n",
    "\n",
    "# Clean up index and resort\n",
    "df = df.sort_values(by=['date', 'slug_cmc'], ignore_index=True)\n",
    "\n",
    "# Calculate equal and mcap weighted returns by quarter\n",
    "equal_df = pd.DataFrame()\n",
    "mcap_df  = pd.DataFrame()\n",
    "for i in range(len(dates)):\n",
    "    # Set up dates and asset universe\n",
    "    date = dates[i]\n",
    "    date_plus_3mo = datetime.strptime(date, '%Y-%m-%d') + relativedelta(months=3)\n",
    "    asset_universe = asset_universe_dict[date]\n",
    "\n",
    "    # Subset to relevant data\n",
    "    temp_df = df[(df.date >= date) & (df.date < date_plus_3mo)]\n",
    "    temp_df = temp_df[temp_df.slug_cmc.isin(asset_universe)]\n",
    "\n",
    "    # Form equal weighted returns\n",
    "    temp_eq_df = temp_df.groupby('date')[['r_t']].mean()\n",
    "    equal_df = pd.concat((equal_df, temp_eq_df))\n",
    "\n",
    "    # Form mcap weighted returns\n",
    "    temp_df['mcap_sum'] = temp_df.groupby('date')['usd_mcap'].transform('sum')\n",
    "    temp_df['mcap_weight'] = temp_df.usd_mcap / temp_df.mcap_sum\n",
    "    temp_df['mcap_r_t'] = temp_df.r_t * temp_df.mcap_weight\n",
    "    temp_mcap_df = temp_df.groupby('date')[['mcap_r_t']].sum()\n",
    "    mcap_df = pd.concat((mcap_df, temp_mcap_df))\n",
    "\n",
    "# Ensure no missing\n",
    "assert(0==equal_df.isnull().sum().values)\n",
    "assert(0==mcap_df.isnull().sum().values)\n",
    "\n",
    "# Report returns\n",
    "print('equal weighted return:')\n",
    "print(equal_df.apply(geometricAverageSimpleReturns, axis=0).values[0])\n",
    "print('sharpe:')\n",
    "print(np.mean(equal_df.r_t.values)/np.std(equal_df.r_t.values))\n",
    "print('mcap weighted return:')\n",
    "print(mcap_df.apply(geometricAverageSimpleReturns, axis=0).values[0])\n",
    "print('sharpe:')\n",
    "print(np.mean(mcap_df.mcap_r_t.values)/np.std(mcap_df.mcap_r_t.values))\n",
    "\n",
    "# Form the returns by year\n",
    "equal_df['year'] = equal_df.index.year\n",
    "mcap_df['year'] = mcap_df.index.year\n",
    "print('equal weighted return:')\n",
    "print(equal_df.groupby('year').apply(geometricAverageSimpleReturns))\n",
    "print('mcap weighted return:')\n",
    "print(mcap_df.groupby('year').apply(geometricAverageSimpleReturns))\n",
    "equal_df = equal_df.drop('year', axis=1)\n",
    "mcap_df  = mcap_df.drop('year', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save object and clear memory\n",
    "with open('../3-data/clean/asset_universe_dates_and_lists.pkl', 'wb') as f:\n",
    "    pickle.dump(asset_universe_dict, f)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVE THESE NOTES TO CLEANING\n",
    "\n",
    "# manually look through it to confirm they are legit tokens\n",
    "# or maybe give this task to jacob\n",
    "# or maybe schedule a time to do this with jacob so we 2x the speed\n",
    "\n",
    "# Lets look to see if the 0.01% mcap rule is good for the entire time period\n",
    "\n",
    "# Jan 1 2015 - $5B - $500k\n",
    "# Jan 1 2016 - $7B - $700k\n",
    "# Jan 1 2017 - $18B - $1.8M\n",
    "# Jan 1 2018 - $600B - $60M\n",
    "# Apr 1 2018 - $300B - $30M\n",
    "# Jul 1 2018 - $250B - $25M\n",
    "# Jan 1 2019 - $125B - $12M\n",
    "# Apr 1 2019 - $145B - $14M\n",
    "# Jul 1 2019 - $330B - $33M\n",
    "# Oct 1 2019 - $220B - $22M\n",
    "# Jan 1 2020 - $200B - $20M\n",
    "# Apr 1 2020 - $175B - $17M\n",
    "# Jul 1 2020 - $260B - $26M\n",
    "# Oct 1 2020 - $340B - $34M\n",
    "# Jan 1 2021 - $770B - $77M\n",
    "# Apr 1 2021 - $1.9T - $190M\n",
    "# Jul 1 2021 - $1.4T - $140M\n",
    "# Oct 1 2021 - $2T - $200M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a45bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure each asset has consecutive data, interpolate where needed with forward fill\n",
    "\n",
    "# group the data by cmc_id to loop over\n",
    "grouped = panel_df.groupby('cmc_id')\n",
    "\n",
    "# interate through each cmc_id\n",
    "dfs = []\n",
    "for name, group in grouped:\n",
    "    # find the first and last dates for the current id\n",
    "    first_date = group['date'].min()\n",
    "    last_date  = group['date'].max()\n",
    "\n",
    "    # create a new dataframe with all the possible combinations of cmc_id and date\n",
    "    dates = pd.date_range(first_date, last_date)\n",
    "    index = pd.MultiIndex.from_product([[name], dates], names=['cmc_id', 'date'])\n",
    "    full_df = pd.DataFrame(index=index).reset_index()\n",
    "\n",
    "    # merge the full dataframe with the original dataframe to fill in missing values with NaNs\n",
    "    merged_df = pd.merge(full_df, group, on=['cmc_id', 'date'], how='left')\n",
    "\n",
    "    # interpolate the missing values using forward fill for up to 7 consecutive observations\n",
    "    interpolated_df = merged_df.fillna(method='ffill', limit=21)\n",
    "\n",
    "    # Check if there are any missing values in the remaining columns for the current id and date range\n",
    "    if interpolated_df.isnull().values.any():\n",
    "        print(f\"ID {name} has missing values in the given date range, precisely: {int(interpolated_df.isnull().sum().sum()/3)}.\")\n",
    "        break\n",
    "\n",
    "    # combine    \n",
    "    dfs.append(interpolated_df)\n",
    "\n",
    "# Combine all the dataframes and drop the 'cmc_id' index level\n",
    "result_df = pd.concat(dfs).reset_index()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
