{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ca0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE WITH NEW PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99b42fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653da35c",
   "metadata": {},
   "source": [
    "New code importing t-bill data. Use the discount-basis rate (https://fred.stlouisfed.org/series/DTB4WK) for now. 'DGS1MO.csv' is the investment-basis rate (https://fred.stlouisfed.org/series/DGS1MO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e8028ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>DTB4WK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>2020-12-21</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>2020-12-22</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>2020-12-25</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           DATE  DTB4WK\n",
       "1299 2020-12-18    0.08\n",
       "1300 2020-12-21    0.08\n",
       "1301 2020-12-22    0.06\n",
       "1302 2020-12-23    0.07\n",
       "1303 2020-12-24    0.09\n",
       "1304 2020-12-25   -1.00\n",
       "1305 2020-12-28    0.09\n",
       "1306 2020-12-29    0.08\n",
       "1307 2020-12-30    0.06\n",
       "1308 2020-12-31    0.08\n",
       "1309 2021-01-01   -1.00"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "# use discount-basis rate for now\n",
    "df_tbill_daily = pd.read_csv(\"../3-data/DTB4WK.csv\")\n",
    "# convert date to datetime type\n",
    "df_tbill_daily['DATE'] = pd.to_datetime(df_tbill_daily['DATE'])\n",
    "# map invalid values to -1\n",
    "df_tbill_daily = df_tbill_daily.replace('^\\.$', '-1', regex=True)\n",
    "df_tbill_daily['DTB4WK'] = pd.to_numeric(df_tbill_daily['DTB4WK'])\n",
    "df_tbill_daily.tail(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59aff35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rf_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>0.00130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>0.00196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>0.00208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>0.00245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-31</td>\n",
       "      <td>0.00252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     rf_t\n",
       "0 2016-01-03  0.00130\n",
       "1 2016-01-10  0.00196\n",
       "2 2016-01-17  0.00208\n",
       "3 2016-01-24  0.00245\n",
       "4 2016-01-31  0.00252"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a df of weekly rf data\n",
    "df_tbill_weekly = pd.DataFrame(columns=['date', 'rf_t'])\n",
    "\n",
    "# convert daily data to weekly\n",
    "for i in range(len(df_tbill_daily.index)): # loop over 1305 rows\n",
    "    # start processing every monday\n",
    "    if(i%5 != 0):\n",
    "        continue\n",
    "\n",
    "    #if(i+4>=len(df_tbill_daily.index)): \n",
    "    #    break\n",
    "    # 1/1/2021 is the last data point, it is -1(invalid) and it is a Friday\n",
    "\n",
    "    weekly_rate = 1\n",
    "    valid_days = 0 # count of valid days in that week\n",
    "    \n",
    "    for day in range(5): # loop over each day of the week\n",
    "        # if the value is invalid, ignore\n",
    "        if(df_tbill_daily.iat[i+day, 1] == -1): \n",
    "            continue\n",
    "        # else increment valid_days and multiply \n",
    "        valid_days += 1\n",
    "        weekly_rate = weekly_rate * (1+df_tbill_daily.iat[i+day, 1]/100)\n",
    "\n",
    "   # weekly_rate = weekly_rate**(1/valid_days)-1\n",
    "    weekly_rate -= 1 \n",
    "    sunday_date = df_tbill_daily.iat[i+day, 0] + timedelta(days=2)\n",
    "    # add a row to weekly df\n",
    "    df_tbill_weekly.loc[len(df_tbill_weekly.index)] = [sunday_date, weekly_rate]\n",
    "\n",
    "df_tbill_weekly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353aeb77",
   "metadata": {},
   "source": [
    "end of new code for t-bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f95dd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN DATA\n",
    "with open('../3-data/clean/asset_universe_dates_and_lists.pkl', 'rb') as handle:\n",
    "    asset_universe_dict = pickle.load(handle) \n",
    "df = pd.read_csv('../3-data/panel_factor_analysis.csv')\n",
    "\n",
    "# CLEAN UP CSV FILE GIVEN IT ISNT PKL\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "# LOOP OVER TOKEN UNIVERSE TO SUBSET DF TO VALID ROWS\n",
    "included_df = pd.DataFrame()\n",
    "\n",
    "# Loop over asset_universe_dict len with i\n",
    "for i in range(len(asset_universe_dict)):\n",
    "    # Extract this quarter and its included assets\n",
    "    date = list(asset_universe_dict.keys())[i]\n",
    "    assets = asset_universe_dict[date]\n",
    "\n",
    "    # Form start and end date for this window\n",
    "    start_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    end_date   = datetime.strptime(date, '%Y-%m-%d') + relativedelta(months=3)\n",
    "    \n",
    "    # Extract asset dates\n",
    "    temp_df = df[(df.asset.isin(assets)) & (df.date >= start_date) & (df.date < end_date)][['date', 'asset']]\n",
    "    included_df = pd.concat((included_df, temp_df.reset_index(drop=True)))\n",
    "\n",
    "# Keep just those included rows\n",
    "assert(included_df.shape[0] == included_df[~included_df.duplicated()].shape[0]) # ensure included df is unique\n",
    "df = df.merge(included_df,\n",
    "              on=['date', 'asset'],\n",
    "              how='inner',\n",
    "              validate='one_to_one')\n",
    "\n",
    "# ADD YEAR COLUMN AND BUILD LIST OF RHS\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "rhs_cols = df.columns.values.tolist()\n",
    "to_remove = ['date', 'asset', 'mcap_t', 'week_idx', 'r_tplus7', 'year']\n",
    "for var in to_remove:\n",
    "    rhs_cols.remove(var)   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8730b",
   "metadata": {},
   "source": [
    "Adjust returns to be excess, i.e. subtract from r_tplus7 the weekly risk free return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on a new col\n",
    "\"\"\" \n",
    "df['exr_tplus7'] = df['r_tplus7']\n",
    "col_ind = df.columns.get_loc('exr_tplus7')\n",
    "for i in range(len(df.index)):\n",
    "    rf = df_tbill_weekly.loc[df_tbill_weekly['date'] == df.iat[i, 0], 'rf_t']\n",
    "    df.iat[i, col_ind] -= rf\n",
    "\"\"\"\n",
    "# work on the original col\n",
    "col_ind = df.columns.get_loc('r_tplus7')\n",
    "for i in range(len(df.index)):\n",
    "    rf = df_tbill_weekly.loc[df_tbill_weekly['date'] == df.iat[i, 0], 'rf_t']\n",
    "    df.iat[i, col_ind] -= rf\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f377fb",
   "metadata": {},
   "source": [
    "Extra argument geoavg (0: simple time series average, 1: geometric average)\\\n",
    "Extra argument flip_tertiles that will optionally flip tertiles or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2019c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariateFactorAnalaysis(df, factor, geoavg=True, flip_tertiles=False):\n",
    "    \n",
    "    # create result df\n",
    "    res_col = ['1', '2', '3', '3-1' , 'factor', 'sort']\n",
    "    res_row = ['2016', '2017', '2018', '2019', '2020', 'all', 'all_tstat']\n",
    "    res_df = pd.DataFrame(np.nan, index=res_row, columns=res_col)\n",
    "    res_df = res_df.assign(factor=factor)\n",
    "\n",
    "    \n",
    "    # drop cols from df\n",
    "    temp_df = df[['date', 'asset', 'r_tplus7', 'mcap_t', 'year', factor]].copy()\n",
    "    \n",
    "    \n",
    "    # Form the tertiles based on given factor\n",
    "    if(flip_tertiles):\n",
    "        temp_df = temp_df.sort_values(by=['date', factor], ascending=False)\n",
    "    else:\n",
    "        temp_df = temp_df.sort_values(by=['date', factor])\n",
    "    temp_df['ranking'] = temp_df.groupby(['date']).cumcount()+1\n",
    "    temp_df['counts'] = 1\n",
    "    temp_df['assets_per_week'] = temp_df.groupby('date').counts.transform(lambda x: x.sum())\n",
    "    temp_df['ranking'] = temp_df.ranking / temp_df.assets_per_week\n",
    "    temp_df.loc[temp_df.ranking <= 1/3, 'tertile'] = 1\n",
    "    temp_df.loc[(temp_df.ranking > 1/3) & (temp_df.ranking <= 2/3), 'tertile'] = 2\n",
    "    temp_df.loc[(temp_df.ranking > 2/3) & (temp_df.ranking <= 1), 'tertile'] = 3\n",
    "\n",
    "    \n",
    "    # Calculate value-weighted average returns for each tertile\n",
    "    temp_df['mcap_sum']    = temp_df.groupby(['date', 'tertile'])['mcap_t'].transform('sum')\n",
    "    temp_df['weight']      = temp_df.mcap_t / temp_df.mcap_sum\n",
    "    temp_df['tertile_r_tplus7'] = temp_df.weight * temp_df.r_tplus7\n",
    "    temp_df['tertile_r_tplus7'] = temp_df.groupby(['date', 'tertile'])['tertile_r_tplus7'].transform('sum')\n",
    "    temp_df.drop_duplicates(['date', 'tertile'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Determine if factor is pos or neg corr with returns\n",
    "    long_short_diff_overall = (np.mean(temp_df[temp_df.tertile==3].tertile_r_tplus7.values) \n",
    "                               - np.mean(temp_df[temp_df.tertile==1].tertile_r_tplus7.values))\n",
    "    if long_short_diff_overall > 0:\n",
    "        res_df['sort'] = '1_low_to_3_high'\n",
    "    else:\n",
    "        temp_df.loc[temp_df.tertile == 3, 'tertile'] = 4 # change to something else first\n",
    "        temp_df.loc[temp_df.tertile == 1, 'tertile'] = 3\n",
    "        temp_df.loc[temp_df.tertile == 4, 'tertile'] = 1\n",
    "        res_df['sort'] = '1_high_to_3_low'\n",
    "\n",
    "    \n",
    "    # Take the geometric average (and t_stat) of tertiles 1-3\n",
    "    # by year\n",
    "    for year in [2016, 2017, 2018, 2019, 2020]: \n",
    "        year_df = temp_df[temp_df['year'] == year]\n",
    "        for t in [1, 2, 3]:\n",
    "            tertile_df = year_df[year_df['tertile'] == t]\n",
    "            tertile_simple_returns = tertile_df.tertile_r_tplus7.values\n",
    "            mean = 0\n",
    "            if(geoavg): # geometric mean\n",
    "                gmean = np.prod(tertile_simple_returns+1)**(1/len(tertile_simple_returns))-1\n",
    "                mean = gmean\n",
    "            else: # arithmetic mean\n",
    "                amean = np.average(tertile_simple_returns+1)\n",
    "                mean = amean\n",
    "            res_df.iloc[int(year-2016),int(t-1)] = mean\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    # and overall\n",
    "    for t in [1.0, 2.0, 3.0]:\n",
    "        tertile_df = temp_df[temp_df['tertile'] == t]\n",
    "        tertile_simple_returns = tertile_df.tertile_r_tplus7.values\n",
    "        # mean\n",
    "        mean = 0\n",
    "        if(geoavg): # geometric mean\n",
    "            gmean = np.prod(tertile_simple_returns+1)**(1/len(tertile_simple_returns))-1\n",
    "            mean = gmean\n",
    "        else: # arithmetic mean\n",
    "            amean = np.average(tertile_simple_returns+1)\n",
    "            mean = amean\n",
    "        res_df.iloc[int(year-2016),int(t-1)] = mean\n",
    "        # t-stat\n",
    "        t_stat = (np.mean(tertile_simple_returns) / \n",
    "                   np.std(tertile_simple_returns)*np.sqrt(len(tertile_simple_returns)))\n",
    "        res_df.iloc[6,int(t-1)] = t_stat\n",
    "\n",
    "        \n",
    "    # Form the week by week 3-1 portfolio\n",
    "    t3_df = temp_df[temp_df['tertile'] == 3]\n",
    "    t1_df = temp_df[temp_df['tertile'] == 1]\n",
    "    t3_df.rename(columns = {'tertile_r_tplus7':'t3_r'}, inplace = True)\n",
    "    t1_df.rename(columns = {'tertile_r_tplus7':'t1_r'}, inplace = True)\n",
    "    t3_1_df = t3_df.merge(t1_df,on='date',how='inner',validate='one_to_one')\n",
    "    t3_1_df['tertile_r_3_1_t'] = t3_1_df.t3_r - t3_1_df.t1_r \n",
    "    t3_1_df = t3_1_df[['date','tertile_r_3_1_t']]\n",
    "    t3_1_df['tertile'] = '3-1'\n",
    "    t3_1_df['year'] = t3_1_df['date'].dt.year\n",
    "    \n",
    "    \n",
    "    # fix returns below -1 to just blow up the strategy with a strict -1\n",
    "    t3_1_df.loc[t3_1_df.tertile_r_3_1_t < -1, 'tertile_r_3_1_t'] = -1 \n",
    "    \n",
    "\n",
    "    # Take the average (and t_stat) of 3-1 portfolio\n",
    "    # by year\n",
    "    for year in [2016, 2017, 2018, 2019, 2020]: \n",
    "        year_df = t3_1_df[t3_1_df['year'] == year]\n",
    "        simple_returns = year_df.tertile_r_3_1_t.values\n",
    "        mean = 0\n",
    "        if(geoavg): # geometric mean\n",
    "            gmean = np.prod(simple_returns+1)**(1/len(simple_returns))-1\n",
    "            mean = gmean\n",
    "        else: # arithmetic mean\n",
    "            amean = np.average(simple_returns+1)\n",
    "            mean = amean\n",
    "        res_df.iloc[int(year-2016),3] = mean\n",
    "        \n",
    "    # overall \n",
    "    simple_returns = t3_1_df.tertile_r_3_1_t.values\n",
    "    mean = 0\n",
    "    if(geoavg): # geometric mean\n",
    "        gmean = np.prod(simple_returns+1)**(1/len(simple_returns))-1\n",
    "        mean = gmean\n",
    "    else: # arithmetic mean\n",
    "        amean = np.average(simple_returns+1)\n",
    "        mean = amean\n",
    "    res_df.iloc[5,3] = mean\n",
    "\n",
    "\n",
    "    # t-stat\n",
    "    t_stats = np.mean(simple_returns)/np.std(simple_returns)*np.sqrt(len(simple_returns))\n",
    "    res_df.iloc[6,3] = t_stats\n",
    "\n",
    "\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd507de",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "for factor in rhs_cols:\n",
    "    output_df = univariateFactorAnalaysis(df, factor)\n",
    "    final_df = pd.concat((final_df, output_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fp = '../4-output/univariate_factor_analysis.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "    final_df.to_excel(writer,sheet_name='raw_univariate')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
