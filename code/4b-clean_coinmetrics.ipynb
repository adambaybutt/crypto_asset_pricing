{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from helper_functions import Helper\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPanel(df: pd.DataFrame, asset_universe_dict: Dict[str, list]) -> tuple:\n",
    "    \"\"\" A whole lotta code that should be a bunch of separate functions to\n",
    "        clean the panel down to columnns with full data and then a macro\n",
    "        dataframe for the columns that had too many missing for the cross-\n",
    "        sectional dataframe.\n",
    "    \"\"\"\n",
    "    # Clean asset column\n",
    "    asset_universe = Helper.findUniqueAssets(asset_universe_dict)\n",
    "    df = df.rename(columns={'asset': 'asset_cm'})\n",
    "    assert 0 == df.asset_cm.isnull().sum()\n",
    "    assert len(df) == df[df.asset_cm.isin(asset_universe)].shape[0]\n",
    "\n",
    "    # Convert date column to datetime format and remove timezone information\n",
    "    assert 0 == df.date.isnull().sum()\n",
    "    assert len(df) == df[df.date.dt.minute==0].shape[0]\n",
    "    df = df.sort_values(by=['date', 'asset_cm'])\n",
    "\n",
    "    # Cut down to relevant dates between July 1, 2016 and January 2, 2023\n",
    "    df = df[(df['date'] >= '2016-07-01') & (df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # Drop any duplicates\n",
    "    df = df.drop_duplicates(subset=['date', 'asset_cm'])\n",
    "\n",
    "    # Convert all columns to float32\n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    cols.remove('asset_cm')\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # Set column order\n",
    "    cols.sort()\n",
    "    df = df[['date', 'asset_cm']+cols]\n",
    "\n",
    "    # Determine the first tradable date for each asset\n",
    "    first_tradable_dates = {}\n",
    "    for date, assets in asset_universe_dict.items():\n",
    "        for asset in assets:\n",
    "            if asset not in first_tradable_dates:\n",
    "                first_tradable_dates[asset] = date\n",
    "    first_tradable_df = pd.DataFrame(list(first_tradable_dates.items()), columns=[\"asset_cm\", \"first_tradable_date\"])\n",
    "    first_tradable_df['first_tradable_date'] = pd.to_datetime(first_tradable_df['first_tradable_date'], \n",
    "                                                            utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # Shift first tradable date back three months to form lagged covariates\n",
    "    first_tradable_df['first_tradable_date'] = first_tradable_df.first_tradable_date - pd.DateOffset(months=3)\n",
    "\n",
    "    # For each asset in my panel, drop any rows that are before the first tradable date\n",
    "    for asset in df.asset_cm.unique():\n",
    "        first_tradable_date = first_tradable_df[first_tradable_df.asset_cm==asset].first_tradable_date.values[0]\n",
    "        df = df[~((df.asset_cm==asset) & (df.date<first_tradable_date))]\n",
    "\n",
    "    # Loop over all assets to add any missing datetimes and fill missing columns\n",
    "    final_df = pd.DataFrame()\n",
    "    df = df.sort_values(by=['date', 'asset_cm'], ignore_index=True)\n",
    "    assets = list(np.unique(df.asset_cm.values))\n",
    "    for asset in assets:\n",
    "        # subset to asset of interest\n",
    "        asset_df = df[df.asset_cm==asset].copy()\n",
    "\n",
    "        # determine the date gaps\n",
    "        date_gaps = []\n",
    "        dates = asset_df.date.values\n",
    "        for i in range(1, len(dates)):\n",
    "            date_gaps.append(np.timedelta64(dates[i]-dates[i-1], 'h').astype(int))\n",
    "        \n",
    "        # determine new datetimes to add\n",
    "        indices_to_expand = [i for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32*24)]\n",
    "        num_datetime_to_add = [date_gaps[i] for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32*24)]\n",
    "        start_datetimes = dates[indices_to_expand]\n",
    "        new_datetimes = []\n",
    "        for i in range(len(start_datetimes)):\n",
    "            start_datetime = start_datetimes[i]\n",
    "            datetime_to_add = num_datetime_to_add[i]\n",
    "            for j in range(1, datetime_to_add):\n",
    "                new_datetimes.append(start_datetime+np.timedelta64(j, 'h'))\n",
    "\n",
    "        # add the new datetimes to the asset df\n",
    "        new_asset_df = pd.DataFrame(data={'date': new_datetimes})\n",
    "        new_asset_df['asset_cm'] = asset\n",
    "        asset_df = pd.concat((asset_df, new_asset_df))\n",
    "        asset_df = asset_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # drop any duplicates added\n",
    "        asset_df = asset_df.drop_duplicates(subset=['date'])\n",
    "\n",
    "        # forward fill \n",
    "        asset_df = asset_df.ffill(limit=32*24)\n",
    "\n",
    "        # if asset contains nonmissing obs in a column but starts missing, then fill first obs until nonmissing with 0\n",
    "        cols_to_fill_first_row_with_zero = list(asset_df.columns[asset_df.notna().any() & asset_df.iloc[0].isna()].values)\n",
    "        for col in cols_to_fill_first_row_with_zero:\n",
    "            first_non_missing_index = asset_df[col].first_valid_index()\n",
    "            asset_df.loc[:(first_non_missing_index-1), col] = 0\n",
    "\n",
    "        # add data to master df\n",
    "        final_df = pd.concat((final_df, asset_df))\n",
    "\n",
    "    # reset df name\n",
    "    del df\n",
    "    df = final_df.copy()\n",
    "\n",
    "    # Reset column names to snake case\n",
    "    def insert_underscore_before_numbers_and_keywords(string_list):\n",
    "        modified_list = []\n",
    "        for s in string_list:\n",
    "            s = re.sub(r'(?<=[^\\d_])(\\d)', r'_\\1', s, count=1)  # Insert underscore before the first number\n",
    "            s = re.sub(r'(?<![A-Za-z_])(usd|cnt)', r'_\\1', s, flags=re.IGNORECASE)  # Insert underscore before \"usd\" or \"cnt\"\n",
    "            modified_list.append(s)\n",
    "        return modified_list\n",
    "\n",
    "    cols = list(df.columns.values)\n",
    "    cols = cols[2:-7] # assumes a particular ordering\n",
    "\n",
    "    new_col_names = [re.sub(r'(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])', '_', s).lower() for s in cols]\n",
    "    new_col_names = insert_underscore_before_numbers_and_keywords(new_col_names)\n",
    "\n",
    "    rename_dict = {old_col: new_col for old_col, new_col in zip(cols, new_col_names)}\n",
    "    df = df.rename(columns=rename_dict)\n",
    "\n",
    "    # Ensure all columns have cm in name\n",
    "    df.columns = [col if col == 'date' or col.endswith('_cm') else col + '_cm' for col in df.columns]\n",
    "\n",
    "    # Clean missing in bid ask and form bid ask difference\n",
    "    df['usd_bid_size_cm'] = df.groupby('asset_cm')['usd_bid_size_cm'].fillna(method='ffill')\n",
    "    df['usd_ask_size_cm'] = df.groupby('asset_cm')['usd_ask_size_cm'].fillna(method='ffill')\n",
    "    df['usd_bid_size_cm'].fillna(df.groupby('date')['usd_bid_size_cm'].transform('median'), inplace=True)\n",
    "    df['usd_ask_size_cm'].fillna(df.groupby('date')['usd_ask_size_cm'].transform('median'), inplace=True)\n",
    "    assert 0 == df.usd_bid_size_cm.isnull().sum()\n",
    "    assert 0 == df.usd_ask_size_cm.isnull().sum()\n",
    "    df['usd_bid_cm'] = df.groupby('asset_cm')['usd_bid_cm'].fillna(method='ffill')\n",
    "    df['usd_ask_cm'] = df.groupby('asset_cm')['usd_ask_cm'].fillna(method='ffill')\n",
    "    df['bidask_cm'] = df.usd_ask_cm-df.usd_bid_cm\n",
    "    df['bidask_cm'].fillna(df.groupby('date')['bidask_cm'].transform('median'), inplace=True)\n",
    "    assert 0 == df.bidask_cm.isnull().sum()\n",
    "    df.loc[df.usd_bid_cm.isnull(), 'usd_bid_cm'] = df.usd_per_token_cm - df.bidask_cm/2\n",
    "    df.loc[df.usd_ask_cm.isnull(), 'usd_ask_cm'] = df.usd_per_token_cm + df.bidask_cm/2\n",
    "    assert 0 == df.usd_bid_cm.isnull().sum()\n",
    "    assert 0 == df.usd_ask_cm.isnull().sum()\n",
    "\n",
    "    # Clean price\n",
    "    df['usd_per_token_cm'] = df.groupby('asset_cm')['usd_per_token_cm'].fillna(method='ffill')\n",
    "\n",
    "    # Fix outliers in bid ask columns\n",
    "    usd_bid_cm = df['usd_bid_cm'].values\n",
    "    usd_per_token_cm = df['usd_per_token_cm'].values\n",
    "    bidask_cm = df['bidask_cm'].values\n",
    "    mask = usd_bid_cm > usd_per_token_cm\n",
    "    usd_bid_cm[mask] = usd_per_token_cm[mask] - bidask_cm[mask]/2\n",
    "    df['usd_bid_cm'] = usd_bid_cm\n",
    "    usd_ask_cm = df['usd_ask_cm'].values\n",
    "    usd_per_token_cm = df['usd_per_token_cm'].values\n",
    "    bidask_cm = df['bidask_cm'].values\n",
    "    mask = usd_ask_cm < usd_per_token_cm\n",
    "    usd_ask_cm[mask] = usd_per_token_cm[mask] + bidask_cm[mask]/2\n",
    "    df['usd_ask_cm'] = usd_ask_cm\n",
    "\n",
    "    df.loc[df.usd_ask_size_cm>1e5, 'usd_ask_size_cm'] = np.nan\n",
    "    df.loc[df.usd_bid_size_cm>1e5, 'usd_ask_size_cm'] = np.nan\n",
    "    df['usd_bid_size_cm'].fillna(df.groupby('date')['usd_bid_size_cm'].transform('max'), inplace=True)\n",
    "    df['usd_ask_size_cm'].fillna(df.groupby('date')['usd_ask_size_cm'].transform('max'), inplace=True)\n",
    "\n",
    "    df.loc[(df.bidask_cm/df.usd_per_token_cm) > 0.1, 'bidask_cm'] = np.nan\n",
    "    df['bidask_cm'].fillna(df.groupby('date')['bidask_cm'].transform('median'), inplace=True)\n",
    "\n",
    "    df.loc[(df.usd_bid_cm/df.usd_per_token_cm) < 0.9, 'usd_bid_cm'] = np.nan\n",
    "    df.loc[df.usd_bid_cm.isnull(), 'usd_bid_cm'] = (df.loc[df.usd_bid_cm.isnull(), 'usd_per_token_cm']\n",
    "        -df.loc[df.usd_bid_cm.isnull(), 'bidask_cm']/2)\n",
    "\n",
    "    df.loc[(df.usd_ask_cm/df.usd_per_token_cm) > 1.1, 'usd_ask_cm'] = np.nan\n",
    "    df.loc[df.usd_ask_cm.isnull(), 'usd_ask_cm'] = (df.loc[df.usd_ask_cm.isnull(), 'usd_per_token_cm']\n",
    "        +df.loc[df.usd_ask_cm.isnull(), 'bidask_cm']/2)\n",
    "\n",
    "    assert 0 == df.bidask_cm.isnull().sum()\n",
    "    assert 0 == df.usd_bid_cm.isnull().sum()\n",
    "    assert 0 == df.usd_ask_cm.isnull().sum()\n",
    "    assert 0 == df.usd_bid_size_cm.isnull().sum()\n",
    "    assert 0 == df.usd_ask_size_cm.isnull().sum()\n",
    "\n",
    "    # Create separate dataframes\n",
    "    btc_cols = ['date',    \n",
    "        \"adr_act_cnt_cm\", \"adr_act_rec_cnt_cm\", \"adr_act_sent_cnt_cm\",\n",
    "        \"adr_bal_cnt_cm\", \"cap_act_1yr_usd_cm\",\n",
    "        \"cap_mvrv_cur_cm\", \"cap_mrkt_ffusd_cm\",\n",
    "        \"cap_real_usd_cm\", \"diff_mean_cm\",\n",
    "        \"fee_med_usd_cm\", \"fee_rev_pct_cm\", \"fee_tot_usd_cm\",\n",
    "        \"flow_in_ex_usd_cm\", \"flow_out_ex_usd_cm\",\n",
    "        \"flow_miner_net_0hop_all_usd_cm\", \"hash_rate_cm\",\n",
    "        \"iss_tot_usd_cm\", 'mcrc_cm', 'mctc_cm', 'momr_cm', \n",
    "        'mri_0hop_all30d_cm', 'ndf_cm', \"nvt_adj_ff_cm\", \n",
    "        \"puell_mul_rev_cm\", \"puell_mul_tot_cm\", \"rvt_adj_cm\",\n",
    "        \"rev_hash_usd_cm\", \"rev_usd_cm\",\n",
    "        \"ser_cm\", \"sopr_cm\", \"sply_act_ever_cm\",\n",
    "        \"sply_act_10yr_cm\", \"sply_act_5yr_cm\",\n",
    "        \"sply_act_1yr_cm\", \"sply_act_180d_cm\",\n",
    "        \"sply_act_30d_cm\", \"sply_act_7d_cm\",\n",
    "        \"sply_act_1d_cm\", \"sply_act_pct_1yr_cm\",\n",
    "        \"sply_adr_bal_usd_1_cm\", \"sply_adr_bal_usd_100_cm\",\n",
    "        \"sply_adr_bal_usd_10k_cm\", \"sply_adr_bal_usd_1m_cm\",\n",
    "        \"sply_adr_top_100_cm\", \"sply_adr_top_1pct_cm\",\n",
    "        \"sply_ex_usd_cm\", \"sply_cur_cm\", \"sply_ff_cm\", \n",
    "        \"sply_exp_fut_10yr_cm\", \"sply_miner_0hop_all_usd_cm\",\n",
    "        \"sply_rvv_5yr_cm\", \"sply_rvv_1yr_cm\", \"sply_rvv_180d_cm\", \n",
    "        \"sply_rvv_30d_cm\", \"sply_rvv_7d_cm\", \"sply_utxo_loss_cm\", \n",
    "        \"sply_utxo_prof_cm\", \"tx_tfr_cnt_cm\", \n",
    "        \"tx_tfr_val_adj_usd_cm\", \"tx_tfr_val_day_dst_cm\",\n",
    "        \"tx_tfr_val_med_usd_cm\", \"tx_tfr_val_usd_cm\",\n",
    "        \"utxo_age_med_cm\", \"utxo_prof_unreal_usd_cm\",\n",
    "        \"utxo_loss_unreal_usd_cm\", \"vel_act_1yr_cm\", \"vel_cur_1yr_cm\"]\n",
    "    btc_df = df[df.asset_cm=='btc'][btc_cols].reset_index(drop=True)\n",
    "\n",
    "    eth_cols = ['date', \"adr_act_cnt_cm\", \"adr_act_cont_cnt_cm\",\n",
    "        \"adr_act_rec_cnt_cm\", \"adr_act_sent_cnt_cm\",\n",
    "        \"adr_bal_cnt_cm\", \"cap_act_1yr_usd_cm\",\n",
    "        \"cap_mvrv_cur_cm\", \"cap_mrkt_ffusd_cm\",\n",
    "        \"cap_real_usd_cm\", \"cont_erc_20_cnt_cm\",\n",
    "        \"fee_med_usd_cm\", \"fee_rev_pct_cm\",\n",
    "        \"fee_tot_usd_cm\", \"flow_in_ex_usd_cm\",\n",
    "        \"flow_out_ex_usd_cm\", \"gas_used_tx_cm\", \"iss_tot_usd_cm\",\n",
    "        \"ndf_cm\", \"nvt_adj_ff_cm\", \"puell_mul_rev_cm\",\n",
    "        \"puell_mul_tot_cm\", \"rvt_adj_cm\", \"rev_hash_usd_cm\",\n",
    "        \"rev_usd_cm\", \"ser_cm\", \"sply_act_ever_cm\",\n",
    "        \"sply_act_10yr_cm\", \"sply_act_5yr_cm\",\n",
    "        \"sply_act_1yr_cm\", \"sply_act_180d_cm\",\n",
    "        \"sply_act_30d_cm\", \"sply_act_7d_cm\", \"sply_act_1d_cm\",\n",
    "        \"sply_act_pct_1yr_cm\", \"sply_adr_bal_usd_1_cm\",\n",
    "        \"sply_adr_bal_usd_100_cm\", \"sply_adr_bal_usd_10k_cm\",\n",
    "        \"sply_adr_bal_usd_1m_cm\", \"sply_adr_top_100_cm\",\n",
    "        \"sply_adr_top_1pct_cm\", \"sply_burnt_usd_cm\",\n",
    "        \"sply_ex_usd_cm\", \"sply_cur_cm\", \"sply_ff_cm\",\n",
    "        \"sply_exp_fut_10yr_cm\", \"tx_tfr_cnt_cm\",\n",
    "        \"tx_tfr_val_adj_usd_cm\", \"tx_tfr_val_med_usd_cm\",\n",
    "        \"tx_tfr_val_usd_cm\", \"vel_act_1yr_cm\", \"vel_cur_1yr_cm\"]\n",
    "    eth_df = df[df.asset_cm=='eth'][eth_cols].reset_index(drop=True)\n",
    "\n",
    "    xsec_sum_cols = ['adr_act_cnt_cm', 'adr_act_rec_cnt_cm',\n",
    "        'adr_act_sent_cnt_cm', 'adr_bal_cnt_cm',\n",
    "        'adr_bal_usd_100_cnt_cm', 'adr_bal_usd_10k_cnt_cm',\n",
    "        'adr_bal_usd_100k_cnt_cm', 'adr_bal_usd_1m_cnt_cm',\n",
    "        'cap_act_1yr_usd_cm', 'cap_fut_exp_10yr_usd_cm',\n",
    "        'cap_mvrv_cur_cm', 'cap_mrkt_cur_usd_cm',\n",
    "        'cap_mrkt_ffusd_cm', 'cap_real_usd_cm', 'fee_tot_usd_cm',\n",
    "        'iss_tot_usd_cm', 'rev_usd_cm', 'sply_act_ever_cm',\n",
    "        'sply_act_10yr_cm', 'sply_act_5yr_cm', 'sply_act_1yr_cm',\n",
    "        'sply_act_180d_cm', 'sply_act_30d_cm', 'sply_act_7d_cm',\n",
    "        'sply_act_1d_cm', 'sply_adr_bal_usd_1_cm',\n",
    "        'sply_adr_bal_usd_100_cm', 'sply_adr_bal_usd_10k_cm',\n",
    "        'sply_adr_bal_usd_1m_cm', 'sply_adr_top_100_cm',\n",
    "        'sply_adr_top_1pct_cm', 'sply_ff_cm', 'tx_tfr_cnt_cm',\n",
    "        'tx_tfr_val_adj_usd_cm', 'tx_tfr_val_usd_cm',\n",
    "        'usd_ask_size_cm', 'usd_bid_size_cm']\n",
    "    xsec_sum_df = df.groupby('date')[xsec_sum_cols].sum().reset_index()\n",
    "\n",
    "    xsec_avg_cols = ['fee_mean_usd_cm', 'fee_med_usd_cm',    \n",
    "        'fee_rev_pct_cm', 'ndf_cm', 'nvt_adj_ff_cm', 'rvt_adj_cm',    \n",
    "        'ser_cm', 'sopr_cm', 'sply_act_pct_1yr_cm',\n",
    "        'vel_act_1yr_cm', 'vel_cur_1yr_cm', 'bidask_cm']\n",
    "    xsec_avg_df = df.groupby('date')[xsec_avg_cols].mean().reset_index()\n",
    "\n",
    "    panel_df = df[['date', 'asset_cm', 'cap_mrkt_est_usd_cm', 'reference_rate_usd_cm', \n",
    "        'trades_cm', 'usd_per_token_cm', 'usd_volume_cm', 'bidask_cm',\n",
    "        'usd_ask_cm', 'usd_bid_cm', 'usd_ask_size_cm', 'usd_bid_size_cm']].copy()\n",
    "\n",
    "    # forwarwd fill missings\n",
    "    eth_df = eth_df.ffill()\n",
    "    panel_df.sort_values(by=['asset_cm', 'date'], inplace=True)\n",
    "    panel_df = panel_df.groupby('asset_cm').apply(lambda group: group.fillna(method='ffill'))\n",
    "\n",
    "    # Fill remaining missingness with crossectional median besides price, mcap, and volume\n",
    "    cols_with_missing = [col for col in panel_df.columns if panel_df[col].isna().any()]\n",
    "    for col in cols_with_missing:\n",
    "        medians = panel_df.groupby('date')[col].transform('median')\n",
    "        panel_df[col].fillna(medians, inplace=True)\n",
    "\n",
    "    # confirm no missing\n",
    "    for temp_df in [btc_df, eth_df, xsec_sum_df, xsec_avg_df, panel_df]:\n",
    "        assert 0 == temp_df.isnull().sum().sum()\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not panel_df.duplicated(subset=['date', 'asset_cm']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    panel_df = panel_df.sort_values(by=['date', 'asset_cm'], ignore_index=True)\n",
    "\n",
    "    # Add btc and eth to col names for those dfs\n",
    "    btc_df.columns = [col if col == 'date' else 'btc_'+col for col in btc_df.columns]\n",
    "    eth_df.columns = [col if col == 'date' else 'eth_'+col for col in eth_df.columns]\n",
    "    xsec_sum_df.columns = [col if col == 'date' else 'total_'+col for col in xsec_sum_df.columns]\n",
    "    xsec_avg_df.columns = [col if col == 'date' else 'avg_'+col for col in xsec_avg_df.columns]\n",
    "\n",
    "    # Form temp_macro_df \n",
    "    temp_macro_df = btc_df.merge(eth_df, on='date', how='outer', validate='one_to_one')\n",
    "    temp_macro_df = temp_macro_df.merge(xsec_sum_df, on='date', how='outer', validate='one_to_one')\n",
    "    temp_macro_df = temp_macro_df.merge(xsec_avg_df, on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    return panel_df, temp_macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMacro(macro_df: pd.DataFrame, temp_macro_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Clean the CM macro data. \"\"\"\n",
    "    # Ensure date is top of hour\n",
    "    assert 0 == macro_df.date.isnull().sum()\n",
    "    assert len(macro_df) == macro_df[macro_df.date.dt.minute==0].shape[0]\n",
    "    macro_df = macro_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # Cut down to relevant dates between July 1, 2016 and January 2, 2023\n",
    "    macro_df = macro_df[(macro_df['date'] >= '2016-07-01') & (macro_df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # Ensure all dates are present\n",
    "    macro_df.set_index('date', inplace=True)\n",
    "    min_dt, max_dt = macro_df.index.min(), macro_df.index.max()\n",
    "    full_date_range = pd.date_range(start=min_dt, end=max_dt, freq='1H')\n",
    "    assert len(full_date_range) == macro_df.shape[0]\n",
    "    macro_df = macro_df.reset_index()\n",
    "\n",
    "    # Merge together the dataframes\n",
    "    macro_df = macro_df.merge(temp_macro_df, on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # Rename columns\n",
    "    macro_df = macro_df.rename(columns={'ex_open_interest_reported_future_usd': 'ex_open_interest_future_usd',\n",
    "                                        'ex_volume_reported_future_usd_1h': 'ex_volume_future_usd',\n",
    "                                        'ex_volume_reported_spot_usd_1h': 'ex_volume_spot_usd',\n",
    "                                        'us_ex_open_interest_reported_future_usd': 'us_ex_open_interest_future_usd',\n",
    "                                        'us_ex_volume_reported_future_usd_1h': 'us_ex_volume_future_usd',\n",
    "                                        'us_ex_volume_reported_spot_usd_1h': 'us_ex_volume_spot_usd'})\n",
    "\n",
    "    # Form stablecoin deviation from one column\n",
    "    macro_df.loc[macro_df.usd_per_usdc.isnull(), 'usdt_usdc_dev_from_one'] = macro_df.usd_per_usdt - 1\n",
    "    macro_df.loc[macro_df.usd_per_usdc.notnull()\n",
    "        & (np.abs(macro_df.usd_per_usdc-1) > np.abs(macro_df.usd_per_usdt-1)), 'usdt_usdc_dev_from_one'] = macro_df.usd_per_usdc-1\n",
    "    macro_df.loc[macro_df.usd_per_usdc.notnull()\n",
    "        & (np.abs(macro_df.usd_per_usdc-1) <= np.abs(macro_df.usd_per_usdt-1)), 'usdt_usdc_dev_from_one'] = macro_df.usd_per_usdt-1\n",
    "    macro_df = macro_df.drop(['usd_per_usdc', 'usd_per_usdt'], axis=1)\n",
    "\n",
    "    # Confirm no missings in the df\n",
    "    assert(macro_df.isnull().sum().sum() == 0)\n",
    "\n",
    "    # Set column order\n",
    "    cols = list(macro_df.columns.values)\n",
    "    cols.remove('date')\n",
    "    cols.sort()\n",
    "    macro_df = macro_df[['date']+cols]\n",
    "\n",
    "    # Convert all columns to float32\n",
    "    for col in cols:\n",
    "        macro_df[col] = macro_df[col].astype('float32')\n",
    "\n",
    "    # Ensure all columns have cm in name\n",
    "    macro_df.columns = [col if col == 'date' or col.endswith('_cm') else col + '_cm' for col in macro_df.columns]\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not macro_df.duplicated(subset=['date']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    macro_df = macro_df.sort_values(by=['date'], ignore_index=True)\n",
    "\n",
    "    return macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_247648/3152524850.py:267: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  panel_df = panel_df.groupby('asset_cm').apply(lambda group: group.fillna(method='ffill'))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    ASSET_IN_FP = '../data/derived/asset_universe_dict.pickle'\n",
    "    PANEL_IN_FP = '../data/raw/coinmetrics_panel_hourly.pkl'\n",
    "    MACRO_IN_FP = '../data/raw/coinmetrics_macro_hourly.pkl'\n",
    "    PANEL_OUT_FP = \"../data/derived/cm_panel.pkl\"\n",
    "    MACRO_OUT_FP = '../data/derived/cm_macro.pkl'\n",
    "\n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    macro_df = pd.read_pickle(MACRO_IN_FP)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # clean\n",
    "    panel_df, temp_macro_df = cleanPanel(df, asset_universe_dict)\n",
    "    macro_df = cleanMacro(macro_df, temp_macro_df)\n",
    "\n",
    "    # output\n",
    "    panel_df.to_pickle(PANEL_OUT_FP)\n",
    "    macro_df.to_pickle(MACRO_OUT_FP)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
