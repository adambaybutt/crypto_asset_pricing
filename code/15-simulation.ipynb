{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR, AutoReg\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import Dict, List, Tuple\n",
    "from joblib import Parallel, delayed\n",
    "from ipca import InstrumentedPCA\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formIndexCrosswalk(in_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Form a crosswalk dataframe to new indices that are integer indices for use with IPCA package.\n",
    "    \"\"\"\n",
    "    cross_df = in_df[['date', 'asset']].copy()\n",
    "    cross_df['time'] = cross_df['date'].factorize()[0] + 1\n",
    "    cross_df['asset_num'] = cross_df['asset'].factorize()[0] + 1\n",
    "    return cross_df\n",
    "\n",
    "def normalize_column(df, column_name):\n",
    "    \"\"\" \n",
    "    Cross sectionally normalize column by values in 'time' column.\n",
    "    \"\"\"\n",
    "    def normalize_within_date_group(column_name, group):\n",
    "        # Calculate the number of assets for this date\n",
    "        n = group.shape[0]\n",
    "        \n",
    "        # Add random noise to the values to ensure unique ranks\n",
    "        noise = np.random.uniform(-1e-6, 1e-6, size=n)\n",
    "        group[column_name] += noise\n",
    "        \n",
    "        # Rank the values, divide by the number of assets, and subtract 0.5\n",
    "        group[column_name] = group[column_name].rank() / n - 0.5\n",
    "        return group\n",
    "    \n",
    "    # Apply the normalization to a specific column within each date group\n",
    "    return df.groupby('time', group_keys=False).apply(lambda group: normalize_within_date_group(column_name, group))\n",
    "\n",
    "def subsetAndNormalizeColumns(in_df: pd.DataFrame, lhs_col: str, p: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset the weekly panel to relevant columns for fitting IPCA and \n",
    "        normalizes the characteristic columns using cross-sectional ranking.\n",
    "    \n",
    "    This function linearly spaces the values in the characteristic columns within each date\n",
    "    to the range [-0.5, 0.5]. The values are ranked, divided by the number of assets for\n",
    "    that date, and subtracted by 0.5.\n",
    "\n",
    "    Args:\n",
    "        in_df (pd.DataFrame): weekly panel data with all the columns.\n",
    "        lhs_col (str): name of the left hand side column.\n",
    "        p (int): number of characteristics to use in the simulation.\n",
    "    \n",
    "    Returns: (pd.DataFrame): weekly panl with IPCA asset and time columns\n",
    "        lhs col and the p characteristics.\n",
    "    \"\"\"\n",
    "    # Form a copy\n",
    "    df = in_df.copy()\n",
    "\n",
    "    # Determine characteristic columns\n",
    "    char_cols = [col for col in list(df.columns.values) \n",
    "                    if ('char_' in col) & ~('char_industry_' in col) \n",
    "                    & ~('char_asset_usage' in col)]\n",
    "    for col in ['char_pow', 'char_pos', 'char_ico_price', 'char_ico']:\n",
    "        char_cols.remove(col)\n",
    "\n",
    "    # Randomly obtain p of the characteristics.\n",
    "    random.seed(42)\n",
    "    selected_columns = random.sample(char_cols, p)\n",
    "\n",
    "    # Subset to relevant columns\n",
    "    df = df[['time', 'asset_num', lhs_col]+selected_columns].copy()\n",
    "\n",
    "    # Loop over the specified columns to normalize\n",
    "    for column_name in selected_columns:\n",
    "        df = normalize_column(df, column_name)\n",
    "    \n",
    "    # Random\n",
    "    return df, selected_columns\n",
    "\n",
    "def fitIPCAInPanelForEstimatedFactorsAndLoadings(\n",
    "    in_df: pd.DataFrame, lhs_col: str, p: int, k: int, fit_alpha: bool, num_cpus: int):\n",
    "    \"\"\"\n",
    "    Generates an AR(1) process for a given set of parameters and length T using numpy.\n",
    "    \n",
    "    Args:\n",
    "        in_df (pd.DataFrame): weekly panel data with all the columns.\n",
    "        lhs_col (str): name of the left hand side column.\n",
    "        p (int): number of characteristics to use in the simulation.\n",
    "        k (int): number of factors.\n",
    "        fit_alpha (bool): whether to include an intercept alpha term.\n",
    "        num_cpus (int): number of processors to use when working in parallel.\n",
    "        \n",
    "    Returns: (Tuple) numpy array of factors of num of weeks in panel by k; \n",
    "                numpy array of loadings of p * k; \n",
    "                and a float of the empirical r^2_total.\n",
    "    \"\"\"\n",
    "    # Form a copy of the data\n",
    "    df = in_df.copy()\n",
    "\n",
    "    # form index crosswalk, in case needed\n",
    "    cross_df = formIndexCrosswalk(df)\n",
    "    df = df.merge(cross_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "\n",
    "    # Subset columns\n",
    "    df, selected_columns = subsetAndNormalizeColumns(df, lhs_col, p)\n",
    "\n",
    "    # Form list of rhs char columns\n",
    "    char_cols = [col for col in list(df.columns.values) if 'char_' in col]\n",
    "    assert(len(char_cols)==p)\n",
    "\n",
    "    # Form datasets to fit ipca\n",
    "    df = df.sort_values(by=['time', 'asset_num'], ignore_index=True)\n",
    "    Y  = df[['time', 'asset_num', lhs_col]].copy()\n",
    "    Y[lhs_col] = Y[lhs_col].astype('float64')\n",
    "    Y = Y.set_index(keys=['asset_num', 'time'], verify_integrity=True)\n",
    "    Y = Y.squeeze()\n",
    "    X = df[['time', 'asset_num']+char_cols].copy()\n",
    "    X = X.set_index(keys=['asset_num', 'time'], verify_integrity=True)\n",
    "    X = X.astype('float64')\n",
    "\n",
    "    # Fit\n",
    "    ipca = InstrumentedPCA(n_factors=k, intercept=fit_alpha, n_jobs=num_cpus)\n",
    "    ipca = ipca.fit(X=X, y=Y, data_type='panel')\n",
    "\n",
    "    # Form factors and loadings to return\n",
    "    estimates = ipca.get_factors()\n",
    "    loadings  = estimates[0]\n",
    "    factors   = estimates[1].transpose()\n",
    "\n",
    "    # Obtain the R^2_total\n",
    "    yhats = ipca.predict(X, data_type='panel')\n",
    "    r2_total = 1 - np.mean(np.square(Y - yhats))/np.mean(np.square(Y))\n",
    "\n",
    "    return factors, loadings, r2_total, selected_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitVAR1(matrix: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Fit a VAR(1) model to the given matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix (ndarray): Input data matrix with dimensions T x k.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Intercept vector (mu) and coefficients matrix (phi).\n",
    "    \"\"\"\n",
    "    if matrix.shape[1] == 1:\n",
    "        model = AutoReg(matrix.squeeze(), lags=1)\n",
    "        model_fitted = model.fit()\n",
    "        mu = model_fitted.params[0]  # intercept\n",
    "        phi = model_fitted.params[1]  # coefficients matrix\n",
    "    else:\n",
    "        model = VAR(matrix)\n",
    "        model_fitted = model.fit(1)\n",
    "        mu = model_fitted.params[0]  # intercept\n",
    "        phi = model_fitted.params[1:]  # coefficients matrix\n",
    "    return mu, phi\n",
    "\n",
    "def simulateVAR1(mu: np.ndarray,\n",
    "    phi: np.ndarray,\n",
    "    mean: np.ndarray,\n",
    "    variance: np.ndarray,\n",
    "    n_steps: int,\n",
    "    k: int,\n",
    "    s: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate new realizations from a fitted VAR(1) model.\n",
    "\n",
    "    Parameters:\n",
    "    - mu (ndarray): Intercept vector from the fitted model.\n",
    "    - phi (ndarray): Coefficients matrix from the fitted model.\n",
    "    - mean (ndarray): Mean of the normal innovations.\n",
    "    - variance (ndarray): Variance of the normal innovations.\n",
    "    - n_steps (int): Number of time steps to simulate.\n",
    "    - k (int): Number of variables.\n",
    "    - s (int): seed number to pass to produce variation.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray: Simulated data of dimensions n_steps x k.\n",
    "    \"\"\"\n",
    "    # Set buffer to throw away first obs\n",
    "    buffer = int(n_steps/4)\n",
    "\n",
    "    # Set seed for replicability\n",
    "    np.random.seed(s)\n",
    "\n",
    "    # Initialize array to store simulated values\n",
    "    simulated_data = np.zeros((n_steps+buffer, k))\n",
    "\n",
    "    # Set initial value\n",
    "    if k > 1:\n",
    "        simulated_data[0] = np.random.multivariate_normal(mean, np.diag(variance), 1)\n",
    "    else:\n",
    "        simulated_data[0] = np.random.normal(mean, np.sqrt(variance), 1)\n",
    "\n",
    "    # Run simulation\n",
    "    for t in range(1, n_steps+buffer):\n",
    "        # form factors for a single or multivariate AR model\n",
    "        if k > 1:\n",
    "            epsilon_t = np.random.multivariate_normal(np.zeros(k), np.diag(variance), 1)\n",
    "            simulated_data[t] = mu + np.matmul(phi, simulated_data[t-1]) + epsilon_t\n",
    "        else:\n",
    "            epsilon_t = np.random.normal(0, np.sqrt(variance), 1)\n",
    "            simulated_data[t] = mu + phi * simulated_data[t-1] + epsilon_t\n",
    "        \n",
    "    return simulated_data[buffer:,:]\n",
    "\n",
    "def simulateFactors(ipca_factors: np.ndarray, T: int, S: int, k: int, sigma_f_2: float) -> np.ndarray:\n",
    "    \"\"\" Use VAR(1) model to simulate the requested factor matrix from empirical data in ipca_factors.\n",
    "\n",
    "    Args:\n",
    "        ipca_factors (np.ndarray): matrix of factors of dimensions T by number of factors.\n",
    "        T (int): number of time periods in the simulation.\n",
    "        S (int): number of simulations to run.\n",
    "        k (int): number of factors.\n",
    "        sigma_f_2 (float): variance of normal innovations in VAR(1) model for factors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: sim'ed factors of dimensions T x number of covar_cols in `factors' x S.\n",
    "    \"\"\"\n",
    "    # Create objects\n",
    "    new_factors = np.zeros((T, k, S), dtype=np.float64)\n",
    "\n",
    "    # Fit VAR(1) on the fitted latent factors from empirical data.\n",
    "    mu, phi = fitVAR1(ipca_factors)\n",
    "\n",
    "    # Set VAR(1) simulation params\n",
    "    if k > 1:\n",
    "        innovations_mean = np.zeros(k)\n",
    "        innovations_var = np.repeat(sigma_f_2, k)\n",
    "    else:\n",
    "        innovations_mean = 0\n",
    "        innovations_var = sigma_f_2\n",
    "\n",
    "    # Simulate new factors using the VAR(1) model with normal innovations for given number of sims\n",
    "    for s in range(S):\n",
    "        new_factors[:,:,s] = simulateVAR1(mu, phi, innovations_mean, innovations_var, T, k, s)\n",
    "\n",
    "    return new_factors\n",
    "\n",
    "def simulateCharacteristics(chars_df: pd.DataFrame, S: int, T: int, N: int, p: int, sigma_z_2: float, num_cpus: int):\n",
    "    \"\"\" Use VAR(1) model to simulate the characteristics a la Kelly et al 2020 IPCA Theory paper.\n",
    "\n",
    "    Args:\n",
    "        chars_df (pd.DataFrame): contains characteristic data for all time periods and assets in empirical data.\n",
    "        N (int): number of assets in the simulation.\n",
    "        S (int): number of simulations to run.\n",
    "        T (int): number of time periods in the simulation.\n",
    "        p (int): number of characteristics used in the simulation.\n",
    "        sigma_z_2 (float): variance of normal innovations in VAR(1) model for characteristics.\n",
    "        num_cpus (int): number of cpus to use to run this simulation in parallel.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: sim'ed characteristics of dimensions T*N x p x S.\n",
    "    \"\"\"\n",
    "    # List of assets to loop over\n",
    "    assets = list(np.unique(chars_df.asset.values))\n",
    "\n",
    "    # Remove assets to use to generate simulated characteristics if less than 200 time periods of data\n",
    "    raw_assets = assets.copy()\n",
    "    for asset in raw_assets:\n",
    "        num_obs = chars_df[chars_df.asset==asset].shape[0]\n",
    "        if num_obs < 200:\n",
    "            assets.remove(asset)\n",
    "\n",
    "    # If number of assets we have is more than we need, then downsample; else, upsample.\n",
    "    if len(assets) >= N:\n",
    "        assets = random.sample(assets, N)\n",
    "    else:\n",
    "        diff = N - len(assets)\n",
    "        assets += random.choices(assets, k=diff)\n",
    "        assets = sorted(assets, key=lambda x: random.random())\n",
    "    assert(len(assets)==N)\n",
    "\n",
    "    # Define function to loop over all the assets\n",
    "    def loopOverAsset(i):\n",
    "        # Grab asset name\n",
    "        asset = assets[i]\n",
    "\n",
    "        # Subset to asset data\n",
    "        asset_df = chars_df[chars_df.asset==asset].copy()\n",
    "\n",
    "        # Obtain the matrix of characteristics\n",
    "        chars_mat = asset_df.drop(['date', 'asset'], axis=1).values\n",
    "\n",
    "        # Normalize the data\n",
    "        column_means = np.mean(chars_mat, axis=0)\n",
    "        column_stds = np.std(chars_mat, axis=0)\n",
    "        chars_mat = (chars_mat - column_means) / column_stds\n",
    "\n",
    "        # Calculate the time series average of each characteristic\n",
    "        char_means = np.mean(chars_mat, axis=0)\n",
    "\n",
    "        # Calcuate the VAR(1) on the matrix of characteristics\n",
    "        mu, phi = fitVAR1(chars_mat)\n",
    "\n",
    "        # Set VAR(1) simulation params\n",
    "        innovations_means = random.choices(char_means, k=p)\n",
    "        innovations_var = np.repeat(sigma_z_2, p)\n",
    "\n",
    "        # Simulate new instruments using the VAR(1) model with normal innovations for given number of sims\n",
    "        sim_chars = []\n",
    "        for s in range(S):\n",
    "            sim_chars.append(simulateVAR1(mu, phi, innovations_means, innovations_var, T, p, s))\n",
    "\n",
    "        return np.stack(sim_chars, axis=-1)\n",
    "\n",
    "    # Generate characteristics for given number of assets\n",
    "    sim_chars_per_asset = Parallel(n_jobs=num_cpus)(delayed(loopOverAsset)(i) for i in range(len(assets)))\n",
    "\n",
    "    # Reshape and return results\n",
    "    Z = np.stack(sim_chars_per_asset, axis=-1).transpose(0, 3, 1, 2).reshape(T*N, p, S)\n",
    "\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGP(T: int, N: int, k: int, p: int, sparse_s: int, S: int, eta: np.ndarray,\n",
    "    sigma_f_2: float, sigma_z_2: float, sigma_g_2: float, sigma_e_2: float,\n",
    "    ipca_factors: np.ndarray, ipca_loadings: np.ndarray, r2_total_emp: float,\n",
    "    selected_columns: List[str], panel_df: pd.DataFrame, num_cpus: int) -> tuple:\n",
    "    ''' Generate random variables and parameters for simulation.\n",
    "\n",
    "    Args: \n",
    "        T (int): number of time periods.\n",
    "        N (int): number of observational units per time period.\n",
    "        k (int): dimensionality of latent factors.\n",
    "        p (int): dimensionality of the covariates.\n",
    "        sparse_s (int): sparsity index of factor loadings.\n",
    "        S (int): number of simulations.\n",
    "        eta (np.ndarray): parameter mapping latent factors to observable factor.\n",
    "        sigma_f_2 (float): variance of errors in latent factor VAR(1) sim. \n",
    "        sigma_z_2 (float): variance of errors in characteristic PVAR(1) sim. \n",
    "        sigma_g_2 (float): variance of errors in observable factor model.\n",
    "        sigma_e_2 (float): variance of idiosyncratic errors.\n",
    "        ipca_factors (np.ndarray): matrix of factors of dimensions T by number of factors.\n",
    "        ipca_loadings (np.ndarray): matrix of loadings of dimensions p by k estimated from data.\n",
    "        r2_total_emp (float): the empirical r^2_total.\n",
    "        selected_columns (List[str]): columns selected from empirical panel from simulation characteristics.\n",
    "        panel_df (pd.DataFrame): panel of empirical data.\n",
    "        num_cpus (int): number of CPUs to use for parallel processing.\n",
    "    \n",
    "    Returns: \n",
    "        (tuple):\n",
    "            - R (np.ndarray): outcome/returns ndarray of dimensions T*N by 1 by S.\n",
    "            - Gamma_beta (np.ndarray): true loading on latent factors of dim p by k.\n",
    "            - F (np.ndarray): true latent factors of dimensions T by k by S.\n",
    "            - G (np.ndarray): true observable factor of dimensions T by 1 by S.\n",
    "            - Z (np.ndarray): covariates of dimensions T*N by p by S.\n",
    "            - Gamma (np.ndarray): true time series averages of latent factors k by S.\n",
    "            - e (np.ndarray): true idiosyncratic error of dimensions T*N, 1, S.\n",
    "            - e_g (np.ndarray): true error in ob factor model of dimensions T, 1, S.\n",
    "    '''\n",
    "    # Initialize data objects \n",
    "    R     = np.zeros((T*N, 1, S), dtype=float)\n",
    "    G     = np.zeros((T, 1, S), dtype=float)\n",
    "    Gamma = np.zeros((k, 1, S), dtype=float)\n",
    "    e     = np.zeros((T*N, 1, S), dtype=float)\n",
    "    e_g   = np.zeros((T, 1, S), dtype=float)\n",
    "\n",
    "    # Form Gamma Beta by soft thresholding the empirically estimated loadings to leave only\n",
    "    #     sparse_s rows nonzero and re order from high to low by row l1 norm\n",
    "    emp_loadings_row_l1_norm    = np.sum(np.abs(ipca_loadings), axis=1)\n",
    "    quantile                    = np.quantile(emp_loadings_row_l1_norm, q=1-sparse_s/p)\n",
    "    sparse_mask                 = emp_loadings_row_l1_norm > quantile\n",
    "    assert(sparse_s == np.sum(sparse_mask))\n",
    "    Gamma_beta = ipca_loadings.copy()\n",
    "    Gamma_beta[~sparse_mask, :] = 0\n",
    "    indices_ordered_by_l1_norm  = np.argsort(emp_loadings_row_l1_norm)[::-1]\n",
    "    Gamma_beta                  = Gamma_beta[indices_ordered_by_l1_norm]\n",
    "\n",
    "    # Simulate factors\n",
    "    F = simulateFactors(ipca_factors, T, S, k, sigma_f_2)\n",
    "\n",
    "    # Simulate characteristics\n",
    "    Z = simulateCharacteristics(panel_df[['date', 'asset']+selected_columns], S, T, N, p, sigma_z_2, num_cpus)\n",
    "\n",
    "    # generate data for each simulation\n",
    "    for s in range(S):\n",
    "        # set seed for numpy and random packages for replicable data\n",
    "        np.random.seed(int(42*s))\n",
    "\n",
    "        # form idiosyncratic errors\n",
    "        e[:,:,s] = np.random.multivariate_normal([0], [[sigma_e_2]], size=T*N)\n",
    "\n",
    "        # Form returns for each time period\n",
    "        for t in range(T):\n",
    "            # Form start and end index for indexing T*N vectors\n",
    "            start_index = int(t*N)\n",
    "            end_index   = int((t+1)*N)\n",
    "\n",
    "            # Form returns\n",
    "            R[start_index:end_index, :, s] = ((Z[start_index:end_index,:,s] @ Gamma_beta @ F[t,:,s]).reshape(-1,1) \n",
    "                + e[start_index:end_index,:,s])\n",
    "\n",
    "        # form observable factor and time series mean of factors\n",
    "        e_g[:,:,s]   = np.random.multivariate_normal([0], [[sigma_g_2]], size=T)\n",
    "        Gamma[:,:,s] = F[:,:,s].mean(axis=0).reshape(-1,1)\n",
    "        V            = F[:,:,s] - Gamma[:,:,s].reshape(-1)\n",
    "        G[:,:,s]     = V.dot(eta).reshape(-1,1) + e_g[:,:,s]\n",
    "\n",
    "    return R, Gamma_beta, F, G, Z, Gamma, e, e_g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLasso(Y: np.ndarray, X: np.ndarray, penalty: float) -> np.ndarray:\n",
    "    ''' Runs lasso of Y on X with given penalty param to return fitted coefs.\n",
    "\n",
    "    Args: \n",
    "        X (np.ndarray): RHS variables with rows of obs and covar_cols of covars.\n",
    "                        These data include a constant but have yet to be\n",
    "                        normalized for lasso.\n",
    "        Y (np.ndarray): LHS variable with rows of obs and single column.\n",
    "        penalty (float): real-valued scalar on L1 penalty in Lasso.\n",
    "\n",
    "    Returns:\n",
    "        beta_hat (np.ndarray): vector of fitted coefficients; note: these\n",
    "                                must be used on normalized RHS variables.\n",
    "    '''\n",
    "    # normalize RHS\n",
    "    muhat  = np.mean(X, axis = 0)\n",
    "    stdhat = np.std(X, axis = 0)+np.min(X)/1e3 # NOTE: incase of no variation wont divide by 0\n",
    "    Xtilde = np.divide(np.subtract(X, muhat), stdhat)\n",
    "\n",
    "    # perform lasso\n",
    "    lasso = Lasso(alpha = penalty)\n",
    "    lasso.fit(Xtilde, Y)\n",
    "\n",
    "    # return fitted coefficients\n",
    "    return lasso.coef_\n",
    "\n",
    "def calcPenaltyBCCH(Y: np.ndarray, X: np.ndarray, c: float) -> float:\n",
    "    ''' This function applies Belloni, Chen, Chernozhukov, Hansen 2012 ECMA\n",
    "        closed-form solution for selecting Lasso penalty parmaeter.\n",
    "\n",
    "    Args: \n",
    "        X (np.ndarray): RHS variables with rows of obs and covar_cols of covars.\n",
    "                        These data include a constant but have yet to be\n",
    "                        normalized for lasso.\n",
    "        Y (np.ndarray): LHS variable with rows of obs and single column.\n",
    "        c (float):    scalar constant from theory; usually ~1.\n",
    "\n",
    "    Returns:\n",
    "        penalty (float): BCCH penalty parameter.\n",
    "    '''\n",
    "    # Bickel Ritov Tsybakov constant parameter selection\n",
    "    a = 0.1\n",
    "\n",
    "    # calc pilot penalty parameter\n",
    "    N = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    max_moment_xy = np.max(np.mean((X**2)*(Y**2), axis =0)**0.5) \n",
    "    penalty_pilot = 2*c*norm.ppf(1-a/(2*p))*max_moment_xy/np.sqrt(N)\n",
    "\n",
    "    # run lasso with pilot penalty parameter\n",
    "    beta_hat = runLasso(Y, X, penalty_pilot)\n",
    "    #assert(~np.isclose(0, np.sum(np.abs(beta_hat)), rtol=1e-8, atol=1e-8)),('Pilot penalty kills all coefs. Scale down c?')\n",
    "\n",
    "    # set BCCH penalty parameter\n",
    "    residuals = Y - np.matmul(X, beta_hat).reshape(-1,1)\n",
    "    max_moment_xepi = np.max(np.mean((X**2)*(residuals**2), axis =0)**0.5) \n",
    "    penalty = 2*c*norm.ppf(1-a/(2*p))*max_moment_xepi/np.sqrt(N)\n",
    "\n",
    "    return penalty\n",
    "\n",
    "def runOLS(Y: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "    ''' Runs OLS of Y on X to return fitted coefficients.\n",
    "\n",
    "    Args: \n",
    "        X (np.ndarray): RHS--assumes contains constant--with rows of obs and covar_cols of covars.\n",
    "        Y (np.ndarray): LHS variable with rows of obs and single column.\n",
    "\n",
    "    Returns:\n",
    "        beta_hat (np.ndarray): vector of fitted coefficients.\n",
    "    '''\n",
    "    return np.matmul(np.linalg.inv(np.matmul(np.transpose(X), X)),\n",
    "                        np.matmul(np.transpose(X), Y))\n",
    "\n",
    "def runDoubleSelectionLasso(Y: np.ndarray, D: np.ndarray, X: np.ndarray, c: float,\n",
    "    selected_prct_upper: float=0.5, selected_prct_lower: float=0.05) -> float:\n",
    "    ''' Runs Double Selection Lasso from Belloni et al (2014).\n",
    "\n",
    "    Args: \n",
    "        Y (np.ndarray): LHS variable with rows of obs and single column.\n",
    "        D (np.ndarray): RHS target variable with rows of obs and single column.\n",
    "        X (np.ndarray): RHS controls with rows of obs and p cols of characteristics.\n",
    "        c (float): scalar constant from theory; usually ~1.\n",
    "        selected_prct_upper (float): upper bound on number of columns selected.\n",
    "        selected_prct_lower (float): lower bound on number of columns selected.\n",
    "    \n",
    "    Returns:\n",
    "        alpha_hat (float): estimated target coefficient.\n",
    "    '''\n",
    "    # initialize a percent selected outside range\n",
    "    selected_prct_cols = 1\n",
    "\n",
    "    while ((selected_prct_cols > selected_prct_upper) \n",
    "        | (selected_prct_cols < selected_prct_lower)):\n",
    "        # update scalar constant\n",
    "        if (selected_prct_cols > selected_prct_upper):\n",
    "            c = 1.05*c\n",
    "        else:\n",
    "            c = 0.95*c\n",
    "\n",
    "        # lasso of Y on D and X to select elements of X, I_1_hat\n",
    "        X_all = np.hstack((D,X))\n",
    "        beta_hat_1 = runLasso(Y, X_all, penalty=calcPenaltyBCCH(Y, X_all, c=c))\n",
    "\n",
    "        # lasso of D on X to select elements of X, I_2_hat\n",
    "        beta_hat_2 = runLasso(D, X, penalty=calcPenaltyBCCH(D, X, c=c))\n",
    "\n",
    "        # form union of I_1_hat and I_2_hat\n",
    "        i_1_hat = list(np.nonzero(beta_hat_1)[0] -1 ) # NOTE: subtracting 1 as we added the treatment var to RHS\n",
    "        if -1 in i_1_hat: i_1_hat.remove(-1) # remove treatment variable if it was included\n",
    "        i_2_hat = list(np.nonzero(beta_hat_2)[0])\n",
    "        i_hat   = list(set(i_1_hat).union(set(i_2_hat)))\n",
    "\n",
    "        # update percent of columns that were selected\n",
    "        selected_prct_cols = len(i_hat) / X.shape[1]\n",
    "\n",
    "    # OLS of Y on D plus included Xs\n",
    "    X_sel    = X[:,i_hat]\n",
    "    X_all    = np.hstack((D, X_sel))\n",
    "    beta_hat = runOLS(Y, X_all)\n",
    "    alpha_hat = beta_hat[0,0]\n",
    "\n",
    "    # return target parameter on D\n",
    "    return alpha_hat\n",
    "\n",
    "def fitBaiPCA(\n",
    "    matrix: np.ndarray, T: int, k: int, p: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Calculate the scaling factor\n",
    "    scaling_factor = 1 / (T * p)\n",
    "\n",
    "    # Form the target, symmetric, positive semi-definite matrix\n",
    "    target_matrix = scaling_factor * (matrix @ matrix.T)\n",
    "\n",
    "    # Calculate eigenvalues and vectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(target_matrix)\n",
    "\n",
    "    # Calculate factors and loadings\n",
    "    factors = np.sqrt(T) * eigenvectors[:, -k:][:, ::-1]\n",
    "    loadings = (matrix.T @ factors) / T\n",
    "\n",
    "    # Confirm factors are scaled appropriately\n",
    "    identity = (factors.T @ factors) / T\n",
    "    assert(np.isclose(k, np.sum(np.abs(np.diagonal(identity)))))\n",
    "\n",
    "    return factors, loadings\n",
    "\n",
    "def softThresholdCols(matrix, sparse_prct=0.2):\n",
    "    \"\"\" Implement ell_1 soft thresholding across colums of the given matrix. \"\"\"\n",
    "    dim1 = matrix.shape[0]\n",
    "    ell_1_norm_cols = np.sum(np.abs(matrix), axis=0)\n",
    "    lmbd = np.quantile(ell_1_norm_cols, 1-0.2)\n",
    "    col_mask = 1*(ell_1_norm_cols > lmbd)\n",
    "    mat_mask = np.tile(col_mask, (dim1, 1))\n",
    "    return matrix*mat_mask\n",
    "\n",
    "def softThresholdRows(matrix, sparse_prct=0.2):\n",
    "    \"\"\" Implement ell_1 soft thresholding across rows of the given matrix. \"\"\"\n",
    "    dim2 = matrix.shape[1]\n",
    "    ell_1_norm_rows = np.sum(np.abs(matrix), axis=1)\n",
    "    lmbd = np.quantile(ell_1_norm_rows, 1-sparse_prct)\n",
    "    row_mask = 1*(ell_1_norm_rows > lmbd)\n",
    "    mat_mask = np.repeat(row_mask, dim2).reshape(-1, dim2)\n",
    "    return matrix*mat_mask\n",
    "\n",
    "def fitDSLFM(R: np.ndarray, Z: np.ndarray,\n",
    "    T: int, N: int, k: int, p: int, c: float, num_cpus: int) -> np.ndarray:\n",
    "    ''' This function performs the DSLFM estimation procedure.\n",
    "\n",
    "    Args: \n",
    "        R (np.ndarray):  outcome/returns ndarray of dimensions T*N by 1.\n",
    "        Z (np.ndarray):  covariates of dimensions T*N by p.\n",
    "        T (int): number of time periods.\n",
    "        N (int): number of observational units per time period.\n",
    "        k (int): dimensionality of latent factors.\n",
    "        p (int): dimensionality of the covariates.\n",
    "        c (float):       scalar constant from theory; usually ~1.\n",
    "        num_cpus (int): number of CPUs to use for parallel processing.\n",
    "        demean (bool): optionally to demean the C_hat matrix before PCA.\n",
    "\n",
    "    Returns:\n",
    "        Gamma_beta_hat (np.ndarray): estimated loadings of dimensions p by k.\n",
    "        F_hat (np.ndarray): estimated latent factors of dimensions T by k.\n",
    "    '''\n",
    "    # Figure out number cpus to use for outer and inner loops assuming we have at least 4\n",
    "    assert(num_cpus >= 4)\n",
    "    n_jobs_outer = int(num_cpus / 4)\n",
    "    n_jobs_inner = 4\n",
    "\n",
    "    def runForEachCharacteristic(j):\n",
    "        # form indices\n",
    "        minus_j = list(range(p))\n",
    "        minus_j.remove(j)\n",
    "        \n",
    "        def runForEachTimePeriod(t):\n",
    "            # form time start and end indices\n",
    "            start_ob = int(t*N)\n",
    "            last_ob  = int((t+1)*N)\n",
    "\n",
    "            # form, for this rhs var j, this time periods LHS, target, and controls\n",
    "            Y = R[start_ob:last_ob,:]\n",
    "            D = Z[start_ob:last_ob,j].reshape(-1,1)\n",
    "            X = Z[start_ob:last_ob,minus_j]\n",
    "\n",
    "            # estimate c_{t,j}, i.e. target coef\n",
    "            c_t_j = runDoubleSelectionLasso(Y, D, X, c)\n",
    "\n",
    "            return c_t_j\n",
    "\n",
    "        C_t_hat = Parallel(n_jobs=n_jobs_inner)(delayed(runForEachTimePeriod)(t) for t in range(T))\n",
    "        \n",
    "        return C_t_hat\n",
    "\n",
    "    # Estimate C hat matrix\n",
    "    C_hat = Parallel(n_jobs=n_jobs_outer)(delayed(runForEachCharacteristic)(j) for j in range(p))\n",
    "    C_hat = np.array(C_hat).transpose()\n",
    "\n",
    "    # Soft threshold C hat for DSLFM estimation of it\n",
    "    C_hat_st = softThresholdCols(C_hat)\n",
    "\n",
    "    # Demean C_hat for this version of the estimators\n",
    "    C_hat_d = C_hat - np.mean(C_hat, axis=0)\n",
    "\n",
    "    # Use PCA to decompose C_hat into estimated factors and loadings\n",
    "    factors_hat, loadings_hat = fitBaiPCA(C_hat, T, k, p)\n",
    "    Gamma_beta_hat = loadings_hat\n",
    "\n",
    "    # Use PCA to decompose C_hat_d into estimated factors V and loadings \\G_\\b^d\n",
    "    factors_v_hat, loadings_d_hat = fitBaiPCA(C_hat_d, T, k, p)\n",
    "    Gamma_beta_d_hat = loadings_d_hat\n",
    "\n",
    "    # Soft threshold Gamma beta hat and Gamma_beta_d_hat\n",
    "    Gamma_beta_check = softThresholdRows(Gamma_beta_hat)\n",
    "    Gamma_beta_d_check = softThresholdRows(Gamma_beta_d_hat)\n",
    "    \n",
    "    return C_hat_st, Gamma_beta_hat, factors_hat, Gamma_beta_check, factors_v_hat, Gamma_beta_d_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitIPCA(Y: np.ndarray, X: np.ndarray, T: int, N: int, k: int, num_cpus: int):\n",
    "    ''' Runs IPCA estimation procedure to return estimated loadings and factors.\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): single column of returns of dimension N*T by 1.\n",
    "        X (np.ndarray): p columns of covariates with N*T rows.\n",
    "        T (int): number of time periods.\n",
    "        N (int): number of observational units per time period.\n",
    "        k (int): dimensionality of latent factors.\n",
    "        num_cpus (int): number of processors to use when working in parallel.\n",
    "\n",
    "    Returns: (tuple)\n",
    "        Gamma_beta_hat (np.ndarray): loadings of dimensions p by k.\n",
    "        Factors_hat (np.ndarray): factors of dimensions T by k.\n",
    "    '''\n",
    "    # convert input objects to necessary object type and dimensions\n",
    "    Y = pd.DataFrame(Y)\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    # add multiindex of integers time and asset to both LHS and RHS\n",
    "    Y['time']  = np.repeat(np.arange(1,T+1), N)\n",
    "    Y['asset'] = np.tile(np.arange(1,N+1), T)\n",
    "    X['time']  = np.repeat(np.arange(1,T+1), N)\n",
    "    X['asset'] = np.tile(np.arange(1,N+1), T)\n",
    "    Y = Y.set_index(keys=['asset', 'time'], verify_integrity=True)\n",
    "    Y = Y.squeeze() # convert to Series\n",
    "    X = X.set_index(keys=['asset', 'time'], verify_integrity=True)\n",
    "\n",
    "    # normalize RHS variables\n",
    "    column_names = list(X.columns.values)\n",
    "    for col in column_names:\n",
    "        X = normalize_column(X, col)\n",
    "\n",
    "    # fit IPCA\n",
    "    ipca = InstrumentedPCA(n_factors=k, intercept=False, n_jobs=num_cpus)\n",
    "    ipca = ipca.fit(X=X, y=Y, data_type='panel')\n",
    "\n",
    "    # Form factors and loadings to return\n",
    "    estimates = ipca.get_factors()\n",
    "    Gamma_beta_hat  = estimates[0]\n",
    "    Factors_hat   = estimates[1].transpose()\n",
    "    \n",
    "    return Gamma_beta_hat, Factors_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAsympVar(Z: np.ndarray, ob_factor: np.ndarray, factors_hat: np.ndarray,\n",
    "    gamma_beta_hat: np.ndarray, gamma_hat: np.ndarray, eta_hat: np.ndarray, \n",
    "    T: int, N: int, k: int, p: int):\n",
    "    \"\"\" Calculate the asymptotic variance of the target parameter: risk premium of observable factor.\n",
    "    \n",
    "    Returns: (float) scalar estimated variance of risk premium of observable factor.\n",
    "    \"\"\"\n",
    "    # Calculate the residuals from the time series OLS\n",
    "    residuals = ob_factor - np.matmul(factors_hat, eta_hat)\n",
    "\n",
    "    # Calculate the Z_t_j_jprime scalar\n",
    "    Z_tjjp = np.zeros((T,p,p))\n",
    "\n",
    "    for t in range(T):\n",
    "        start_time_index = t*N\n",
    "        end_time_index   = (t+1)*N\n",
    "        for j in range(p):\n",
    "            zitj_bar = np.mean(Z[start_time_index:end_time_index, j])\n",
    "            Z_tjjp[t,j,:] = (9*Z*zitj_bar).mean(axis=0)\n",
    "\n",
    "    Z_tjjp *= (N*T)**(-1)\n",
    "\n",
    "    # Calculate the Pi_t scalar\n",
    "    Pi = np.zeros((T,k,k))\n",
    "    for t in range(T):\n",
    "        Pi_t = 0\n",
    "        for j in range(p):\n",
    "            for jp in range(p):\n",
    "                gamma_beta_jp = gamma_beta_hat[jp,:]\n",
    "                gamma_beta_j  = gamma_beta_hat[j,:]\n",
    "                Pi_t += gamma_beta_jp @ gamma_beta_j.T * Z_tjjp[t,j,jp]\n",
    "        Pi[t] = Pi_t\n",
    "\n",
    "    # Calc the components of asymp matrix\n",
    "    Phi_11 = T**(-1) * (factors_hat.T @ residuals) * (residuals.T @ factors_hat)\n",
    "\n",
    "    Phi_22 = np.zeros((k,k))\n",
    "    for t in range(T):\n",
    "        for tp in range(T):\n",
    "            Phi_22 += Pi[t,:,:] @ factors_hat[t,:] * factors_hat[tp,:].T @ Pi[tp,:,:].T\n",
    "    Phi_22 *= T**(-1)\n",
    "\n",
    "    Phi_12 = np.zeros((k,k))\n",
    "    for t in range(T):\n",
    "        for tp in range(T):\n",
    "            Phi_12 += factors_hat[t,:] * residuals[t] * factors_hat[tp,:].T @ Pi[tp,:,:].T\n",
    "    Phi_12 *= T**(-1)\n",
    "\n",
    "    # Calc design matrices\n",
    "    A = (factors_hat.T @ factors_hat) / T\n",
    "\n",
    "    Z_bar = Z.reshape(T,N,p).mean(axis=0)\n",
    "    B = (gamma_beta_hat.T @ Z_bar.T @ Z_bar @ gamma_beta_hat) / N\n",
    "\n",
    "    # Calculate the target variance\n",
    "    sigma_g_2 = (gamma_hat.T @ np.linalg.inv(A) @ Phi_11 @ np.linalg.inv(A.T) @ gamma_hat\n",
    "                    + eta_hat.T @ np.linalg.inv(B) @ Phi_22 @ np.linalg.inv(B.T) @ eta_hat\n",
    "                    + gamma_hat.T @ np.linalg.inv(A) @ Phi_12 @ np.linalg.inv(B.T) @ eta_hat\n",
    "                    + eta_hat.T @ np.linalg.inv(B) @ Phi_12.T @ np.linalg.inv(A.T) @ gamma_hat)\n",
    "\n",
    "    return sigma_g_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGiglioVar(\n",
    "    G_demeaned: np.ndarray, factors_hat: np.ndarray, eta_hat: np.ndarray, gamma_hat: np.ndarray):\n",
    "    \"\"\" Calculate the asymptotic variance of the gamma_g estimator following Giglio 2021 eq. 11.\n",
    "\n",
    "    Args:\n",
    "        G_demeaned (np.ndarray): demeaned observable factors.\n",
    "        factors_hat (np.ndarray): factor estimates, mean zero.\n",
    "        eta_hat (np.ndarray): estimate of mapping from mean zero true latent factors to demeaned ob factor.\n",
    "        gamma_hat (np.ndarray): estimate of the risk premium of true latent factors.\n",
    "    \"\"\"\n",
    "    residuals = G_demeaned - factors_hat.dot(eta_hat)\n",
    "\n",
    "    phi_11 = (factors_hat.T @ factors_hat) * (residuals.T @ residuals) / T\n",
    "    phi_12 = (factors_hat.T @ factors_hat) * (residuals.T @ np.ones(T)) / T\n",
    "    phi_22 = ((factors_hat.T @ np.ones(T)).reshape(-1,1) \n",
    "            @ (np.ones(T).T @ factors_hat).reshape(1,-1)) / T\n",
    "\n",
    "    sigma_v = (factors_hat.T @ factors_hat) / T\n",
    "    inv_sigma_v = np.linalg.inv(sigma_v)\n",
    "    term1 = gamma_hat.T @ inv_sigma_v @ phi_11 @ inv_sigma_v @ gamma_hat\n",
    "    term2 = gamma_hat.T @ inv_sigma_v @ phi_12 @ inv_sigma_v @ gamma_hat\n",
    "    term3 = eta_hat.T @ phi_12.T @ inv_sigma_v @ gamma_hat\n",
    "    term4 = eta_hat.T @ phi_22 @ eta_hat\n",
    "\n",
    "    return term1 + term2 + term3 + term4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcEstimationErrors(param: np.ndarray, est: np.ndarray) -> tuple:\n",
    "    ''' Calculate estimation errors for estimates across simulations.\n",
    "    \n",
    "    Args:\n",
    "        param (np.ndarray): X by Y (by S).\n",
    "        est (np.ndarray): X by Y by S.\n",
    "\n",
    "    Where X and Y are arbitrary dimensions.\n",
    "\n",
    "    Returns: (tuple): mse, bias2, and var.\n",
    "    '''\n",
    "    if param.ndim == 1:\n",
    "        mse   = np.mean(np.square(est - param))\n",
    "        bias2 = np.mean(np.square(np.mean(est) - np.mean(param)))\n",
    "        var   = np.mean(np.square(est - np.mean(est)))\n",
    "        return mse, bias2, var\n",
    "\n",
    "    if param.ndim == 2:\n",
    "        param = np.expand_dims(param, axis=-1)\n",
    "\n",
    "    mean_param = np.mean(param, axis=2)\n",
    "    mean_est   = np.expand_dims(np.mean(est, axis=2), axis=-1)\n",
    "\n",
    "    mse   = np.mean(np.square(est - param))\n",
    "    bias2 = np.mean(np.square(np.mean(est, axis=2) - mean_param))\n",
    "    var   = np.mean(np.square(est - mean_est))\n",
    "\n",
    "    return mse, bias2, var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCoverage(est: np.ndarray, var: np.ndarray, param: np.ndarray, alpha: float=0.05) -> float:\n",
    "    \"\"\" Calculate the coverage of an inference procedure.\n",
    "    \n",
    "    Args:\n",
    "        est (np.ndarray): vector of point estimates, dimensions S by 1.\n",
    "        var (np.ndarray): vector of variances estimates, dimensions S by 1.\n",
    "        param (np.ndarray): vector of true values, dimensions S by 1.\n",
    "    \n",
    "    Returns:\n",
    "        float: Coverage probability.\n",
    "    \"\"\"\n",
    "    critical_value = norm.ppf(1-alpha/2)\n",
    "\n",
    "    upper_bound = est + critical_value * np.sqrt(var)\n",
    "    lower_bound = est - critical_value * np.sqrt(var)\n",
    "\n",
    "    coverage = np.mean((lower_bound <= param) & (param <= upper_bound))\n",
    "\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSimulation(T: int, N: int, k: int, p: int, S: int, \n",
    "    R: np.ndarray, F: np.ndarray, G: np.ndarray, Z: np.ndarray, \n",
    "    Gamma_beta: np.ndarray, Gamma: np.ndarray, eta: np.ndarray,\n",
    "    c: float, num_cpus: int):\n",
    "    # Initialize objects to fill\n",
    "    Factors_dslfm = np.zeros((T, k, S))\n",
    "    C_hat_dslfm   = np.zeros((T, p, S))\n",
    "    Beta_hat_dslfm = np.zeros((T*N, k, S))\n",
    "    Gamma_beta_hat_dslfm = np.zeros((p, k, S))\n",
    "    Gamma_beta_check_dslfm = np.zeros((p, k, S))\n",
    "    Gamma_g_hat_dslfm = np.zeros((S))\n",
    "    Gamma_g_var_dslfm = np.zeros((S))\n",
    "\n",
    "    Factors_ipca = np.zeros((T, k, S))\n",
    "    C_hat_ipca   = np.zeros((T, p, S))\n",
    "    Beta_hat_ipca = np.zeros((T*N, k, S))\n",
    "    Gamma_beta_ipca = np.zeros((p, k, S))\n",
    "    Gamma_g_hat_ipca = np.zeros((S))\n",
    "    Gamma_g_var_ipca = np.zeros((S))\n",
    "\n",
    "    Factors_giglio = np.zeros((T, k, S))\n",
    "    Factors_f_giglio = np.zeros((T, k, S))\n",
    "    Beta_hat_giglio = np.zeros((N, k, S))\n",
    "    Gamma_g_hat_giglio = np.zeros((S))\n",
    "    Gamma_g_var_giglio = np.zeros((S))\n",
    "\n",
    "    C_0 = np.zeros((T, p, S))\n",
    "    Beta_0 = np.zeros((T*N, k, S))\n",
    "    Gamma_g_0 = np.zeros((S))\n",
    "\n",
    "    # Run simulation\n",
    "    for s in range(S):\n",
    "        # Monitor progress\n",
    "        print(s)\n",
    "\n",
    "        # Calculate needed random variables\n",
    "        return_matrix = R[:,:,s].reshape(T,N)\n",
    "        giglio_return_matrix = return_matrix - np.mean(return_matrix, axis=0)\n",
    "        R_demeaned = return_matrix - return_matrix.mean(axis=0)\n",
    "        R_bar = (return_matrix - R_demeaned).mean(axis=0).reshape(-1, 1)\n",
    "        G_demeaned = G[:,:,s]-np.mean(G[:,:,s])\n",
    "        Z_bar = Z[:,:,s].reshape(T,N,p).mean(axis=0)\n",
    "\n",
    "        # Calculate true target parameters\n",
    "        Gamma_g_0[s] = eta.reshape(-1,1).T.dot(Gamma[:,:,s])[0][0]\n",
    "        C_0[:,:,s] = F[:,:,s] @ Gamma_beta.T\n",
    "        Beta_0[:,:,s] = Z[:,:,s] @ Gamma_beta\n",
    "\n",
    "        # DSLFM estimation\n",
    "        c_hat_st, gamma_beta_dslfm, factors_dslfm, gamma_beta_check_dslfm, factors_v_dslfm, gamma_beta_d_check_dslfm = fitDSLFM(\n",
    "            R[:,:,s], Z[:,:,s], T, N, k, p, c, num_cpus)\n",
    "        C_hat_dslfm[:,:,s] = c_hat_st\n",
    "        Beta_hat_dslfm[:,:,s] = Z[:,:,s] @ gamma_beta_check_dslfm\n",
    "        beta_hat_bar = np.matmul(Z_bar, gamma_beta_d_check_dslfm)\n",
    "        gamma_hat_dslfm = runOLS(R_bar, beta_hat_bar).reshape(-1)\n",
    "        eta_hat_dslfm   = runOLS(G_demeaned, factors_v_dslfm)\n",
    "        Gamma_g_hat_dslfm[s] = np.dot(eta_hat_dslfm.T, gamma_hat_dslfm)[0]\n",
    "        Gamma_g_var_dslfm[s] = calcAsympVar(Z[:,:,s], G_demeaned, factors_v_dslfm, \n",
    "            gamma_beta_dslfm, gamma_hat_dslfm, eta_hat_dslfm, \n",
    "            T, N, p)\n",
    "        Factors_dslfm[:,:,s] = factors_dslfm\n",
    "        Gamma_beta_hat_dslfm[:,:,s] = gamma_beta_dslfm\n",
    "        Gamma_beta_check_dslfm[:, :, s] = gamma_beta_check_dslfm\n",
    "\n",
    "        # IPCA estimation\n",
    "        gamma_beta_ipca, factors_ipca = fitIPCA(R[:,:,s], Z[:,:,s], T, N, k, num_cpus)\n",
    "        C_hat_ipca[:,:,s] = factors_ipca @ gamma_beta_ipca.T\n",
    "        Beta_hat_ipca[:,:,s] = Z[:,:,s] @ gamma_beta_ipca\n",
    "        gamma_hat_ipca = np.mean(factors_ipca, axis=0)\n",
    "        factors_v_ipca = factors_ipca-gamma_hat_ipca\n",
    "        eta_hat_ipca   = runOLS(G_demeaned, factors_v_ipca)\n",
    "        Gamma_g_hat_ipca[s] = np.dot(eta_hat_ipca.T, gamma_hat_ipca)[0]\n",
    "        Gamma_g_var_ipca[s] = calcAsympVar(G_demeaned, factors_v_ipca, gamma_hat_ipca, \n",
    "            eta_hat_ipca, T)\n",
    "        Factors_ipca[:,:,s] = factors_ipca\n",
    "        Gamma_beta_ipca[:,:,s] = gamma_beta_ipca\n",
    "\n",
    "        # Giglio Estimation\n",
    "        factors_giglio, beta_giglio = fitBaiPCA(giglio_return_matrix, T, k, N)\n",
    "        Beta_hat_giglio[:,:,s] = beta_giglio\n",
    "        gamma_hat_giglio = runOLS(R_bar, beta_giglio)\n",
    "        eta_hat_giglio = runOLS(G_demeaned, factors_giglio)\n",
    "        Factors_giglio[:,:,s] = factors_giglio\n",
    "        Factors_f_giglio[:,:,s] = factors_giglio + gamma_hat_giglio.reshape(-1)\n",
    "        Gamma_g_hat_giglio[s] = eta_hat_giglio.T.dot(gamma_hat_giglio)\n",
    "        Gamma_g_var_giglio[s] = calcGiglioVar(G_demeaned, factors_giglio, eta_hat_giglio, gamma_hat_giglio)\n",
    "\n",
    "    return (C_hat_dslfm, Beta_hat_dslfm, Factors_dslfm, Gamma_beta_check_dslfm, Gamma_beta_hat_dslfm, \n",
    "        Gamma_g_hat_dslfm, Gamma_g_var_dslfm,\n",
    "        C_hat_ipca, Beta_hat_ipca, Factors_ipca, Gamma_beta_ipca, Gamma_g_hat_ipca,\n",
    "        Beta_hat_giglio, Factors_f_giglio, Gamma_g_hat_giglio, Gamma_g_var_giglio,\n",
    "        C_0, Beta_0, Gamma_g_0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set simulation parameters\n",
    "    PANEL_WEEKLY_IN_FP = '../data/clean/panel_weekly.pkl'\n",
    "    ASSET_UNI_IN_FP    = '../data/clean/asset_universe_dict.pickle'\n",
    "    LHS_COL            = 'r_ex_tp7'\n",
    "    NUM_CPUS           = 20\n",
    "    T                  = 100\n",
    "    N                  = 200\n",
    "    k                  = 3\n",
    "    p                  = 10\n",
    "    SPARSE_S           = int(p/5)\n",
    "    S                  = 3\n",
    "    FIT_ALPHA          = False\n",
    "    ETA                = np.array([1]+list(np.zeros(k-1)))\n",
    "    SIGMA_F_2          = 0.01\n",
    "    SIGMA_Z_2          = 0.01\n",
    "    SIGMA_G_2          = 0.01\n",
    "    SIGMA_E_2          = 0.01\n",
    "    C                  = 1\n",
    "\n",
    "    # # TEMP TODO LOOP OVER NOISE PARAMS TO GEN RESULTS ACROSS\n",
    "    # for SIGMA_F_2 in [0.5, 0.1, 0.05]:\n",
    "    #     for SIGMA_Z_2 in [0.5, 0.1, 0.05]:\n",
    "    #         for SIGMA_G_2 in [0.1, 0.05, 0.005]:\n",
    "    #             for SIGMA_E_2 in [0.5, 0.1, 0.05]:\n",
    "                    \n",
    "    # read in data\n",
    "    with open(ASSET_UNI_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    panel_df = pd.read_pickle(PANEL_WEEKLY_IN_FP)\n",
    "\n",
    "    # fit ipca in the panel to obtain factors, loadings, and r^2_total\n",
    "    ipca_factors, ipca_loadings, r2_total_emp, selected_columns = fitIPCAInPanelForEstimatedFactorsAndLoadings(\n",
    "        panel_df, LHS_COL, p, k, FIT_ALPHA, NUM_CPUS\n",
    "    )\n",
    "\n",
    "    # build the data\n",
    "    R, Gamma_beta, F, G, Z, Gamma, e, e_g = DGP(T, N, k, p, SPARSE_S, S, ETA,\n",
    "        SIGMA_F_2, SIGMA_Z_2, SIGMA_G_2, SIGMA_E_2,\n",
    "        ipca_factors, ipca_loadings, r2_total_emp, selected_columns, panel_df, NUM_CPUS)\n",
    "    \n",
    "    # run the simulation\n",
    "    (C_hat_dslfm, Beta_hat_dslfm, Factors_dslfm, Gamma_beta_check_dslfm, Gamma_beta_dslfm, \n",
    "        Gamma_g_hat_dslfm, Gamma_g_var_dslfm,\n",
    "        C_hat_ipca, Beta_hat_ipca, Factors_ipca, Gamma_beta_ipca, Gamma_g_hat_ipca,\n",
    "        Beta_hat_giglio, Factors_f_giglio, Gamma_g_hat_giglio, Gamma_g_var_giglio,\n",
    "        C_0, Beta_0, Gamma_g_0) = runSimulation(T, N, k, p, S, R, F, G, Z, Gamma_beta, Gamma, ETA, C, NUM_CPUS)\n",
    "    \n",
    "    # Calculate estimation errors\n",
    "    mse_c_dslfm, bias2_c_dslfm, var_c_dslfm = calcEstimationErrors(C_0, C_hat_dslfm)\n",
    "    mse_factors_dslfm, bias2_factors_dslfm, var_factors_dslfm = calcEstimationErrors(F, Factors_dslfm)\n",
    "    mse_gamma_b_dslfm, bias2_gamma_b_dslfm, var_gamma_b_dslfm = calcEstimationErrors(Gamma_beta, Gamma_beta_check_dslfm)\n",
    "    mse_beta_dslfm, bias2_beta_dslfm, var_beta_dslfm = calcEstimationErrors(Beta_0.reshape(T,N,k,S).mean(axis=0), \n",
    "                                                                            Beta_hat_dslfm.reshape(T,N,k,S).mean(axis=0))\n",
    "    mse_gamma_g_dslfm, bias2_gamma_g_dslfm, var_gamma_g_dslfm = calcEstimationErrors(Gamma_g_0, Gamma_g_hat_dslfm)\n",
    "    \n",
    "    mse_c_ipca, bias2_c_ipca, var_c_ipca = calcEstimationErrors(C_0, C_hat_ipca)\n",
    "    mse_factors_ipca, bias2_factors_ipca, var_factors_ipca = calcEstimationErrors(F, Factors_ipca)\n",
    "    mse_gamma_b_ipca, bias2_gamma_b_ipca, var_gamma_b_ipca = calcEstimationErrors(Gamma_beta, Gamma_beta_ipca)\n",
    "    mse_beta_ipca, bias2_beta_ipca, var_beta_ipca = calcEstimationErrors(Beta_0.reshape(T,N,k,S).mean(axis=0), \n",
    "                                                                        Beta_hat_ipca.reshape(T,N,k,S).mean(axis=0))\n",
    "    mse_gamma_g_ipca, bias2_gamma_g_ipca, var_gamma_g_ipca = calcEstimationErrors(Gamma_g_0, Gamma_g_hat_ipca)\n",
    "\n",
    "    mse_factors_giglio, bias2_factors_giglio, var_factors_giglio = calcEstimationErrors(F, Factors_f_giglio)\n",
    "    mse_beta_giglio, bias2_beta_giglio, var_beta_giglio = calcEstimationErrors(Beta_0.reshape(T,N,k,S).mean(axis=0), \n",
    "                                                                            Beta_hat_giglio)\n",
    "    mse_gamma_g_giglio, bias2_gamma_g_giglio, var_gamma_g_giglio = calcEstimationErrors(Gamma_g_0, Gamma_g_hat_giglio)\n",
    "\n",
    "    # Calculate coverage\n",
    "    cov90_dslfm = calcCoverage(Gamma_g_hat_dslfm, Gamma_g_var_dslfm, Gamma_g_0, alpha=0.1)\n",
    "    cov90_giglio = calcCoverage(Gamma_g_hat_giglio, Gamma_g_var_giglio, Gamma_g_0, alpha=0.1)\n",
    "    cov95_dslfm = calcCoverage(Gamma_g_hat_dslfm, Gamma_g_var_dslfm, Gamma_g_0, alpha=0.05)\n",
    "    cov95_giglio = calcCoverage(Gamma_g_hat_giglio, Gamma_g_var_giglio, Gamma_g_0, alpha=0.05)\n",
    "\n",
    "    # Save results\n",
    "    timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_df = pd.DataFrame(data = {'timestamp': [timestr, timestr, timestr],\n",
    "        'S': [S, S, S], 'T': [T, T, T], 'N': [N, N, N], 'p': [p, p, p], 'k': [k, k, k],\n",
    "        'sparse_s': [SPARSE_S, SPARSE_S, SPARSE_S], 'c': [C, C, C], \n",
    "        'sigma_f_2': [SIGMA_F_2, SIGMA_F_2, SIGMA_F_2], 'sigma_z_2': [SIGMA_Z_2, SIGMA_Z_2, SIGMA_Z_2], \n",
    "        'sigma_g_2': [SIGMA_G_2, SIGMA_G_2, SIGMA_G_2], 'sigma_e_2': [SIGMA_E_2, SIGMA_E_2, SIGMA_E_2],\n",
    "        'method': ['dslfm', 'ipca', 'giglio'],\n",
    "        'mse_c': [mse_c_dslfm, mse_c_ipca, 0],\n",
    "        'bias2_c': [bias2_c_dslfm, bias2_c_ipca, 0],\n",
    "        'var_c': [var_c_dslfm, var_c_ipca, 0],\n",
    "        'mse_f': [mse_factors_dslfm, mse_factors_ipca, mse_factors_giglio], \n",
    "        'bias2_f': [bias2_factors_dslfm, bias2_factors_ipca, bias2_factors_giglio], \n",
    "        'var_f': [var_factors_dslfm, var_factors_ipca, var_factors_giglio],\n",
    "        'mse_g_b': [mse_gamma_b_dslfm, mse_gamma_b_ipca, 0], \n",
    "        'bias2_g_b': [bias2_gamma_b_dslfm, bias2_gamma_b_ipca, 0], \n",
    "        'var_g_b': [var_gamma_b_dslfm, var_gamma_b_ipca, 0], \n",
    "        'mse_b': [mse_beta_dslfm, mse_beta_ipca, mse_beta_giglio],        \n",
    "        'bias2_b': [bias2_beta_dslfm, bias2_beta_ipca, bias2_beta_giglio],\n",
    "        'var_b': [var_beta_dslfm, var_beta_ipca, var_beta_giglio],\n",
    "        'mse_g_g': [mse_gamma_g_dslfm, mse_gamma_g_ipca, mse_gamma_g_giglio], \n",
    "        'bias2_g_g': [bias2_gamma_g_dslfm, bias2_gamma_g_ipca, bias2_gamma_g_giglio], \n",
    "        'var_g_g': [var_gamma_g_dslfm, var_gamma_g_ipca, var_gamma_g_giglio], \n",
    "        'cov90_g_g': [cov90_dslfm, 0, cov90_giglio],\n",
    "        'cov95_g_g': [cov95_dslfm, 0, cov95_giglio]})\n",
    "    results_df.to_csv('../output/high_dim_fm/sim_'+timestr+'.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
