{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import json.decoder\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCMCApiCall(url: str, params: dict, retries: int=3) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\" makes an API call to CoinGecko using the provided url and parameters.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The API endpoint URL to call.\n",
    "        params (dict): A dictionary of parameters to include in the API call.\n",
    "        retries (int): The number of times to retry the API call if it fails. Default is 3.\n",
    "        \n",
    "    Returns:\n",
    "        response.json() (dict): the data from the api response, or None if the api call failed.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=3)\n",
    "        except requests.exceptions.Timeout:\n",
    "            # Timeout error, retry after a short delay\n",
    "            print('The API call timed out, retrying...')\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        if response.ok:\n",
    "            try:\n",
    "                return response.json()\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f'Error decoding JSON response: {str(e)}')\n",
    "        else:\n",
    "            # There was an error, retry after a short delay\n",
    "            print(f'The API call failed with status code {response.status_code}, retrying...')\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    print('The api call failed after 3 attempts.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullAssetCovariates(base_url: str, base_params: dict, gecko_id_universe: list, panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Pull various asset covariates for a given universe of CoinGecko IDs.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL for the Coingecko API.\n",
    "        base_params (dict): A dictionary containing the basic parameters for the Coingecko API call.\n",
    "        gecko_id_universe (list): A list of unique gecko ids to pull.\n",
    "        panel_df (pd.DataFrame): panel data with columns 'date', 'asset_gecko', 'usd_per_token_cg', \n",
    "                                 'usd_mcap_cg', and 'usd_volume_24h_cg'.\n",
    "\n",
    "    Returns:\n",
    "        asset_covars_df (pd.DataFrame): panel data with asset covariates.\n",
    "    \"\"\"\n",
    "\n",
    "    # set up object to store all\n",
    "    gecko_covars_dict = {'date':[],\n",
    "                        'asset_gecko': [],\n",
    "                        'twitter_followers': [],\n",
    "                        'reddit_average_posts_48h': [],\n",
    "                        'reddit_average_comments_48h': [],\n",
    "                        'reddit_subscribers': [],\n",
    "                        'reddit_accounts_active_48h': [],\n",
    "                        'forks': [],\n",
    "                        'stars': [],\n",
    "                        'subscribers': [],\n",
    "                        'total_issues': [],\n",
    "                        'closed_issues': [],\n",
    "                        'pull_requests_merged': [],\n",
    "                        'pull_request_contributors': [],\n",
    "                        'code_additions_4_weeks': [],\n",
    "                        'code_deletions_4_weeks': [],\n",
    "                        'commit_count_4_weeks': [],\n",
    "                        'alexa_rank': []}\n",
    "\n",
    "    # loop over assets to pull\n",
    "    for i in range(len(gecko_id_universe)):\n",
    "        # set current id to pull\n",
    "        gecko_id = gecko_id_universe[i]\n",
    "\n",
    "        # monitor progress\n",
    "        print(f\"Processing id #{i+1} ({(i+1)/len(gecko_id_universe)*100:.2f}%): {gecko_id}\")\n",
    "        \n",
    "        # set up endpoint\n",
    "        endpoint = f\"/coins/{gecko_id}/history\"\n",
    "        url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "        # set up params\n",
    "        params = base_params.copy()\n",
    "        params['id'] = gecko_id\n",
    "\n",
    "        # extract dates for this asset\n",
    "        first_date = np.min(panel_df[panel_df.asset_gecko==gecko_id]['date']).strftime(format='%d-%m-%Y')\n",
    "        last_date  = np.max(panel_df[panel_df.asset_gecko==gecko_id]['date']).strftime(format='%d-%m-%Y')\n",
    "        all_dates  = getDateList(first_date, last_date)\n",
    "\n",
    "        for current_date in all_dates:\n",
    "            # update params\n",
    "            params['date'] = current_date\n",
    "\n",
    "            # make the call\n",
    "            response_json = makeCMCApiCall(url, params)\n",
    "\n",
    "            # add data to results dict\n",
    "            gecko_covars_dict['date'].append(np.datetime64(datetime.strptime(current_date, '%d-%m-%Y')+timedelta(days=1), 'D'))\n",
    "            gecko_covars_dict['asset_gecko'].append(response_json['id'])\n",
    "            if 'community_data' in response_json.keys():\n",
    "                gecko_covars_dict['twitter_followers'].append(response_json['community_data']['twitter_followers'])\n",
    "                gecko_covars_dict['reddit_average_posts_48h'].append(response_json['community_data']['reddit_average_posts_48h'])\n",
    "                gecko_covars_dict['reddit_average_comments_48h'].append(response_json['community_data']['reddit_average_comments_48h'])\n",
    "                gecko_covars_dict['reddit_subscribers'].append(response_json['community_data']['reddit_subscribers'])\n",
    "                gecko_covars_dict['reddit_accounts_active_48h'].append(response_json['community_data']['reddit_accounts_active_48h'])  \n",
    "            else:\n",
    "                gecko_covars_dict['twitter_followers'].append(None)\n",
    "                gecko_covars_dict['reddit_average_posts_48h'].append(None)\n",
    "                gecko_covars_dict['reddit_average_comments_48h'].append(None)\n",
    "                gecko_covars_dict['reddit_subscribers'].append(None)\n",
    "                gecko_covars_dict['reddit_accounts_active_48h'].append(None)\n",
    "            if 'developer_data' in response_json.keys():\n",
    "                gecko_covars_dict['forks'].append(response_json['developer_data']['forks'])\n",
    "                gecko_covars_dict['stars'].append(response_json['developer_data']['stars'])\n",
    "                gecko_covars_dict['subscribers'].append(response_json['developer_data']['subscribers'])\n",
    "                gecko_covars_dict['total_issues'].append(response_json['developer_data']['total_issues'])\n",
    "                gecko_covars_dict['closed_issues'].append(response_json['developer_data']['closed_issues'])\n",
    "                gecko_covars_dict['pull_requests_merged'].append(response_json['developer_data']['pull_requests_merged'])\n",
    "                gecko_covars_dict['pull_request_contributors'].append(response_json['developer_data']['pull_request_contributors'])\n",
    "                gecko_covars_dict['code_additions_4_weeks'].append(response_json['developer_data']['code_additions_deletions_4_weeks']['additions'])\n",
    "                gecko_covars_dict['code_deletions_4_weeks'].append(response_json['developer_data']['code_additions_deletions_4_weeks']['deletions'])\n",
    "                gecko_covars_dict['commit_count_4_weeks'].append(response_json['developer_data']['commit_count_4_weeks'])\n",
    "            else:\n",
    "                gecko_covars_dict['forks'].append(None)\n",
    "                gecko_covars_dict['stars'].append(None)\n",
    "                gecko_covars_dict['subscribers'].append(None)\n",
    "                gecko_covars_dict['total_issues'].append(None)\n",
    "                gecko_covars_dict['closed_issues'].append(None)\n",
    "                gecko_covars_dict['pull_requests_merged'].append(None)\n",
    "                gecko_covars_dict['pull_request_contributors'].append(None)\n",
    "                gecko_covars_dict['code_additions_4_weeks'].append(None)\n",
    "                gecko_covars_dict['code_deletions_4_weeks'].append(None)\n",
    "                gecko_covars_dict['commit_count_4_weeks'].append(None)\n",
    "            if 'public_interest_stats' in response_json.keys():\n",
    "                gecko_covars_dict['alexa_rank'].append(response_json['public_interest_stats']['alexa_rank'])\n",
    "            else:\n",
    "                gecko_covars_dict['alexa_rank'].append(None)\n",
    "\n",
    "            # Space out the calls and increment counter\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    # convert to df to return\n",
    "    asset_covars_df = pd.DataFrame(gecko_covars_dict)\n",
    "\n",
    "    # clean up the data\n",
    "    asset_covars_df = asset_covars_df[~asset_covars_df.duplicated(subset=['date', 'gecko_id'])]\n",
    "\n",
    "    return asset_covars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    api_fp = '../../admin/coingecko.txt'\n",
    "    base_url = \"https://pro-api.coingecko.com/api/v3\"\n",
    "\n",
    "    # import api key and set base parameters\n",
    "    with open(api_fp) as f:\n",
    "        API_KEY = f.readlines()\n",
    "        API_KEY = API_KEY[0].strip()\n",
    "    base_params = {'x_cg_pro_api_key': API_KEY}\n",
    "\n",
    "    # Test it is working\n",
    "    url = f\"{base_url}/ping\"\n",
    "    r = requests.get(url, params=base_params)\n",
    "    print(r.json()['gecko_says'])\n",
    "\n",
    "    # TODO pull in the cg universe from the inclusion crit stuff\n",
    "    \n",
    "    # pull additional asset covariates\n",
    "    asset_covars_df = pullAssetCovariates(base_url, base_params, gecko_id_universe, panel_df)\n",
    "\n",
    "    # TODO SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# come back to pull the asset covariates for only our final panel of data b/c it is so many API calls\n",
    "# --move the code around to do so"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
