{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPanel(df: pd.DataFrame, cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Fix columns with known issues\n",
    "    df = df.drop(columns='alexa_rank', axis=1)\n",
    "    df = df.rename(columns={'usd_volume_cg': 'usd_volume_24h_cg',\n",
    "                            'forks': 'github_forks',\n",
    "                            'stars': 'github_stars',\n",
    "                            'subscribers': 'github_subscribers',\n",
    "                            'total_issues': 'github_total_issues',\n",
    "                            'closed_issues': 'github_closed_issues',\n",
    "                            'pull_requests_merged': 'github_pull_requests_merged',\n",
    "                            'pull_requests_contributors': 'github_pull_requests_contributors',\n",
    "                            'code_additions_4_weeks': 'github_code_additions_4_weeks',\n",
    "                            'code_deletions_4_weeks': 'github_code_deletions_4_weeks',\n",
    "                            'commit_count_4_weeks': 'github_commit_count_4_weeks'})\n",
    "    df['github_code_deletions_4_weeks'] = np.abs(df['github_code_deletions_4_weeks'])\n",
    "    df['usd_mcap_cg'] = df.groupby('asset_cg')['usd_mcap_cg'].ffill()\n",
    "\n",
    "    # Convert all columns to float32\n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    cols.remove('asset_cg')\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # Convert date column to datetime format and remove timezone information\n",
    "    df['date'] = pd.to_datetime(df['date'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # Confirm no mising dates nor asset ids\n",
    "    assert 0 == df.date.isnull().sum()\n",
    "    assert 0 == df.asset_cg.isnull().sum()\n",
    "\n",
    "    # Confirm all asset ids are in cw\n",
    "    assert cw_df.asset_cg.is_unique\n",
    "    asset_ids = list(cw_df.asset_cg.values)\n",
    "    assert df.shape[0] == df[df.asset_cg.isin(asset_ids)].shape[0]\n",
    "\n",
    "    # Cut down to relevant dates between July 1, 2016 and January 2, 2023\n",
    "    df = df[(df['date'] >= '2016-07-01') & (df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # For price, volume, and mcap columns, ensure no missing obs and below thresholds\n",
    "    thresholds = {'usd_per_token_cg': 1e9, 'usd_volume_24h_cg': 1e11, 'usd_mcap_cg': 1e13}\n",
    "    for col, thresh in thresholds.items():\n",
    "        assert 0 == df[col].isnull().sum()\n",
    "        df = df[df[col] < thresh]\n",
    "\n",
    "    # For each asset_cg, replace missing values with zero if there's at least one non-missing observation in the column\n",
    "    for col in cols:\n",
    "        has_nonmissing = df.groupby('asset_cg')[col].transform(lambda x: x.notnull().any())\n",
    "        df.loc[has_nonmissing, col] = df.loc[has_nonmissing, col].fillna(0)\n",
    "\n",
    "    # Ensure no negative numbers\n",
    "    assert 0 == (df[cols]<0).sum().sum()\n",
    "\n",
    "    # Loop over all assets to add any missing days and fill missing price, volume, and mcap data\n",
    "    final_df = pd.DataFrame()\n",
    "    assets = list(np.unique(df.asset_cg.values))\n",
    "    for asset in assets:\n",
    "        # subset to asset of interest\n",
    "        asset_df = df[df.asset_cg==asset].copy()\n",
    "\n",
    "        # determine the date gaps\n",
    "        date_gaps = []\n",
    "        dates = asset_df.date.values\n",
    "        for i in range(1, len(dates)):\n",
    "            date_gaps.append(np.timedelta64(dates[i]-dates[i-1], 'D').astype(int))\n",
    "\n",
    "        # determine new days to add\n",
    "        indices_to_expand = [i for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32)]\n",
    "        num_datetime_to_add = [date_gaps[i] for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32)]\n",
    "        start_datetimes = dates[indices_to_expand]\n",
    "        new_datetimes = []\n",
    "        for i in range(len(start_datetimes)):\n",
    "            start_datetime = start_datetimes[i]\n",
    "            datetime_to_add = num_datetime_to_add[i]\n",
    "            for j in range(1, datetime_to_add):\n",
    "                new_datetimes.append(start_datetime+np.timedelta64(24*(j), 'h'))\n",
    "\n",
    "        # add the new days to the asset df\n",
    "        new_asset_df = pd.DataFrame(data={'date': new_datetimes})\n",
    "        new_asset_df['asset_cg'] = asset\n",
    "        asset_df = pd.concat((asset_df, new_asset_df))\n",
    "        asset_df = asset_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # forward fill \n",
    "        asset_df = asset_df.ffill()\n",
    "\n",
    "        # add data to master df\n",
    "        final_df = pd.concat((final_df, asset_df))\n",
    "\n",
    "    # reset df name\n",
    "    del df\n",
    "    df = final_df.copy()\n",
    "\n",
    "    # Set column order\n",
    "    cols.remove('usd_per_token_cg')\n",
    "    cols.remove('usd_volume_24h_cg')\n",
    "    cols.remove('usd_mcap_cg')\n",
    "    cols.sort()\n",
    "    df = df[['date', 'asset_cg', 'usd_per_token_cg', 'usd_mcap_cg', 'usd_volume_24h_cg']+cols]\n",
    "\n",
    "    # Ensure all columns have cg in name\n",
    "    df.columns = [col if col == 'date' or col.endswith('_cg') else col + '_cg' for col in df.columns]\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not df.duplicated(subset=['date', 'asset_cg']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    df = df.sort_values(by=['date', 'asset_cg']).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCrosswalk(cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Cleans a crosswalk DataFrame by ensuring that both asset IDs are unique, \n",
    "    sorting the columns, sorting by the asset name, and resetting the index.\n",
    "\n",
    "    Args:\n",
    "        cw_df: A pandas DataFrame representing the crosswalk.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the cleaned crosswalk.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If either of the asset IDs are not unique.\n",
    "    \"\"\"\n",
    "    # Ensure both asset ids are unique\n",
    "    assert cw_df['asset_san'].is_unique, \"asset_san must be unique\"\n",
    "    assert cw_df['asset_cm'].is_unique, \"asset_san must be unique\"\n",
    "\n",
    "    # Sort columns\n",
    "    cw_df = cw_df[['asset_cm', 'asset_san']]\n",
    "\n",
    "    # Sort by cm asset name and reset index\n",
    "    cw_df = cw_df.sort_values(by='asset_cm').reset_index(drop=True)\n",
    "\n",
    "    return cw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    CW_IN_FP = '../data/raw/san_coinmetrics_cw.pkl'\n",
    "    PANEL_IN_FP = \"../data/raw/san_panel.pkl\"\n",
    "    MACRO_IN_FP = '../data/raw/san_macro.pkl'\n",
    "    CW_OUT_FP = '../data/derived/san_cm_cw.pkl'\n",
    "    PANEL_OUT_FP = \"../data/derived/san_panel.pkl\"\n",
    "    MACRO_OUT_FP = '../data/derived/san_macro.pkl'\n",
    "\n",
    "    # import data\n",
    "    cw_df = pd.read_pickle(CW_IN_FP)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "    macro_df = pd.read_pickle(MACRO_IN_FP)\n",
    "\n",
    "    # clean data\n",
    "    # df = cleanPanel(df, cw_df)\n",
    "    # cw_df = cleanCrosswalk(cw_df)\n",
    "    \n",
    "    # macro_df = cleanMacro(macro_df)\n",
    "\n",
    "    # # save data\n",
    "    # cw_df.to_pickle(CW_OUT_FP)\n",
    "    # df.to_pickle(PANEL_OUT_FP)\n",
    "    # macro_df.to_pickle(MACRO_OUT_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix columns with known issues\n",
    "df = df.rename(columns={'asset': 'asset_san'})\n",
    "df = df.merge(cw_df[['asset_san', 'category_san']], on='asset_san', how='left', validate='many_to_one')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datetime',\n",
       " 'asset_san',\n",
       " 'active_addresses_1h',\n",
       " 'active_deposits',\n",
       " 'active_deposits_per_exchange',\n",
       " 'active_holders_distribution_combined_balance_over_1',\n",
       " 'active_holders_distribution_combined_balance_over_10',\n",
       " 'active_holders_distribution_combined_balance_over_100',\n",
       " 'active_holders_distribution_combined_balance_over_100k',\n",
       " 'active_holders_distribution_combined_balance_over_10k',\n",
       " 'active_holders_distribution_combined_balance_over_1M',\n",
       " 'active_holders_distribution_combined_balance_over_1k',\n",
       " 'active_holders_distribution_combined_balance_total',\n",
       " 'active_holders_distribution_over_1',\n",
       " 'active_holders_distribution_over_10',\n",
       " 'active_holders_distribution_over_100',\n",
       " 'active_holders_distribution_over_100k',\n",
       " 'active_holders_distribution_over_10k',\n",
       " 'active_holders_distribution_over_1M',\n",
       " 'active_holders_distribution_over_1k',\n",
       " 'active_holders_distribution_total',\n",
       " 'active_withdrawals',\n",
       " 'active_withdrawals_per_exchange',\n",
       " 'age_consumed',\n",
       " 'age_destroyed',\n",
       " 'all_known_balance',\n",
       " 'amount_in_exchange_top_holders',\n",
       " 'amount_in_non_exchange_top_holders',\n",
       " 'amount_in_top_holders',\n",
       " 'cex_balance',\n",
       " 'cexes_to_defi_flow',\n",
       " 'cexes_to_dex_flow',\n",
       " 'cexes_to_dex_traders_flow',\n",
       " 'cexes_to_traders_flow',\n",
       " 'cexes_to_whale_flow',\n",
       " 'circulation',\n",
       " 'circulation_1d',\n",
       " 'circulation_2y',\n",
       " 'circulation_30d',\n",
       " 'circulation_365d',\n",
       " 'circulation_3y',\n",
       " 'circulation_5y',\n",
       " 'circulation_7d',\n",
       " 'circulation_90d',\n",
       " 'daily_active_addresses',\n",
       " 'defi_balance',\n",
       " 'defi_cex_balance',\n",
       " 'defi_dex_balance',\n",
       " 'defi_exchange_balance',\n",
       " 'defi_to_cexes_flow',\n",
       " 'defi_to_dex_traders_flow',\n",
       " 'defi_to_dexes_flow',\n",
       " 'defi_to_exchanges_flow',\n",
       " 'defi_to_traders_flow',\n",
       " 'defi_to_whale_flow',\n",
       " 'deposit_balance',\n",
       " 'deposit_transactions',\n",
       " 'deposit_transactions_per_exchange',\n",
       " 'dev_activity',\n",
       " 'dev_activity_contributors_count',\n",
       " 'dex_balance',\n",
       " 'dex_cex_balance',\n",
       " 'dex_to_cexes_flow',\n",
       " 'dex_trader_balance',\n",
       " 'dex_traders_cex_balance',\n",
       " 'dex_traders_defi_balance',\n",
       " 'dex_traders_dex_balance',\n",
       " 'dex_traders_exchange_balance',\n",
       " 'dex_traders_to_cexes_flow',\n",
       " 'dex_traders_to_defi_flow',\n",
       " 'dex_traders_to_dexes_flow',\n",
       " 'dex_traders_to_exchanges_flow',\n",
       " 'dex_traders_to_whale_flow',\n",
       " 'dex_traders_whale_balance',\n",
       " 'dexes_to_defi_flow',\n",
       " 'dexes_to_dex_traders_flow',\n",
       " 'dexes_to_traders_flow',\n",
       " 'dexes_to_whale_flow',\n",
       " 'dormant_circulation_180d',\n",
       " 'dormant_circulation_365d',\n",
       " 'dormant_circulation_90d',\n",
       " 'exchange_balance',\n",
       " 'exchange_inflow',\n",
       " 'exchange_inflow_usd',\n",
       " 'exchange_outflow',\n",
       " 'exchange_outflow_usd',\n",
       " 'exchanges_to_defi_flow',\n",
       " 'exchanges_to_dex_traders_flow',\n",
       " 'exchanges_to_genesis_flow',\n",
       " 'exchanges_to_traders_flow',\n",
       " 'exchanges_to_whales_flow',\n",
       " 'github_activity',\n",
       " 'github_activity_contributors_count',\n",
       " 'holders_distribution_combined_balance_over_1',\n",
       " 'holders_distribution_combined_balance_over_10',\n",
       " 'holders_distribution_combined_balance_over_100',\n",
       " 'holders_distribution_combined_balance_over_100k',\n",
       " 'holders_distribution_combined_balance_over_10k',\n",
       " 'holders_distribution_combined_balance_over_1M',\n",
       " 'holders_distribution_combined_balance_over_1k',\n",
       " 'holders_distribution_combined_balance_total',\n",
       " 'holders_distribution_over_1',\n",
       " 'holders_distribution_over_10',\n",
       " 'holders_distribution_over_100',\n",
       " 'holders_distribution_over_100k',\n",
       " 'holders_distribution_over_10k',\n",
       " 'holders_distribution_over_1M',\n",
       " 'holders_distribution_over_1k',\n",
       " 'holders_distribution_total',\n",
       " 'marketcap_usd',\n",
       " 'mean_age',\n",
       " 'mean_dollar_invested_age',\n",
       " 'mean_realized_price_usd',\n",
       " 'mvrv_long_short_diff_usd',\n",
       " 'mvrv_usd',\n",
       " 'network_growth',\n",
       " 'nvt',\n",
       " 'nvt_transaction_volume',\n",
       " 'payments_count',\n",
       " 'percent_of_total_supply_in_profit',\n",
       " 'percent_of_total_supply_on_exchanges',\n",
       " 'price_usd',\n",
       " 'realized_value_usd',\n",
       " 'sentiment_balance_reddit',\n",
       " 'sentiment_balance_total',\n",
       " 'sentiment_balance_twitter',\n",
       " 'sentiment_balance_twitter_crypto',\n",
       " 'sentiment_negative_reddit',\n",
       " 'sentiment_negative_total',\n",
       " 'sentiment_negative_twitter',\n",
       " 'sentiment_negative_twitter_crypto',\n",
       " 'sentiment_positive_reddit',\n",
       " 'sentiment_positive_total',\n",
       " 'sentiment_positive_twitter',\n",
       " 'sentiment_positive_twitter_crypto',\n",
       " 'sentiment_volume_consumed_reddit',\n",
       " 'sentiment_volume_consumed_total',\n",
       " 'sentiment_volume_consumed_twitter',\n",
       " 'sentiment_volume_consumed_twitter_crypto',\n",
       " 'social_dominance_reddit',\n",
       " 'social_dominance_total',\n",
       " 'social_dominance_twitter',\n",
       " 'social_dominance_twitter_crypto',\n",
       " 'social_volume_reddit',\n",
       " 'social_volume_total',\n",
       " 'social_volume_twitter',\n",
       " 'social_volume_twitter_crypto',\n",
       " 'stock_to_flow',\n",
       " 'supply_on_exchanges',\n",
       " 'supply_outside_exchanges',\n",
       " 'total_supply',\n",
       " 'total_supply_in_profit',\n",
       " 'trader_balance',\n",
       " 'traders_cex_balance',\n",
       " 'traders_defi_balance',\n",
       " 'traders_dex_balance',\n",
       " 'traders_exchange_balance',\n",
       " 'traders_to_cexes_flow',\n",
       " 'traders_to_defi_flow',\n",
       " 'traders_to_dexes_flow',\n",
       " 'traders_to_exchanges_flow',\n",
       " 'traders_to_whale_flow',\n",
       " 'traders_whale_balance',\n",
       " 'transaction_volume',\n",
       " 'transactions_count',\n",
       " 'unique_social_volume_total_1h',\n",
       " 'volume_usd',\n",
       " 'whale_balance',\n",
       " 'whale_cex_balance',\n",
       " 'whale_defi_balance',\n",
       " 'whale_dex_balance',\n",
       " 'whale_to_cexes_flow',\n",
       " 'whale_to_defi_flow',\n",
       " 'whale_to_dex_traders_flow',\n",
       " 'whale_to_dexes_flow',\n",
       " 'whale_to_traders_flow',\n",
       " 'whales_exchange_balance',\n",
       " 'whales_to_exchanges_flow',\n",
       " 'withdrawal_balance',\n",
       " 'withdrawal_transactions',\n",
       " 'category_san']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean datetime column to form proper date column\n",
    "# TODO SCOPE TO ENSURE I GET THE TIMESTAMP IN HOW I CLEAN IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['active_addresses_1h',\n",
    " 'active_deposits',\n",
    " 'active_deposits_per_exchange',\n",
    " 'active_holders_distribution_combined_balance_over_1',\n",
    " 'active_holders_distribution_combined_balance_over_10',\n",
    " 'active_holders_distribution_combined_balance_over_100',\n",
    " 'active_holders_distribution_combined_balance_over_100k',\n",
    " 'active_holders_distribution_combined_balance_over_10k',\n",
    " 'active_holders_distribution_combined_balance_over_1M',\n",
    " 'active_holders_distribution_combined_balance_over_1k',\n",
    " 'active_holders_distribution_combined_balance_total',\n",
    " 'active_holders_distribution_over_1',\n",
    " 'active_holders_distribution_over_10',\n",
    " 'active_holders_distribution_over_100',\n",
    " 'active_holders_distribution_over_100k',\n",
    " 'active_holders_distribution_over_10k',\n",
    " 'active_holders_distribution_over_1M',\n",
    " 'active_holders_distribution_over_1k',\n",
    " 'active_holders_distribution_total',\n",
    " 'active_withdrawals',\n",
    " 'active_withdrawals_per_exchange',\n",
    " 'age_consumed',\n",
    " 'age_destroyed',\n",
    " 'all_known_balance',\n",
    " 'amount_in_exchange_top_holders',\n",
    " 'amount_in_non_exchange_top_holders',\n",
    " 'amount_in_top_holders',\n",
    " 'cex_balance',\n",
    " 'cexes_to_defi_flow',\n",
    " 'cexes_to_dex_flow',\n",
    " 'cexes_to_dex_traders_flow',\n",
    " 'cexes_to_traders_flow',\n",
    " 'cexes_to_whale_flow',\n",
    " 'circulation',\n",
    " 'circulation_1d',\n",
    " 'circulation_2y',\n",
    " 'circulation_30d',\n",
    " 'circulation_365d',\n",
    " 'circulation_3y',\n",
    " 'circulation_5y',\n",
    " 'circulation_7d',\n",
    " 'circulation_90d',\n",
    " 'daily_active_addresses',\n",
    " 'defi_balance',\n",
    " 'defi_cex_balance',\n",
    " 'defi_dex_balance',\n",
    " 'defi_exchange_balance',\n",
    " 'defi_to_cexes_flow',\n",
    " 'defi_to_dex_traders_flow',\n",
    " 'defi_to_dexes_flow',\n",
    " 'defi_to_exchanges_flow',\n",
    " 'defi_to_traders_flow',\n",
    " 'defi_to_whale_flow',\n",
    " 'deposit_balance',\n",
    " 'deposit_transactions',\n",
    " 'deposit_transactions_per_exchange',\n",
    " 'dev_activity',\n",
    " 'dev_activity_contributors_count',\n",
    " 'dex_balance',\n",
    " 'dex_cex_balance',\n",
    " 'dex_to_cexes_flow',\n",
    " 'dex_trader_balance',\n",
    " 'dex_traders_cex_balance',\n",
    " 'dex_traders_defi_balance',\n",
    " 'dex_traders_dex_balance',\n",
    " 'dex_traders_exchange_balance',\n",
    " 'dex_traders_to_cexes_flow',\n",
    " 'dex_traders_to_defi_flow',\n",
    " 'dex_traders_to_dexes_flow',\n",
    " 'dex_traders_to_exchanges_flow',\n",
    " 'dex_traders_to_whale_flow',\n",
    " 'dex_traders_whale_balance',\n",
    " 'dexes_to_defi_flow',\n",
    " 'dexes_to_dex_traders_flow',\n",
    " 'dexes_to_traders_flow',\n",
    " 'dexes_to_whale_flow',\n",
    " 'dormant_circulation_180d',\n",
    " 'dormant_circulation_365d',\n",
    " 'dormant_circulation_90d',\n",
    " 'exchange_balance',\n",
    " 'exchange_inflow',\n",
    " 'exchange_inflow_usd',\n",
    " 'exchange_outflow',\n",
    " 'exchange_outflow_usd',\n",
    " 'exchanges_to_defi_flow',\n",
    " 'exchanges_to_dex_traders_flow',\n",
    " 'exchanges_to_genesis_flow',\n",
    " 'exchanges_to_traders_flow',\n",
    " 'exchanges_to_whales_flow',\n",
    " 'github_activity',\n",
    " 'github_activity_contributors_count',\n",
    " 'holders_distribution_combined_balance_over_1',\n",
    " 'holders_distribution_combined_balance_over_10',\n",
    " 'holders_distribution_combined_balance_over_100',\n",
    " 'holders_distribution_combined_balance_over_100k',\n",
    " 'holders_distribution_combined_balance_over_10k',\n",
    " 'holders_distribution_combined_balance_over_1M',\n",
    " 'holders_distribution_combined_balance_over_1k',\n",
    " 'holders_distribution_combined_balance_total',\n",
    " 'holders_distribution_over_1',\n",
    " 'holders_distribution_over_10',\n",
    " 'holders_distribution_over_100',\n",
    " 'holders_distribution_over_100k',\n",
    " 'holders_distribution_over_10k',\n",
    " 'holders_distribution_over_1M',\n",
    " 'holders_distribution_over_1k',\n",
    " 'holders_distribution_total',\n",
    " 'marketcap_usd',\n",
    " 'mean_age',\n",
    " 'mean_dollar_invested_age',\n",
    " 'mean_realized_price_usd',\n",
    " 'mvrv_long_short_diff_usd',\n",
    " 'mvrv_usd',\n",
    " 'network_growth',\n",
    " 'nvt',\n",
    " 'nvt_transaction_volume',\n",
    " 'payments_count',\n",
    " 'percent_of_total_supply_in_profit',\n",
    " 'percent_of_total_supply_on_exchanges',\n",
    " 'price_usd',\n",
    " 'realized_value_usd',\n",
    " 'sentiment_balance_reddit',\n",
    " 'sentiment_balance_total',\n",
    " 'sentiment_balance_twitter',\n",
    " 'sentiment_balance_twitter_crypto',\n",
    " 'sentiment_negative_reddit',\n",
    " 'sentiment_negative_total',\n",
    " 'sentiment_negative_twitter',\n",
    " 'sentiment_negative_twitter_crypto',\n",
    " 'sentiment_positive_reddit',\n",
    " 'sentiment_positive_total',\n",
    " 'sentiment_positive_twitter',\n",
    " 'sentiment_positive_twitter_crypto',\n",
    " 'sentiment_volume_consumed_reddit',\n",
    " 'sentiment_volume_consumed_total',\n",
    " 'sentiment_volume_consumed_twitter',\n",
    " 'sentiment_volume_consumed_twitter_crypto',\n",
    " 'social_dominance_reddit',\n",
    " 'social_dominance_total',\n",
    " 'social_dominance_twitter',\n",
    " 'social_dominance_twitter_crypto',\n",
    " 'social_volume_reddit',\n",
    " 'social_volume_total',\n",
    " 'social_volume_twitter',\n",
    " 'social_volume_twitter_crypto',\n",
    " 'stock_to_flow',\n",
    " 'supply_on_exchanges',\n",
    " 'supply_outside_exchanges',\n",
    " 'total_supply',\n",
    " 'total_supply_in_profit',\n",
    " 'trader_balance',\n",
    " 'traders_cex_balance',\n",
    " 'traders_defi_balance',\n",
    " 'traders_dex_balance',\n",
    " 'traders_exchange_balance',\n",
    " 'traders_to_cexes_flow',\n",
    " 'traders_to_defi_flow',\n",
    " 'traders_to_dexes_flow',\n",
    " 'traders_to_exchanges_flow',\n",
    " 'traders_to_whale_flow',\n",
    " 'traders_whale_balance',\n",
    " 'transaction_volume',\n",
    " 'transactions_count',\n",
    " 'unique_social_volume_total_1h',\n",
    " 'volume_usd',\n",
    " 'whale_balance',\n",
    " 'whale_cex_balance',\n",
    " 'whale_defi_balance',\n",
    " 'whale_dex_balance',\n",
    " 'whale_to_cexes_flow',\n",
    " 'whale_to_defi_flow',\n",
    " 'whale_to_dex_traders_flow',\n",
    " 'whale_to_dexes_flow',\n",
    " 'whale_to_traders_flow',\n",
    " 'whales_exchange_balance',\n",
    " 'whales_to_exchanges_flow',\n",
    " 'withdrawal_balance',\n",
    " 'withdrawal_transactions',\n",
    " 'category_san']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4842144, 182)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# TODO LOOK TO SEE IF SAN PRICE, MCAP, OR VOLUME IS MISSING\n",
    "\n",
    "# Convert all columns to float32\n",
    "cols = list(df.columns.values)\n",
    "cols.remove('date')\n",
    "cols.remove('asset_cg')\n",
    "for col in cols:\n",
    "    df[col] = df[col].astype('float32')\n",
    "\n",
    "# Convert date column to datetime format and remove timezone information\n",
    "df['date'] = pd.to_datetime(df['date'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "# Confirm no mising dates nor asset ids\n",
    "assert 0 == df.date.isnull().sum()\n",
    "assert 0 == df.asset_cg.isnull().sum()\n",
    "\n",
    "# Confirm all asset ids are in cw\n",
    "assert cw_df.asset_cg.is_unique\n",
    "asset_ids = list(cw_df.asset_cg.values)\n",
    "assert df.shape[0] == df[df.asset_cg.isin(asset_ids)].shape[0]\n",
    "\n",
    "# Cut down to relevant dates between July 1, 2016 and January 2, 2023\n",
    "df = df[(df['date'] >= '2016-07-01') & (df['date'] <= '2023-01-02')]\n",
    "\n",
    "# For price, volume, and mcap columns, ensure no missing obs and below thresholds\n",
    "thresholds = {'usd_per_token_cg': 1e9, 'usd_volume_24h_cg': 1e11, 'usd_mcap_cg': 1e13}\n",
    "for col, thresh in thresholds.items():\n",
    "    assert 0 == df[col].isnull().sum()\n",
    "    df = df[df[col] < thresh]\n",
    "\n",
    "# For each asset_cg, replace missing values with zero if there's at least one non-missing observation in the column\n",
    "for col in cols:\n",
    "    has_nonmissing = df.groupby('asset_cg')[col].transform(lambda x: x.notnull().any())\n",
    "    df.loc[has_nonmissing, col] = df.loc[has_nonmissing, col].fillna(0)\n",
    "\n",
    "# Ensure no negative numbers\n",
    "assert 0 == (df[cols]<0).sum().sum()\n",
    "\n",
    "# Loop over all assets to add any missing days and fill missing price, volume, and mcap data\n",
    "final_df = pd.DataFrame()\n",
    "assets = list(np.unique(df.asset_cg.values))\n",
    "for asset in assets:\n",
    "    # subset to asset of interest\n",
    "    asset_df = df[df.asset_cg==asset].copy()\n",
    "\n",
    "    # determine the date gaps\n",
    "    date_gaps = []\n",
    "    dates = asset_df.date.values\n",
    "    for i in range(1, len(dates)):\n",
    "        date_gaps.append(np.timedelta64(dates[i]-dates[i-1], 'D').astype(int))\n",
    "\n",
    "    # determine new days to add\n",
    "    indices_to_expand = [i for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32)]\n",
    "    num_datetime_to_add = [date_gaps[i] for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32)]\n",
    "    start_datetimes = dates[indices_to_expand]\n",
    "    new_datetimes = []\n",
    "    for i in range(len(start_datetimes)):\n",
    "        start_datetime = start_datetimes[i]\n",
    "        datetime_to_add = num_datetime_to_add[i]\n",
    "        for j in range(1, datetime_to_add):\n",
    "            new_datetimes.append(start_datetime+np.timedelta64(24*(j), 'h'))\n",
    "\n",
    "    # add the new days to the asset df\n",
    "    new_asset_df = pd.DataFrame(data={'date': new_datetimes})\n",
    "    new_asset_df['asset_cg'] = asset\n",
    "    asset_df = pd.concat((asset_df, new_asset_df))\n",
    "    asset_df = asset_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # forward fill the price col\n",
    "    asset_df['usd_per_token_cg'] = asset_df['usd_per_token_cg'].ffill()\n",
    "\n",
    "    # replace volume and trades with zeros\n",
    "    asset_df.loc[asset_df.usd_per_token_cg.isnull(), 'usd_volume_cg'] = 0\n",
    "    asset_df.loc[asset_df.usd_mcap_cg.isnull(), 'usd_mcap_cg'] = 0\n",
    "\n",
    "    # add data to master df\n",
    "    final_df = pd.concat((final_df, asset_df))\n",
    "\n",
    "# reset df name\n",
    "del df\n",
    "df = final_df.copy()\n",
    "\n",
    "# Set column order\n",
    "cols.remove('usd_per_token_cg')\n",
    "cols.remove('usd_volume_24h_cg')\n",
    "cols.remove('usd_mcap_cg')\n",
    "cols.sort()\n",
    "df = df[['date', 'asset_cg', 'usd_per_token_cg', 'usd_mcap_cg', 'usd_volume_24h_cg']+cols]\n",
    "\n",
    "# Ensure all columns have cg in name\n",
    "df.columns = [col if col == 'date' or col.endswith('_cg') else col + '_cg' for col in df.columns]\n",
    "\n",
    "# ensure no duplicates by date and asset\n",
    "assert not df.duplicated(subset=['date', 'asset_cg']).any()\n",
    "\n",
    "# Sort by date then asset and reset index\n",
    "df = df.sort_values(by=['date', 'asset_cg']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO CLEAN CROSSWALK\n",
    "# TODO CLEAN MACRO DATA\n",
    "# TODO CLEAN PANEL\n",
    "# TODO also how to handle the missing values for it\n",
    "# TODO make sure the timestamp is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE OLD CLEANING SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN THE MACRO DATA\n",
    "\n",
    "# Move over some data from the timeseries df to the macro df\n",
    "temp_df = ts_df[['date', 'san_slug', 'stock_to_flow', 'traders_to_defi_flow', 'traders_defi_balance']]\n",
    "ts_df = ts_df.drop(['stock_to_flow', 'traders_to_defi_flow', 'traders_defi_balance'], axis=1)\n",
    "macro_df = macro_df.merge(temp_df,\n",
    "                          on=['date', 'san_slug'],\n",
    "                          how='outer',\n",
    "                          validate='one_to_one')\n",
    "\n",
    "# Form mcap weighted average variables\n",
    "temp_df = macro_df[['date', 'san_slug', 'stock_to_flow', 'mvrv_usd_intraday', 'mcd_collat_ratio']]\n",
    "temp_df = temp_df.dropna(how='all', subset=['stock_to_flow', 'mvrv_usd_intraday', 'mcd_collat_ratio'])\n",
    "stf_tokens  = list(np.unique(temp_df[~temp_df.stock_to_flow.isnull()].san_slug.values))\n",
    "mvrv_tokens = list(np.unique(temp_df[~temp_df.mvrv_usd_intraday.isnull()].san_slug.values))\n",
    "mcd_tokens  = list(np.unique(temp_df[~temp_df.mcd_collat_ratio.isnull()].san_slug.values))\n",
    "mcap_needed_tokens = list(np.unique(np.array(stf_tokens + mvrv_tokens + mcd_tokens + scd_tokens)))\n",
    "temp_mcap_df = ts_df[ts_df.san_slug.isin(mcap_needed_tokens)][['date', 'san_slug', \"marketcap_usd\"]]\n",
    "temp_df = temp_df.merge(temp_mcap_df,\n",
    "                        on=['date', 'san_slug'],\n",
    "                        how='inner',\n",
    "                        validate='one_to_one')\n",
    "for col in ['stock_to_flow', 'mvrv_usd_intraday', 'mcd_collat_ratio']:\n",
    "    mcap_avg_temp_df = temp_df[['date', 'san_slug', col, 'marketcap_usd']]\n",
    "    mcap_avg_temp_df = mcap_avg_temp_df.dropna()\n",
    "    mcap_avg_temp_df['total_mcap'] = mcap_avg_temp_df.groupby('date')['marketcap_usd'].transform('sum')\n",
    "    mcap_avg_temp_df['mcap_fraction'] = mcap_avg_temp_df.marketcap_usd / mcap_avg_temp_df.total_mcap\n",
    "    mcap_avg_temp_df['temp'] = mcap_avg_temp_df.mcap_fraction * mcap_avg_temp_df[col]\n",
    "    mcap_avg_temp_df = mcap_avg_temp_df.groupby('date')[['temp']].sum()\n",
    "    mcap_avg_temp_df['san_slug'] = 'macro'\n",
    "    mcap_avg_temp_df = mcap_avg_temp_df.reset_index()\n",
    "    mcap_avg_temp_df = mcap_avg_temp_df.rename(columns = {'temp': ('santiment_token_mcap_avg_'+col)})\n",
    "    macro_df = macro_df.merge(mcap_avg_temp_df,\n",
    "                              on=['date', 'san_slug'],\n",
    "                              how='outer',\n",
    "                              validate='one_to_one')\n",
    "    \n",
    "# Form columns of dollar sum across tokens\n",
    "sum_columns = ['cexes_to_dex_flow', 'exchanges_to_defi_flow', 'whale_to_defi_flow', 'dex_traders_to_defi_flow', \n",
    "               'whale_defi_balance', 'traders_to_defi_flow', 'traders_defi_balance']\n",
    "for col in sum_columns: \n",
    "    temp_df = macro_df[['date', col]]\n",
    "    temp_df = temp_df.groupby('date')[[col]].sum()\n",
    "    temp_df = temp_df.rename(columns={col: 'santiment_token_sum_'+col})\n",
    "    temp_df['san_slug'] = 'macro'\n",
    "    temp_df = temp_df.reset_index()\n",
    "    macro_df = macro_df.merge(temp_df,\n",
    "                              on=['date', 'san_slug'],\n",
    "                              how='outer',\n",
    "                              validate='one_to_one')\n",
    "    \n",
    "# Form macro variables from column with just bitcoin and/or ethereum\n",
    "columns = ['defi_total_value_locked_usd', 'nft_trade_volume_usd', \n",
    "           'nft_trades_count', 'nft_retail_trade_volume_usd', \n",
    "           'nft_whale_trade_volume_usd', 'nft_whale_trades_count', \n",
    "           'percent_of_whale_stablecoin_total_supply', \n",
    "           'average_fees_usd', 'fees_usd', 'eth2_roi', 'median_fees_usd',\n",
    "           'miners_to_exchanges_flow', 'miners_exchange_balance', \n",
    "           'traders_to_defi_flow', 'traders_defi_balance', \n",
    "           'mvrv_usd_intraday', 'stock_to_flow']\n",
    "for col in columns:\n",
    "    temp_df = macro_df[['date', 'san_slug', col]]\n",
    "    temp_df = temp_df.dropna()\n",
    "    btc_eth_token = list(np.unique(temp_df.san_slug.values))\n",
    "    if col in ['traders_defi_balance', 'traders_to_defi_flow',\n",
    "               'mvrv_usd_intraday', 'stock_to_flow']:\n",
    "        btc_eth_token = ['ethereum']\n",
    "    for token in btc_eth_token:\n",
    "        temp_token_df = temp_df[temp_df.san_slug == token]\n",
    "        temp_token_df['san_slug'] = 'macro'\n",
    "        temp_token_df = temp_token_df.rename(columns = {col: 'santiment_'+token+'_'+col})\n",
    "        macro_df = macro_df.merge(temp_token_df,\n",
    "                                  on=['date', 'san_slug'],\n",
    "                                  how='outer',\n",
    "                                  validate='one_to_one')\n",
    "\n",
    "# Drop sum_columns and columns from macro_df\n",
    "macro_df = macro_df.drop((sum_columns + columns), axis=1)\n",
    "macro_df = macro_df.drop(['mcd_collat_ratio'], axis=1)\n",
    "\n",
    "# Keep just the new rows\n",
    "macro_df = macro_df[macro_df.san_slug == 'macro']\n",
    "macro_df = macro_df.drop('san_slug', axis=1)\n",
    "\n",
    "# Clean it up\n",
    "macro_df = macro_df.reset_index(drop=True)\n",
    "macro_df = macro_df.sort_values(by='date')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
