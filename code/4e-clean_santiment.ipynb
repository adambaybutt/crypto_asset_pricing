{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCrosswalk(cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Cleans a crosswalk DataFrame by ensuring that both asset IDs are unique, \n",
    "    sorting the columns, sorting by the asset name, and resetting the index.\n",
    "\n",
    "    Args:\n",
    "        cw_df: A pandas DataFrame representing the crosswalk.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the cleaned crosswalk.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If either of the asset IDs are not unique.\n",
    "    \"\"\"\n",
    "    # Ensure both asset ids are unique\n",
    "    assert cw_df['asset_san'].is_unique, \"asset_san must be unique\"\n",
    "    assert cw_df['asset_cm'].is_unique, \"asset_cm must be unique\"\n",
    "\n",
    "    # Sort columns\n",
    "    cw_df = cw_df[['asset_cm', 'asset_san']]\n",
    "\n",
    "    # Sort by cm asset name and reset index\n",
    "    cw_df = cw_df.sort_values(by='asset_cm').reset_index(drop=True)\n",
    "\n",
    "    return cw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPanel(df: pd.DataFrame, cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Clean the santiment panel with a whole lotta steps that should\n",
    "        be in their own functions!\n",
    "    \"\"\"\n",
    "    # Convert date column to datetime format and remove timezone information\n",
    "    df['date'] = pd.to_datetime(df['datetime'], utc=True).dt.tz_localize(None)\n",
    "    df = df.drop('datetime', axis=1)\n",
    "    assert len(df) == df[df.date.dt.minute==0].shape[0]\n",
    "\n",
    "    # Cut down to relevant dates between July 1, 2016 and January 2, 2023\n",
    "    df = df[(df['date'] >= '2016-07-01') & (df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # Clean up category\n",
    "    df.loc[df.asset_san=='bnb-pancakeswap', 'category_san'] = 'Decentralized Exchange'\n",
    "    df.loc[df.asset_san=='xrp', 'category_san'] = 'Cryptocurrency'\n",
    "    df.loc[df.asset_san=='a-benqi', 'category_san'] = 'Lending'\n",
    "    df.loc[df.asset_san=='creditcoin', 'category_san'] = 'Lending'\n",
    "\n",
    "    # Confirm no mising dates nor asset ids nor category\n",
    "    assert 0 == df.date.isnull().sum()\n",
    "    assert 0 == df.asset_san.isnull().sum()\n",
    "    assert 0 == df.category_san.isnull().sum()\n",
    "\n",
    "    # Confirm all asset ids are in cw\n",
    "    assert cw_df.asset_san.is_unique\n",
    "    asset_ids = list(cw_df.asset_san.values)\n",
    "    assert df.shape[0] == df[df.asset_san.isin(asset_ids)].shape[0]\n",
    "\n",
    "    # Drop any duplicates\n",
    "    df = df.drop_duplicates(subset=['date', 'asset_san'])\n",
    "\n",
    "    # Convert all columns to float32\n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    cols.remove('asset_san')\n",
    "    cols.remove('category_san')\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # Set column order\n",
    "    cols.sort()\n",
    "    df = df[['date', 'asset_san', 'category_san']+cols]\n",
    "\n",
    "    # Loop over all assets to add any missing days and fill missing price, volume, and mcap data\n",
    "    final_df = pd.DataFrame()\n",
    "    df = df.sort_values(by=['date', 'asset_san'], ignore_index=True)\n",
    "    assets = list(np.unique(df.asset_san.values))\n",
    "    for asset in assets:\n",
    "        # subset to asset of interest\n",
    "        asset_df = df[df.asset_san==asset].copy()\n",
    "\n",
    "        # determine the date gaps\n",
    "        date_gaps = []\n",
    "        dates = asset_df.date.values\n",
    "        for i in range(1, len(dates)):\n",
    "            date_gaps.append(np.timedelta64(dates[i]-dates[i-1], 'h').astype(int))\n",
    "        \n",
    "        # determine new days to add\n",
    "        indices_to_expand = [i for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32*24)]\n",
    "        num_datetime_to_add = [date_gaps[i] for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32*24)]\n",
    "        start_datetimes = dates[indices_to_expand]\n",
    "        new_datetimes = []\n",
    "        for i in range(len(start_datetimes)):\n",
    "            start_datetime = start_datetimes[i]\n",
    "            datetime_to_add = num_datetime_to_add[i]\n",
    "            for j in range(1, datetime_to_add):\n",
    "                new_datetimes.append(start_datetime+np.timedelta64(j, 'h'))\n",
    "\n",
    "        # add the new datetimes to the asset df\n",
    "        new_asset_df = pd.DataFrame(data={'date': new_datetimes})\n",
    "        new_asset_df['asset_san'] = asset\n",
    "        asset_df = pd.concat((asset_df, new_asset_df))\n",
    "        asset_df = asset_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # drop any duplicates added\n",
    "        asset_df = asset_df.drop_duplicates(subset=['date'])\n",
    "\n",
    "        # forward fill \n",
    "        asset_df = asset_df.ffill(limit=32*24)\n",
    "\n",
    "        # if asset contains nonmissing obs in a column but starts missing, then fill first obs until nonmissing with 0\n",
    "        cols_to_fill_first_row_with_zero = list(asset_df.columns[asset_df.notna().any() & asset_df.iloc[0].isna()].values)\n",
    "        for col in cols_to_fill_first_row_with_zero:\n",
    "            first_non_missing_index = asset_df[col].first_valid_index()\n",
    "            asset_df.loc[:(first_non_missing_index-1), col] = 0\n",
    "\n",
    "        # add data to master df\n",
    "        final_df = pd.concat((final_df, asset_df))\n",
    "\n",
    "    # reset df name\n",
    "    del df\n",
    "    df = final_df.copy()\n",
    "\n",
    "    # Fill remaining missingness with crossectional median besides price, mcap, and volume\n",
    "    cols_with_missing = [col for col in df.columns if df[col].isna().any()]\n",
    "    for col in cols_with_missing:\n",
    "        medians = df.groupby('date')[col].transform('median')\n",
    "        df[col].fillna(medians, inplace=True)\n",
    "\n",
    "    # Ensure no missing values\n",
    "    assert 0 == df.isnull().sum().sum()\n",
    "\n",
    "    # Ensure all columns have san in name\n",
    "    df.columns = [col if col == 'date' or col.endswith('_san') else col + '_san' for col in df.columns]\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not df.duplicated(subset=['date', 'asset_san']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    df = df.sort_values(by=['date', 'asset_san'], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMacro(macro_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Clean the Santiment macro data with a whole lotta steps that\n",
    "        should be their own functions... :) \n",
    "    \"\"\"\n",
    "    # Drop columns\n",
    "    macro_df = macro_df.drop(['total_assets_issued', 'mcd_locked_token',\n",
    "                            'uniswap_total_user_claims_amount',\n",
    "                            'uniswap_total_lp_claims_amount'], axis=1)\n",
    "\n",
    "    # Convert date column to datetime format and remove timezone information\n",
    "    macro_df['date'] = pd.to_datetime(macro_df['datetime'], utc=True).dt.tz_localize(None)\n",
    "    macro_df = macro_df.drop('datetime', axis=1)\n",
    "    assert len(macro_df) == macro_df[macro_df.date.dt.minute==0].shape[0]\n",
    "    macro_df = macro_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # Cut down to relevant dates between July 1, 2016 and January 2, 2023\n",
    "    macro_df = macro_df[(macro_df['date'] >= '2016-07-01') & (macro_df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # drop duplicates\n",
    "    assert 0 == macro_df.asset.isnull().sum()\n",
    "    macro_df = macro_df.drop_duplicates(subset=['date', 'asset'])\n",
    "\n",
    "    # Form cross-sectional sum\n",
    "    x_sec_sum_cols = ['aave_v2_total_borrowed_usd', 'aave_v2_total_deposits_usd',\n",
    "        'aave_v2_total_liquidations_usd', 'aave_v2_total_new_debt_usd',\n",
    "        'aave_v2_total_supplied_usd', 'compound_total_borrowed_usd',\n",
    "        'compound_total_deposits_usd', 'compound_total_liquidations_usd',\n",
    "        'compound_total_new_debt_usd', 'compound_total_supplied_usd',\n",
    "        'dai_created', 'dai_repaid', 'makerdao_total_borrowed_usd',\n",
    "        'makerdao_total_deposits_usd', 'makerdao_total_supplied_usd',\n",
    "        'total_trade_volume_by_dex', 'nft_retail_trade_volume_usd',\n",
    "        'nft_retail_trades_count', 'nft_trade_volume_usd', 'nft_trades_count',\n",
    "        'nft_whale_trade_volume_usd', 'nft_whale_trades_count',\n",
    "        'uniswap_total_claims_amount', 'usdt_binance_open_interest', 'usdt_binance_open_value']\n",
    "    x_sec_sum_df = macro_df.groupby(['date'])[x_sec_sum_cols].sum().reset_index()\n",
    "    x_sec_sum_df = x_sec_sum_df.rename(columns={'aave_v2_total_borrowed_usd': 'total_aave_borrowed', \n",
    "        'aave_v2_total_deposits_usd': 'total_aave_deposits',\n",
    "        'aave_v2_total_liquidations_usd': 'total_aave_liq', \n",
    "        'aave_v2_total_new_debt_usd': 'total_aave_new_debt',\n",
    "        'aave_v2_total_supplied_usd': 'total_aave_supply', \n",
    "        'compound_total_borrowed_usd': 'total_compound_borrowed',\n",
    "        'compound_total_deposits_usd': 'total_compound_deposits', \n",
    "        'compound_total_liquidations_usd': 'total_compound_liq',\n",
    "        'compound_total_new_debt_usd': 'total_compound_new_debt', \n",
    "        'compound_total_supplied_usd': 'total_compound_supply',\n",
    "        'dai_created': 'total_dai_created',\n",
    "        'dai_repaid': 'total_dai_repaid', \n",
    "        'makerdao_total_borrowed_usd': 'total_maker_borrowed',\n",
    "        'makerdao_total_deposits_usd': 'total_maker_deposits', \n",
    "        'makerdao_total_supplied_usd': 'total_maker_supply',\n",
    "        'total_trade_volume_by_dex': 'total_dex_volume', \n",
    "        'nft_retail_trade_volume_usd': 'total_nft_retail_volume',\n",
    "        'nft_retail_trades_count': 'total_nft_retail_trades', \n",
    "        'nft_trade_volume_usd': 'total_nft_volume', \n",
    "        'nft_trades_count': 'total_nft_trades',\n",
    "        'nft_whale_trade_volume_usd': 'total_nft_whale_volume', \n",
    "        'nft_whale_trades_count': 'total_nft_whale_trades',\n",
    "        'uniswap_total_claims_amount': 'total_uni_claims', \n",
    "        'usdt_binance_open_interest': 'total_open_interest_usdt_binance', \n",
    "        'usdt_binance_open_value': 'total_open_value_usdt_binance'})\n",
    "\n",
    "    # Form cross-sectional medians\n",
    "    x_sec_median_cols = ['aave_v2_stable_borrow_apy', 'aave_v2_supply_apy',\n",
    "                    'aave_v2_variable_borrow_apy', 'mcd_collat_ratio',\n",
    "                    'mvrv_usd_intraday', 'usdt_binance_funding_rate']\n",
    "    x_sec_med_df = macro_df.groupby(['date'])[x_sec_median_cols].median().reset_index()\n",
    "    x_sec_med_df = x_sec_med_df.ffill()\n",
    "    x_sec_med_df = x_sec_med_df.fillna(0)\n",
    "    x_sec_med_df = x_sec_med_df.rename(columns = {'aave_v2_stable_borrow_apy': 'aave_med_borrow_apy', \n",
    "        'aave_v2_supply_apy': 'aave_med_supply_apy',\n",
    "        'aave_v2_variable_borrow_apy': 'aave_med_variable_borrow_apy', \n",
    "        'mcd_collat_ratio': 'mcd_med_collat_ratio',\n",
    "        'mvrv_usd_intraday': 'mvrv_med', \n",
    "        'usdt_binance_funding_rate': 'funding_rate_med_usdt_binance'})\n",
    "\n",
    "    # Form cross-sectional means\n",
    "    x_sec_avg_df = macro_df.groupby(['date'])[['mcd_liquidation']].mean().reset_index()\n",
    "    x_sec_avg_df = x_sec_avg_df.fillna(0)\n",
    "    x_sec_avg_df = x_sec_avg_df.rename(columns={'mcd_liquidation': 'mcd_avg_liq'})\n",
    "\n",
    "    # Form single time-series but just for eth\n",
    "    eth_df = macro_df[macro_df.asset=='ethereum'][['date', 'eth2_roi', 'eth2_stakers_count',\n",
    "        'average_fees_usd', 'fees_usd', 'median_fees_usd', 'mvrv_usd_intraday']].reset_index(drop=True)\n",
    "    eth_df = eth_df.rename(columns = {'eth2_roi': 'eth_roi', \n",
    "        'eth2_stakers_count': 'eth_stakers_count',\n",
    "        'average_fees_usd': 'eth_avg_fee', \n",
    "        'fees_usd': 'eth_total_fee', \n",
    "        'median_fees_usd': 'eth_median_fee', \n",
    "        'mvrv_usd_intraday': 'eth_mvrv'})\n",
    "    eth_df.loc[eth_df.eth_roi.isnull(), 'eth_roi'] = 0\n",
    "    eth_df.loc[eth_df.eth_stakers_count.isnull(), 'eth_stakers_count'] = 0\n",
    "    eth_df = eth_df.ffill()\n",
    "\n",
    "    # Form single times-series but just for btc\n",
    "    btc_df = macro_df[macro_df.asset=='bitcoin'][['date', 'mvrv_usd_intraday']].reset_index(drop=True)\n",
    "    btc_df = btc_df.rename(columns = {'mvrv_usd_intraday': 'btc_mvrv'})\n",
    "\n",
    "    # merge dfs\n",
    "    final_df = x_sec_sum_df.merge(x_sec_med_df, on='date', how='outer', validate='one_to_one')\n",
    "    final_df = final_df.merge(x_sec_avg_df, on='date', how='outer', validate='one_to_one')\n",
    "    final_df = final_df.merge(eth_df, on='date', how='outer', validate='one_to_one')\n",
    "    final_df = final_df.merge(btc_df, on='date', how='outer', validate='one_to_one')\n",
    "    macro_df = final_df.copy()\n",
    "    del final_df\n",
    "\n",
    "    # Ensure all dates are present\n",
    "    macro_df.set_index('date', inplace=True)\n",
    "    min_dt, max_dt = macro_df.index.min(), macro_df.index.max()\n",
    "    full_date_range = pd.date_range(start=min_dt, end=max_dt, freq='1H')\n",
    "    assert len(full_date_range) == macro_df.shape[0]\n",
    "    macro_df = macro_df.reset_index()\n",
    "\n",
    "    # Confirm no missings in the df\n",
    "    assert(macro_df.isnull().sum().sum() == 0)\n",
    "\n",
    "    # Set column order\n",
    "    cols = list(macro_df.columns.values)\n",
    "    cols.remove('date')\n",
    "    cols.sort()\n",
    "    macro_df = macro_df[['date']+cols]\n",
    "\n",
    "    # Convert all columns to float32\n",
    "    for col in cols:\n",
    "        macro_df[col] = macro_df[col].astype('float32')\n",
    "\n",
    "    # Ensure all columns have cmc in name\n",
    "    macro_df.columns = [col if col == 'date' or col.endswith('_san') else col + '_san' for col in macro_df.columns]\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not macro_df.duplicated(subset=['date']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    macro_df = macro_df.sort_values(by=['date'], ignore_index=True)\n",
    "    \n",
    "    return macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    CW_IN_FP = '../data/raw/san_coinmetrics_cw.pkl'\n",
    "    PANEL_IN_FP = \"../data/raw/san_panel.pkl\"\n",
    "    MACRO_IN_FP = '../data/raw/san_macro.pkl'\n",
    "    CW_OUT_FP = '../data/derived/san_cm_cw.pkl'\n",
    "    PANEL_OUT_FP = \"../data/derived/san_panel.pkl\"\n",
    "    MACRO_OUT_FP = '../data/derived/san_macro.pkl'\n",
    "\n",
    "    # import data\n",
    "    cw_df = pd.read_pickle(CW_IN_FP)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "    macro_df = pd.read_pickle(MACRO_IN_FP)\n",
    "\n",
    "    # Extract column from crosswalk before dropping it\n",
    "    df = df.rename(columns={'asset': 'asset_san'})\n",
    "    df = df.merge(cw_df[['asset_san', 'category_san']], on='asset_san', how='left', validate='many_to_one')\n",
    "\n",
    "    # clean data\n",
    "    cw_df = cleanCrosswalk(cw_df)\n",
    "    df = cleanPanel(df, cw_df)    \n",
    "    macro_df = cleanMacro(macro_df)\n",
    "\n",
    "    # save data\n",
    "    cw_df.to_pickle(CW_OUT_FP)\n",
    "    df.to_pickle(PANEL_OUT_FP)\n",
    "    macro_df.to_pickle(MACRO_OUT_FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
