{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab026fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# IMPORT PACKAGES\n",
    "from ipca import InstrumentedPCA\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from tools import QuantTools\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a0896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formIndexCrosswalk(all_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Form a crosswalk dataframe to new indices that are integer indices for use with IPCA package.\n",
    "    \"\"\"\n",
    "    cross_df = all_df[['date', 'asset']].copy()\n",
    "    cross_df['time'] = cross_df['date'].factorize()[0] + 1\n",
    "    cross_df['asset_num'] = cross_df['asset'].factorize()[0] + 1\n",
    "    return cross_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19adf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeChars(df: pd.DataFrame, ipca_char_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes the specified columns in the DataFrame using cross-sectional ranking.\n",
    "\n",
    "    This function linearly spaces the values of the specified columns within each date\n",
    "    to the range [-0.5, 0.5]. The values are ranked, divided by the number of assets for\n",
    "    that date, and subtracted by 0.5.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing the data.\n",
    "        ipca_char_cols (List[str]): List of column names to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the specified columns normalized.\n",
    "    \"\"\"\n",
    "    def normalize_within_date_group(column_name, group):\n",
    "        # Calculate the number of assets for this date\n",
    "        n = group.shape[0]\n",
    "        \n",
    "        # Add random noise to the values to ensure unique ranks\n",
    "        noise = np.random.uniform(-1e-10, 1e-10, size=n)\n",
    "        group[column_name] += noise\n",
    "        \n",
    "        # Rank the values, divide by the number of assets, and subtract 0.5\n",
    "        group[column_name] = group[column_name].rank() / n - 0.5\n",
    "        return group\n",
    "\n",
    "    def normalize_column(df, column_name):\n",
    "        # Apply the normalization to a specific column within each date group\n",
    "        return df.groupby('time', group_keys=False).apply(lambda group: normalize_within_date_group(column_name, group))\n",
    "\n",
    "    # Loop over the specified columns to normalize\n",
    "    for col in ipca_char_cols:\n",
    "        df = normalize_column(df, col)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09e0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndPredict(\n",
    "    in_df: pd.DataFrame, lhs_col: str, test_int: int, ipca_char_cols: List[str], num_factor: int, num_cpus: int\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fits and predicts data using Instrumented Principal Component Analysis (IPCA).\n",
    "\n",
    "    :param in_df: Input DataFrame containing the data.\n",
    "    :param lhs_col: Column name representing the left-hand side variable to predict.\n",
    "    :param test_int: Integer value representing the threshold for splitting the data into training and out-of-sample sets.\n",
    "    :param ipca_char_cols: List of column names used as characteristics for IPCA.\n",
    "    :param num_factor: Number of factors to be used in IPCA.\n",
    "    :param num_cpus: Number of CPUs to be utilized for parallel processing.\n",
    "    \n",
    "    :return: A DataFrame containing predictions (out-of-sample) with columns 'time', 'asset_num', and 'yhats'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Form datasets to fit ipca\n",
    "    in_df    = in_df.sort_values(by=['time', 'asset_num'], ignore_index=True)\n",
    "    train_df = in_df[in_df.time < test_int].copy()\n",
    "    X_oos    = in_df[in_df.time == test_int].copy()\n",
    "\n",
    "    Y_train = train_df[['time', 'asset_num', lhs_col]].copy()\n",
    "    Y_train[lhs_col] = Y_train[lhs_col].astype('float64')\n",
    "    X_train = train_df[['time', 'asset_num']+ipca_char_cols].copy()\n",
    "    Y_train = Y_train.set_index(keys=['asset_num', 'time'], verify_integrity=True)\n",
    "    Y_train = Y_train.squeeze() # convert to Series\n",
    "    X_train = X_train.set_index(keys=['asset_num', 'time'], verify_integrity=True)\n",
    "    X_train = X_train.astype('float64')\n",
    "\n",
    "    X_oos = X_oos[['time', 'asset_num']+ipca_char_cols].copy()\n",
    "    X_oos = X_oos.set_index(keys=['asset_num', 'time'], verify_integrity=True)\n",
    "    X_oos = X_oos.astype('float64')\n",
    "\n",
    "    # Fit\n",
    "    ipca = InstrumentedPCA(n_factors=num_factor, intercept=True)\n",
    "    ipca = ipca.fit(X=X_train, y=Y_train, data_type='panel', n_jobs=num_cpus)\n",
    "\n",
    "    # Predict\n",
    "    yhats = ipca.predict(X=X_oos, mean_factor=True)\n",
    "\n",
    "    # Form results object to return\n",
    "    oos_df          = in_df[['time', 'asset_num']][in_df.time == test_int].reset_index(drop=True)\n",
    "    oos_df['yhats'] = yhats\n",
    "    \n",
    "    return oos_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfac3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndPredictTestPeriod(\n",
    "    df: pd.DataFrame, cross_df: pd.DataFrame, asset_universe_dict: dict, lhs_col: str, \n",
    "    test_start_int: int, num_factors: int, ipca_char_cols: List[str], num_cpus: int\n",
    "    ) -> pd.DataFrame:\n",
    "    # Initialize object for results\n",
    "    test_df = df[df.time >= test_start_int][['time', 'asset_num', lhs_col]].reset_index(drop=True).copy()\n",
    "\n",
    "    # Obtain all test period integers\n",
    "    test_ints = np.unique(df[df.time>=test_start_int].time.values)\n",
    "\n",
    "    # Loop over each model to gen yhats for\n",
    "    for num_factor in range(1,1+num_factors):\n",
    "        # iterate over all the test period weeks to gen yhats\n",
    "        temp_dfs = []\n",
    "        for test_int in list(test_ints):\n",
    "            # form the relevant asset universe\n",
    "            test_week = np.unique(cross_df[cross_df.time==test_int].date.values)[0]\n",
    "            first_day_of_month = np.datetime64(test_week, 'M')\n",
    "            first_day_of_month = np.datetime_as_string(first_day_of_month) + '-01'\n",
    "            asset_universe = asset_universe_dict[first_day_of_month]\n",
    "            asset_universe_ints = list(\n",
    "                np.unique(cross_df[cross_df.asset.isin(asset_universe)].asset_num.values))\n",
    "\n",
    "            # form the relevant dataset\n",
    "            rel_df = df[(df.asset_num.isin(asset_universe_ints))\n",
    "                & (df.time <= test_int)].copy()\n",
    "\n",
    "            # fit and predict\n",
    "            temp_df = fitAndPredict(rel_df, lhs_col, test_int, ipca_char_cols, num_factor, num_cpus)\n",
    "\n",
    "            # save results across the test weeks\n",
    "            temp_dfs.append(temp_df)\n",
    "            \n",
    "        # aggregate results across the test period for this combo\n",
    "        temp_df = pd.concat(temp_dfs)\n",
    "        temp_df = temp_df.rename(columns={'yhats': 'yhats_'+str(num_factor)+'_factors'})\n",
    "\n",
    "        # merge results for this combo onto the main df\n",
    "        test_df = test_df.merge(temp_df, on=['time', 'asset_num'], how='inner', validate='one_to_one')\n",
    "\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "250dbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportResults(\n",
    "    df: pd.DataFrame, lhs_col: str, num_factors: int, \n",
    "    num_qntls_prtls: int, periods_in_year: int, model_prefix: str,\n",
    "    out_fp: str, out_sheet: str, mcap_weighted: bool\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Generates and reports portfolio statistics for a given number of factors.\n",
    "\n",
    "    :param df: DataFrame containing the data to be analyzed.\n",
    "    :param lhs_col: Name of the t+1 returns column.\n",
    "    :param num_factors: The number of factors to consider.\n",
    "    :param num_qntls_prtls: Number of quantiles for portfolio.\n",
    "    :param periods_in_year: Number of periods in a year.\n",
    "    :param model_prefix: name of the model as a prefix for the results.\n",
    "    :param out_fp: Filepath for the output Excel file.\n",
    "    :param out_sheet: Sheet name for the output in the Excel file.\n",
    "    :param mcap_weighted: Boolean indicating if the results are market capitalization weighted.\n",
    "\n",
    "    :return: None. The results are saved directly to the Excel file.\n",
    "    \"\"\"\n",
    "    # Initialize results to return\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    # Generate results for each model\n",
    "    for num_factor in range(1,1+num_factors):\n",
    "        # Rename model's yhats and name\n",
    "        df = df.rename(columns={'yhats_'+str(num_factor)+'_factors': 'yhats'})\n",
    "        model_name =  model_prefix+str(num_factor)\n",
    "\n",
    "        # Generate this model's portfolio statistics\n",
    "        temp_results_df = QuantTools.calcPortfolioStatistics(\n",
    "            df, lhs_col, 'yhats', 'macro_cmkt_tp7', model_name, \n",
    "            num_qntls_prtls, periods_in_year, mcap_weighted\n",
    "        )\n",
    "\n",
    "        # Append results\n",
    "        results_df = pd.concat([results_df, temp_results_df])\n",
    "        \n",
    "        # Update yhat labels for next iteration\n",
    "        df = df.rename(columns={'yhats': 'yhats_'+str(num_factor)+'_factors'})\n",
    "\n",
    "    # Save the results\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        results_df.to_excel(writer, sheet_name=out_sheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    IN_FP           = '../data/clean/panel_weekly.pkl'\n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    DF_OUT_FP       = '../data/clean/test_yhats_ipca.pkl'\n",
    "    OUT_FP          = '../output/low_dim_fm/low_dim_fms.xlsx'\n",
    "    OUT_SHEET       = 'raw_ipca'\n",
    "    LHS_COL         = 'r_ex_tp7'\n",
    "    VAL_START_DATE  = '2021-07-01'\n",
    "    TEST_START_DATE = '2022-07-01'\n",
    "    PERIODS_IN_YEAR = 52\n",
    "    NUM_QNTLS_PRTLS = 5\n",
    "    NUM_FACTORS     = 5\n",
    "    IPCA_CHAR_COLS = ['char_addr_new_log_delta_tm2_tm1',\n",
    "        'char_beta_tm7',\n",
    "        'char_iskew_tm30',\n",
    "        'char_r_tm14',\n",
    "        'char_shortfall5_tm7',\n",
    "        'char_trades_t']\n",
    "    NUM_CPUS = 22\n",
    "\n",
    "    # read in data\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    all_df = pd.read_pickle(IN_FP)\n",
    "\n",
    "    # form index crosswalk\n",
    "    cross_df = formIndexCrosswalk(all_df)\n",
    "    all_df = all_df.merge(cross_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "\n",
    "    # subset and normalize data\n",
    "    df = all_df[['time', 'asset_num', LHS_COL]+IPCA_CHAR_COLS].copy()\n",
    "    df = normalizeChars(df, IPCA_CHAR_COLS)\n",
    "\n",
    "    # convert test start date to an interger\n",
    "    test_start_week_datetime = np.min(all_df[all_df.date >= TEST_START_DATE].date.values)\n",
    "    test_start_int = np.min(all_df[all_df.date==test_start_week_datetime].time)\n",
    "\n",
    "    # generate test period yhats\n",
    "    test_df = fitAndPredictTestPeriod(\n",
    "        df, cross_df, asset_universe_dict, LHS_COL, \n",
    "        test_start_int, NUM_FACTORS, IPCA_CHAR_COLS, NUM_CPUS)\n",
    "        \n",
    "    # form dataframe to use for reporting results\n",
    "    t_df = test_df.merge(cross_df, on=['time', 'asset_num'], how='inner', validate='one_to_one')\n",
    "    t_df = t_df.drop(['time', 'asset_num'], axis=1)\n",
    "    mcap_df = all_df[['date', 'asset', 'char_size_t']].copy()\n",
    "    mcap_df = mcap_df.rename(columns={'char_size_t': 'mcap'})\n",
    "    t_df = mcap_df.merge(t_df, on=['date', 'asset'], how='right', validate='one_to_one')\n",
    "\n",
    "    # save ipca yhats\n",
    "    out_df = t_df.drop(['mcap', LHS_COL], axis=1)\n",
    "    out_df.to_pickle(DF_OUT_FP)\n",
    "\n",
    "    # Form cmkt over future horizon for calc 5-1 strat alpha and beta; add to test results\n",
    "    cmkt_df = all_df[['date', 'macro_cmkt_tm7']].drop_duplicates().copy()\n",
    "    cmkt_df['macro_cmkt_tp7'] = cmkt_df.macro_cmkt_tm7.shift(-1)\n",
    "    cmkt_df = cmkt_df.drop('macro_cmkt_tm7', axis=1)\n",
    "    t_df = t_df.merge(cmkt_df, on=['date'], how='left', validate='many_to_one')\n",
    "\n",
    "    # Report results\n",
    "    reportResults(\n",
    "        t_df, LHS_COL, NUM_FACTORS, NUM_QNTLS_PRTLS, PERIODS_IN_YEAR, \n",
    "        'ipca_', OUT_FP, 'raw_ipca_mcap', True)\n",
    "    reportResults(\n",
    "        t_df, LHS_COL, NUM_FACTORS, NUM_QNTLS_PRTLS, PERIODS_IN_YEAR, \n",
    "        'ipca_', OUT_FP, 'raw_ipca_equal', False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f14a76dcff07d5b93f2c0fc65ce65c8ac7788ddb1ef5c63daa3feefa016fa519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
