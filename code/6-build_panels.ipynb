{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860176d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import Helper\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad17381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPanel(cw_in_fp: str, ca_panel_in_fp: str, cm_panel_in_fp: str, cmc_panel_in_fp: str, cg_panel_in_fp: str,\n",
    "    san_panel_in_fp: str, asset_ico_in_fp: str, messari_in_fp: str) -> pd.DataFrame:\n",
    "    \"\"\" Form a single panel DataFrame from all the data sources. \"\"\"\n",
    "\n",
    "    # import\n",
    "    cw_df = pd.read_pickle(cw_in_fp)\n",
    "    ca_panel_df = pd.read_pickle(ca_panel_in_fp)\n",
    "    cm_panel_df = pd.read_pickle(cm_panel_in_fp)\n",
    "    cmc_panel_df = pd.read_pickle(cmc_panel_in_fp)\n",
    "    cg_panel_df = pd.read_pickle(cg_panel_in_fp)\n",
    "    san_panel_df = pd.read_pickle(san_panel_in_fp)\n",
    "    ico_panel_df = pd.read_pickle(asset_ico_in_fp)\n",
    "    messari_df = pd.read_pickle(messari_in_fp)\n",
    "\n",
    "    # Merge CM and CA data together\n",
    "    panel_df = ca_panel_df.merge(cw_df[['asset_ca', 'asset_cm']], on='asset_ca', how='inner', validate='many_to_one')\n",
    "    panel_df = panel_df.merge(cm_panel_df, on=['date', 'asset_cm'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Merge on rest of panel data\n",
    "    san_panel_df = san_panel_df.merge(cw_df[cw_df.asset_san.notnull()][['asset_san', 'asset_cm']], \n",
    "                                    on='asset_san', how='inner', validate='many_to_one')\n",
    "    panel_df = panel_df.merge(san_panel_df, on=['date', 'asset_cm'], how='left', validate='one_to_one')\n",
    "    cmc_panel_df = cmc_panel_df.merge(cw_df[cw_df.asset_cmc.notnull()][['asset_cmc', 'asset_cm']], \n",
    "                                    on='asset_cmc', how='inner', validate='many_to_one')\n",
    "    panel_df = panel_df.merge(cmc_panel_df, on=['date', 'asset_cm'], how='left', validate='one_to_one')\n",
    "    cg_panel_df = cg_panel_df.merge(cw_df[cw_df.asset_cg.notnull()][['asset_cg', 'asset_cm']], \n",
    "                                    on='asset_cg', how='inner', validate='many_to_one')\n",
    "    panel_df = panel_df.merge(cg_panel_df, on=['date', 'asset_cm'], how='left', validate='one_to_one')\n",
    "    panel_df = panel_df.merge(ico_panel_df, on=['asset_cm'], how='left', validate='many_to_one')\n",
    "    panel_df = panel_df.merge(messari_df, on=['asset_cm'], how='left', validate='many_to_one')\n",
    "\n",
    "    # Drop other asset id columns\n",
    "    panel_df = panel_df.drop(['asset_ca', 'asset_san', 'asset_cmc', 'asset_cg'], axis=1)\n",
    "\n",
    "    # Rename id column\n",
    "    panel_df = panel_df.rename(columns={'asset_cm': 'asset'})\n",
    "\n",
    "    # Ensure all columns have char in front of the col name\n",
    "    panel_df.columns = [col if col in ['date', 'asset'] else 'char_'+col for col in panel_df.columns]\n",
    "\n",
    "    # Make sure unique on id columns\n",
    "    assert 0 == panel_df[['date', 'asset']].duplicated().sum()\n",
    "\n",
    "    # Resort\n",
    "    cols = list(panel_df.columns.values)\n",
    "    cols.remove('date')\n",
    "    cols.remove('asset')\n",
    "    panel_df = panel_df[['date', 'asset']+cols]\n",
    "    panel_df = panel_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "\n",
    "    # Clean up CoinGecko columns that were at 24h freq to fill missing at 1h freq\n",
    "    cg_cols = ['char_usd_per_token_cg', 'char_usd_mcap_cg', 'char_usd_volume_24h_cg', \n",
    "        'char_twitter_followers_cg', 'char_github_activity_cg', 'char_reddit_activity_cg']\n",
    "    for cg_col in cg_cols:\n",
    "        panel_df[cg_col] = panel_df.groupby('asset')[cg_col].fillna(method='ffill')\n",
    "\n",
    "    # ensure no missing in critical columns\n",
    "    panel_df.loc[panel_df['char_usd_per_token_cm'].isnull(), 'char_usd_per_token_cm'] = 0\n",
    "    panel_df.loc[panel_df['char_usd_volume_cm'].isnull(), 'char_usd_volume_cm'] = 0\n",
    "    critical_cols = ['char_usd_per_token_ca',\n",
    "        'char_usd_volume_ca',\n",
    "        'char_usd_per_token_cm',\n",
    "        'char_usd_volume_cm']\n",
    "    for col in critical_cols:\n",
    "        assert 0 == panel_df[col].isnull().sum()\n",
    "    panel_df.loc[panel_df.char_usd_per_token_cm.isnull(), 'char_usd_per_token_cm'] = panel_df.char_usd_per_token_ca\n",
    "    panel_df.loc[panel_df.char_usd_volume_cm.isnull(), 'char_usd_volume_cm'] = panel_df.char_usd_volume_ca\n",
    "\n",
    "    # Clear memory\n",
    "    del cw_df, ca_panel_df, cm_panel_df, cmc_panel_df, \n",
    "    del cg_panel_df, san_panel_df, ico_panel_df, messari_df\n",
    "    gc.collect()\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93338d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formMacro(macro_in_fp: str, ca_macro_in_fp: str, cm_macro_in_fp: str, \n",
    "    cmc_macro_in_fp: str, san_macro_in_fp: str, macro_ico_in_fp: str) -> pd.DataFrame:\n",
    "    \"\"\" Form single macro DataFrame out of all macro data. \"\"\"\n",
    "\n",
    "    # import\n",
    "    macro_df = pd.read_pickle(macro_in_fp)\n",
    "    ca_macro_df = pd.read_pickle(ca_macro_in_fp)\n",
    "    cm_macro_df = pd.read_pickle(cm_macro_in_fp)\n",
    "    cmc_macro_df = pd.read_pickle(cmc_macro_in_fp)\n",
    "    san_macro_df = pd.read_pickle(san_macro_in_fp)\n",
    "    ico_macro_df = pd.read_pickle(macro_ico_in_fp)\n",
    "\n",
    "    # convert macro to hourly dataframe\n",
    "    assert 0 == macro_df.isnull().sum().sum()\n",
    "    macro_df.set_index('date', inplace=True)\n",
    "    min_dt, max_dt = macro_df.index.min(), macro_df.index.max()\n",
    "    full_date_range = pd.date_range(start=min_dt, end=max_dt, freq='1H')\n",
    "    macro_df = macro_df.reindex(full_date_range)\n",
    "    macro_df = macro_df.ffill()\n",
    "    macro_df = macro_df.reset_index()\n",
    "    macro_df = macro_df.rename(columns={'index': 'date'})\n",
    "    assert 0 == macro_df.isnull().sum().sum()\n",
    "\n",
    "    # merge on other macro data\n",
    "    macro_df = macro_df.merge(ca_macro_df, on='date', how='left', validate='one_to_one')\n",
    "    macro_df = macro_df.merge(cm_macro_df, on='date', how='left', validate='one_to_one')\n",
    "    macro_df = macro_df.merge(cmc_macro_df, on='date', how='left', validate='one_to_one')\n",
    "    macro_df = macro_df.merge(san_macro_df, on='date', how='left', validate='one_to_one')\n",
    "    macro_df = macro_df.merge(ico_macro_df, on='date', how='left', validate='one_to_one')\n",
    "\n",
    "    # Ensure all columns have char in front of the col name\n",
    "    macro_df.columns = [col if col=='date' else 'macro_'+col for col in macro_df.columns]\n",
    "\n",
    "    # Make sure unique on id columns\n",
    "    assert macro_df.date.is_unique\n",
    "\n",
    "    # Resort\n",
    "    macro_df = macro_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # Clear memory\n",
    "    del ca_macro_df, cm_macro_df, cmc_macro_df, san_macro_df, ico_macro_df\n",
    "    gc.collect()\n",
    "\n",
    "    return macro_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62dffe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalizeAssetUniverse(panel_df: pd.DataFrame, asset_universe_dict: Dict[str, list]) -> tuple:\n",
    "    \"\"\" Finalize the asset universe by confirming if every asset and start_of_month_date\n",
    "        have the necessary 90 days of data before that month; subset the panel to these\n",
    "        included asset-dates 90 days before and the month of that asset-start_of_month_date.\n",
    "\n",
    "    Note: this takes about twenty-five minutes to run.\n",
    "\n",
    "    Args:\n",
    "        panel_df (pd.DataFrame): the study's panel data.\n",
    "        asset_universe_dict (dict): dictionary with keys for the start of each month of\n",
    "            the study period and values of included assets for that month.\n",
    "\n",
    "    Returns: tuple\n",
    "        panel_df (pd.DataFrame): subsetted panel to the applicable asset-timestamps.\n",
    "        final_asset_universe_dict (dict): updated included assets and start_of_month_dates.\n",
    "    \"\"\"\n",
    "    # Initialize dataframe for final date-asset pairs and a new asset_universe_dict\n",
    "    dates_df = pd.DataFrame(columns=[\"date\", \"asset\"])\n",
    "    final_asset_universe_dict = {}\n",
    "    for date in asset_universe_dict.keys():\n",
    "        # add this date to the new dictionary\n",
    "        final_asset_universe_dict[date] = []\n",
    "\n",
    "        # loop over all assets for this start of month date\n",
    "        for asset in asset_universe_dict[date]:\n",
    "            # form start and end date for window 90 days before trading date\n",
    "            start_date = np.datetime64(date) - np.timedelta64(90, 'D')\n",
    "            end_date = np.datetime64(date)\n",
    "            end_date_plus_one_month = end_date + pd.DateOffset(months=1)\n",
    "\n",
    "            # adjust to add the 2023 days if it is last month of study\n",
    "            if date == '2022-12-01':\n",
    "                end_date_plus_one_month += pd.DateOffset(days=2)\n",
    "\n",
    "            # Form asset dataframe for the dates associated with this asset trading window,\n",
    "            # including the entire month for this month of the study.\n",
    "            asset_df = panel_df.loc[(panel_df[\"asset\"] == asset) \n",
    "                & (panel_df[\"date\"] >= start_date) \n",
    "                & (panel_df[\"date\"] < end_date_plus_one_month)][['date', 'asset']]\n",
    "\n",
    "            # Count number of obs for just the trailing 3 months\n",
    "            asset_num_obs = asset_df[(asset_df[\"date\"] >= start_date) \n",
    "                                    & (asset_df[\"date\"] < end_date)].shape[0]\n",
    "\n",
    "            # If asset has all trailing 90 days of data (at hourly freq)\n",
    "            if asset_num_obs == 2160:\n",
    "                # then add it to the final asset universe\n",
    "                final_asset_universe_dict[date].append(asset)\n",
    "\n",
    "                # and build asset-timestamps to subset the panel down to\n",
    "                dates_df = pd.concat([dates_df, asset_df])\n",
    "\n",
    "        # cut down size of dates\n",
    "        dates_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # cut panel down to just asset-dates of interest\n",
    "    panel_df = panel_df.merge(dates_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "\n",
    "    return panel_df, final_asset_universe_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308c0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPrices(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Form price column from actually tradable prices and \n",
    "        the global price from all data sources. \n",
    "    \n",
    "    Args: \n",
    "        panel_df (pd.DataFrame): the panel data frame with all columns.\n",
    "        \n",
    "    Returns:\n",
    "        (pd.DataFrame): updated panel data with new columns char_price_t\n",
    "                        and char_price_global_t with the raw prices \n",
    "                        columns removed.\n",
    "    \"\"\"\n",
    "    # confirm cm and ca tradable prices are good to go\n",
    "    assert 0 == panel_df.char_usd_per_token_ca.isnull().sum()\n",
    "    assert 0 == panel_df.char_usd_per_token_cm.isnull().sum()\n",
    "    assert 0 == (panel_df.char_usd_per_token_ca==0).sum()\n",
    "    panel_df.loc[panel_df.char_usd_per_token_cm==0, 'char_usd_per_token_cm'] = np.nan\n",
    "    assert 1e6 > np.max(panel_df.char_usd_per_token_cm)\n",
    "    assert 1e6 > np.max(panel_df.char_usd_per_token_ca)\n",
    "    assert 0 < np.min(panel_df.char_usd_per_token_cm)\n",
    "    assert 0 < np.min(panel_df.char_usd_per_token_ca)\n",
    "\n",
    "    # form char_price_t\n",
    "    panel_df['char_price_t'] = np.nan\n",
    "    panel_df.loc[panel_df.char_usd_per_token_cm.isnull(), 'char_price_t'] = panel_df.char_usd_per_token_ca\n",
    "    panel_df.loc[panel_df.char_usd_per_token_cm.notnull(), 'char_price_t'] = (1/2)*(panel_df.char_usd_per_token_cm\n",
    "                                                                                    + panel_df.char_usd_per_token_ca)\n",
    "    assert 0 == panel_df.char_price_t.isnull().sum()\n",
    "    assert 0 < np.min(panel_df.char_price_t)\n",
    "    assert 1e6 > np.min(panel_df.char_price_t)\n",
    "\n",
    "    # clean up old cm and ca columns\n",
    "    panel_df = panel_df.drop(['char_usd_per_token_ca', 'char_usd_per_token_cm'], axis=1)\n",
    "\n",
    "    # clean up the global price cols before combining\n",
    "    global_price_cols = ['char_usd_ref_price_ca', 'char_reference_rate_usd_cm',\n",
    "        'char_price_usd_san', 'char_usd_per_token_cmc', 'char_usd_per_token_cg']\n",
    "    for col in global_price_cols:\n",
    "        panel_df.loc[(panel_df[col]<=0) | (panel_df[col]>1e6) , col] = np.nan\n",
    "\n",
    "    # form global price column\n",
    "    panel_df['char_price_global_t'] = panel_df[global_price_cols].mean(axis=1)\n",
    "    assert 0 == panel_df.char_price_global_t.isnull().sum()\n",
    "    assert 0 == panel_df[(panel_df.char_price_global_t <= 0) | (panel_df.char_price_global_t > 1e6)].shape[0]\n",
    "\n",
    "    # clean up old global price columns\n",
    "    panel_df = panel_df.drop(global_price_cols, axis=1)\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfaaf218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formExcessReturn(panel_df: pd.DataFrame, hours_ahead: int) -> pd.DataFrame:\n",
    "    \"\"\" Form excess return at given number of hours ahead. \"\"\"\n",
    "    # create new column names\n",
    "    new_price_col = 'char_price_tp'+str(hours_ahead)\n",
    "    new_ret_col = 'r_ex_tp'+str(hours_ahead)\n",
    "    rf_ret_col = 'r_rf_tp'+str(hours_ahead)\n",
    "\n",
    "    # form new df of the prices for given number of hours ahead\n",
    "    temp_df = panel_df[['date', 'asset', 'char_price_t']].copy()\n",
    "    temp_df = temp_df.rename(columns={'char_price_t': new_price_col})\n",
    "    temp_df['date'] = temp_df['date'] - pd.to_timedelta(hours_ahead, unit='H')\n",
    "\n",
    "    # merge it back on\n",
    "    panel_df = panel_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    del temp_df\n",
    "\n",
    "    # form return\n",
    "    panel_df[new_ret_col] = (((panel_df[new_price_col]\n",
    "                                - panel_df.char_price_t)/panel_df.char_price_t)\n",
    "                                - panel_df[rf_ret_col])\n",
    "    \n",
    "    # delete the created column\n",
    "    panel_df = panel_df.drop(new_price_col, axis=1)\n",
    "\n",
    "    return panel_df\n",
    "\n",
    "def formLHSs(panel_df: pd.DataFrame, macro_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Form LHS's of excess returns one hour and day ahead. \"\"\"\n",
    "\n",
    "    # Add on one month tbill\n",
    "    panel_df = panel_df.merge(macro_df[['date', 'macro_dgs1mo_fred']], \n",
    "        on=['date'], how='left', validate='many_to_one')\n",
    "    panel_df['macro_dgs1mo_fred'] = panel_df['macro_dgs1mo_fred'].ffill()\n",
    "\n",
    "    # Form one hour and one day ahead risk free return\n",
    "    panel_df['r_rf_tp1']  = (1+panel_df.macro_dgs1mo_fred.values/100)**(1/(365*24))-1\n",
    "    panel_df['r_rf_tp24'] = (1+panel_df.macro_dgs1mo_fred.values/100)**(1/(365))-1\n",
    "    panel_df = panel_df.drop('macro_dgs1mo_fred', axis=1)\n",
    "\n",
    "    # Form one hour and one day ahead excess return\n",
    "    panel_df = formExcessReturn(panel_df, hours_ahead=1)\n",
    "    panel_df = formExcessReturn(panel_df, hours_ahead=24)\n",
    "\n",
    "    # identify outlier returns\n",
    "    outliers_df = panel_df[(panel_df['r_ex_tp1'] > 2.75) | \n",
    "                        (panel_df['r_ex_tp24'] > 10) |\n",
    "                        (panel_df['r_ex_tp1'] < -0.5) |\n",
    "                        (panel_df['r_ex_tp24'] < -0.8)][['date', 'asset']]\n",
    "\n",
    "    # identify all datetime-asset pairs one hour before and after these outlier returns\n",
    "    before_df = outliers_df.copy()\n",
    "    before_df['date'] = before_df['date'] - pd.Timedelta(hours=1)\n",
    "    after_df = outliers_df.copy()\n",
    "    after_df['date'] = after_df['date'] + pd.Timedelta(hours=1)\n",
    "    outliers_df = pd.concat([outliers_df, before_df, after_df])\n",
    "    del before_df, after_df\n",
    "    outliers_df = outliers_df.drop_duplicates()\n",
    "    outliers_df['set_price_missing'] = 1\n",
    "\n",
    "    # set prices on panel to missing if in this outlier set\n",
    "    panel_df = panel_df.merge(outliers_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    del outliers_df\n",
    "    panel_df.loc[panel_df.set_price_missing==1, 'char_price_t'] = np.nan\n",
    "    panel_df = panel_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "    panel_df['char_price_t'] = panel_df.groupby('asset')['char_price_t'].fillna(method='ffill')\n",
    "    panel_df = panel_df.drop('set_price_missing', axis=1)\n",
    "    assert 0 == panel_df.char_price_t.isnull().sum()\n",
    "\n",
    "    # drop the old returns to form new ones\n",
    "    panel_df = panel_df.drop(columns=['r_ex_tp1', 'r_ex_tp24'], axis=1)\n",
    "    panel_df = formExcessReturn(panel_df, hours_ahead=1)\n",
    "    panel_df = formExcessReturn(panel_df, hours_ahead=24)\n",
    "\n",
    "    # Drop unnecssary columns\n",
    "    panel_df = panel_df.drop(columns=['r_rf_tp1', 'r_rf_tp24'], axis=1)\n",
    "\n",
    "    # Fix outliers\n",
    "    panel_df.loc[panel_df.r_ex_tp24<-1, 'r_ex_tp24'] = -.999999\n",
    "    panel_df.loc[panel_df.r_ex_tp24>10, 'r_ex_tp24'] = 9.71\n",
    "    panel_df.loc[panel_df.r_ex_tp1>2.75, 'r_ex_tp1'] = 2\n",
    "\n",
    "    # Drop rows with missing lhs\n",
    "    panel_df = panel_df[panel_df.r_ex_tp24.notnull()]\n",
    "    panel_df = panel_df[panel_df.r_ex_tp1.notnull()]\n",
    "\n",
    "    # Confirm all nonmissing\n",
    "    assert 0 == panel_df.r_ex_tp24.isnull().sum()\n",
    "    assert 0 == panel_df.r_ex_tp1.isnull().sum()\n",
    "\n",
    "    # Drop 2023 data\n",
    "    panel_df = panel_df[panel_df.date.dt.year < 2023]\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98602e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formVolumes(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Form volume columns from usd trade volume and counts in\n",
    "        actually tradable markets as well as global data on\n",
    "        trade volume in usd.\n",
    "    \n",
    "    Args: \n",
    "        panel_df (pd.DataFrame): the panel data frame with all columns.\n",
    "        \n",
    "    Returns:\n",
    "        (pd.DataFrame): updated panel data with new columns.\n",
    "    \"\"\"\n",
    "    # clean up values in volume columns\n",
    "    panel_df.loc[panel_df.char_trades_cm.isnull(), 'char_trades_cm'] = 0\n",
    "    panel_df.loc[panel_df.char_usd_volume_24h_cmc.isnull(), 'char_usd_volume_24h_cmc'] = 0\n",
    "    panel_df.loc[panel_df.char_usd_volume_24h_cg.isnull(), 'char_usd_volume_24h_cg'] = 0\n",
    "    panel_df.loc[panel_df.char_volume_usd_san.isnull(), 'char_volume_usd_san'] = 0\n",
    "    panel_df.loc[panel_df.char_volume_usd_san>1e11, 'char_volume_usd_san'] = 1e11\n",
    "\n",
    "    # confirm all raw volume columns are nonmissing, weakly positive, and bounded\n",
    "    volume_cols  = ['char_usd_volume_ca',\n",
    "        'char_usd_volume_cm',\n",
    "        'char_trades_volume_ca',\n",
    "        'char_trades_cm',\n",
    "        'char_usd_volume_24h_cmc',\n",
    "        'char_usd_volume_24h_cg',\n",
    "        'char_volume_usd_san']\n",
    "    for col in volume_cols:\n",
    "        assert 0 == panel_df[col].isnull().sum()\n",
    "        assert 0 == (panel_df[col]<0).sum()\n",
    "        assert 0 == (panel_df[col]>2e12).sum()\n",
    "    \n",
    "    # form char_volume_t\n",
    "    panel_df['char_volume_t'] = (1/2)*(panel_df.char_usd_volume_ca\n",
    "                                        + panel_df.char_usd_volume_cm)\n",
    "\n",
    "    # form char_trades_t\n",
    "    panel_df['char_trades_t'] = (1/2)*(panel_df.char_trades_volume_ca\n",
    "                                        + panel_df.char_trades_cm)\n",
    "\n",
    "    # form char_volume_global_t\n",
    "    panel_df['char_volume_24h_global_t'] = (1/3)*(panel_df.char_usd_volume_24h_cmc\n",
    "                                                    + panel_df.char_usd_volume_24h_cg\n",
    "                                                    + panel_df.char_volume_usd_san)\n",
    "\n",
    "    # ensure no missing, weakly positive, and bounded in new cols\n",
    "    new_cols = ['char_volume_t', 'char_trades_t', 'char_volume_24h_global_t']\n",
    "    for col in new_cols:\n",
    "        assert 0 == panel_df[col].isnull().sum()\n",
    "        assert 0 == (panel_df[col]<0).sum()\n",
    "        assert 0 == (panel_df[col]>2e12).sum()\n",
    "\n",
    "    # drop old columns\n",
    "    panel_df = panel_df.drop(volume_cols, axis=1)\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a28d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formMcap(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Form the main marketcap column. \"\"\"\n",
    "\n",
    "    # Confirm valid values\n",
    "    mcap_cols = ['char_cap_mrkt_est_usd_cm', 'char_usd_mcap_cmc',\n",
    "                'char_usd_mcap_cg', 'char_marketcap_usd_san']\n",
    "    for col in mcap_cols:\n",
    "        panel_df.loc[panel_df[col]==0, col] = np.nan\n",
    "        assert 0 == (panel_df[col]<=0).sum()\n",
    "        assert 0 == (panel_df[col]>2e12).sum()\n",
    "\n",
    "    # Form char_mcap_t\n",
    "    panel_df['char_mcap_t'] = panel_df[mcap_cols].mean(axis=1)\n",
    "\n",
    "    # Manually fix known issues\n",
    "    panel_df.loc[(panel_df.asset=='chr')\n",
    "                & (panel_df.date.dt.month==5)\n",
    "                & (panel_df.char_mcap_t.isnull()), 'char_mcap_t'] = 8e6\n",
    "    panel_df.loc[(panel_df.asset=='dia')\n",
    "                & (panel_df.date.dt.month==9)\n",
    "                & (panel_df.date.dt.year==2020)\n",
    "                & (panel_df.char_mcap_t.isnull()), 'char_mcap_t'] = 8e7\n",
    "    panel_df.loc[(panel_df.asset=='movr')\n",
    "                & (panel_df.date.dt.month==9)\n",
    "                & (panel_df.date.dt.year==2021)\n",
    "                & (panel_df.char_mcap_t.isnull()), 'char_mcap_t'] = 5e8\n",
    "    panel_df.loc[(panel_df.asset=='qrdo')\n",
    "                & (panel_df.date.dt.year==2021)\n",
    "                & (panel_df.char_mcap_t.isnull()), 'char_mcap_t'] = 7.7e7\n",
    "    panel_df.loc[(panel_df.asset=='ctc')\n",
    "                & (panel_df.date.dt.year==2022)\n",
    "                & (panel_df.char_mcap_t.isnull()), 'char_mcap_t'] = 2.5e8\n",
    "\n",
    "    # Ensure valid values in new column\n",
    "    assert 0 == panel_df['char_mcap_t'].isnull().sum()\n",
    "    assert 0 == (panel_df['char_mcap_t']<0).sum()\n",
    "    assert 0 == (panel_df['char_mcap_t']>2e12).sum()\n",
    "\n",
    "    # drop old columns\n",
    "    panel_df = panel_df.drop(mcap_cols, axis=1)\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88fd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formStaticCharacteristics(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Form all static characteristics. \"\"\"\n",
    "    # ensure messari usage columns are nonmissing and all ones and zeros\n",
    "    messari_cols = ['char_asset_usage_payments_messari',\n",
    "        'char_asset_usage_vote_messari',\n",
    "        'char_asset_usage_work_messari',\n",
    "        'char_asset_usage_dividends_messari',\n",
    "        'char_asset_usage_access_messari',\n",
    "        'char_asset_usage_discount_messari',\n",
    "        'char_pow_messari',\n",
    "        'char_pos_messari']\n",
    "    for col in messari_cols:\n",
    "        assert len(panel_df) == (panel_df[col].isin([0,1])).sum()\n",
    "    new_messari_cols = [column.replace('_messari', '') for column in messari_cols]\n",
    "    column_mapping = dict(zip(messari_cols, new_messari_cols))\n",
    "    panel_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "    # clean momtaz ico columns\n",
    "    panel_df['char_ico_days_since_t'] = (panel_df.date - panel_df.char_ico_date_momtaz).dt.days\n",
    "    panel_df['char_ico_days_since_t'] = panel_df['char_ico_days_since_t'].fillna(-1)\n",
    "    panel_df['char_ico_days_since_t'] = panel_df['char_ico_days_since_t'].astype(int)\n",
    "    panel_df.loc[panel_df['char_ico_days_since_t']<0, 'char_ico_days_since_t'] = np.nan\n",
    "    panel_df['char_ico_days_since_t'] = panel_df['char_ico_days_since_t'].fillna(0)\n",
    "    panel_df = panel_df.drop('char_ico_date_momtaz', axis=1)\n",
    "    panel_df['char_ico_momtaz'] = panel_df['char_ico_momtaz'].fillna(0)\n",
    "    panel_df['char_ico_price_momtaz'] = panel_df['char_ico_momtaz'].fillna(-1)\n",
    "    panel_df = panel_df.rename(columns={'char_ico_momtaz': 'char_ico',\n",
    "                                        'char_ico_price_momtaz': 'char_ico_price'})\n",
    "\n",
    "    # Form industry\n",
    "    assert 0 == panel_df.char_industry_messari.isnull().sum()\n",
    "    panel_df['char_industry'] = panel_df.char_industry_messari\n",
    "    panel_df.loc[panel_df.asset=='forth', 'char_industry'] = 'other_defi'\n",
    "    panel_df = panel_df.drop(['char_industry_messari', 'char_category_san'], axis=1)\n",
    "    assert 0 == panel_df.char_industry.isnull().sum()\n",
    "    indicator_columns = pd.get_dummies(panel_df['char_industry'], prefix='char_industry')\n",
    "    assert 0 == indicator_columns.isnull().sum().sum()\n",
    "    panel_df = panel_df.join(indicator_columns)\n",
    "    assert (len(panel_df.groupby('asset').char_industry.value_counts().index.get_level_values(0))\n",
    "        == len(panel_df.asset.unique())), \"Industry is not unique within asset.\"\n",
    "    panel_df = panel_df.drop('char_industry', axis=1)\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2783dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDistributionCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # drop columns\n",
    "    dist_cols_to_drop = ['char_holders_distribution_combined_balance_over_1_san',\n",
    "        'char_holders_distribution_combined_balance_over_10_san',\n",
    "        'char_holders_distribution_combined_balance_over_100_san',\n",
    "        'char_holders_distribution_combined_balance_over_100k_san',\n",
    "        'char_holders_distribution_combined_balance_over_10k_san',\n",
    "        'char_holders_distribution_combined_balance_over_1M_san',\n",
    "        'char_holders_distribution_combined_balance_over_1k_san',\n",
    "        'char_holders_distribution_combined_balance_total_san',\n",
    "        'char_active_holders_distribution_combined_balance_over_1_san',\n",
    "        'char_active_holders_distribution_combined_balance_over_10_san',\n",
    "        'char_active_holders_distribution_combined_balance_over_100_san',\n",
    "        'char_active_holders_distribution_combined_balance_over_100k_san',\n",
    "        'char_active_holders_distribution_combined_balance_over_10k_san',\n",
    "        'char_active_holders_distribution_combined_balance_over_1M_san',\n",
    "        'char_active_holders_distribution_combined_balance_over_1k_san',\n",
    "        'char_active_holders_distribution_combined_balance_total_san',\n",
    "        'char_active_holders_distribution_over_1_san',\n",
    "        'char_active_holders_distribution_over_10_san',\n",
    "        'char_active_holders_distribution_over_100_san',\n",
    "        'char_active_holders_distribution_over_100k_san',\n",
    "        'char_active_holders_distribution_over_10k_san',\n",
    "        'char_active_holders_distribution_over_1M_san',\n",
    "        'char_active_holders_distribution_over_1k_san',\n",
    "        'char_active_holders_distribution_total_san']\n",
    "    panel_df = panel_df.drop(dist_cols_to_drop, axis=1)\n",
    "\n",
    "    # clean remaining dist cols\n",
    "    dist_cols = ['char_holders_distribution_over_1_san',\n",
    "        'char_holders_distribution_over_10_san',\n",
    "        'char_holders_distribution_over_100_san',\n",
    "        'char_holders_distribution_over_100k_san',\n",
    "        'char_holders_distribution_over_10k_san',\n",
    "        'char_holders_distribution_over_1M_san',\n",
    "        'char_holders_distribution_over_1k_san',\n",
    "        'char_holders_distribution_total_san']\n",
    "    for col in dist_cols:\n",
    "        new_col = col[:-4]+'_t'\n",
    "        panel_df = panel_df.rename(columns={col: new_col})\n",
    "        panel_df[new_col] = panel_df.groupby('asset')[new_col].fillna(method='ffill')\n",
    "        panel_df.loc[panel_df[new_col].isnull(), new_col] = 0\n",
    "        assert 0 == panel_df[new_col].isnull().sum()\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad3a84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formSupplyCols(panel_df: pd.DataFrame, cm_raw_panel_in_fp: str) -> pd.DataFrame:\n",
    "    \"\"\" From circulating and max supply columns from underlying columns. \"\"\"\n",
    "    # import the cm raw panel data and cut down to needed data\n",
    "    cm_raw_panel_df = pd.read_pickle(cm_raw_panel_in_fp)\n",
    "    cm_supply_cols = ['SplyAct10yr', 'SplyActEver', 'SplyCur', 'SplyFF']\n",
    "    cm_raw_panel_df = cm_raw_panel_df[['date', 'asset']+cm_supply_cols]\n",
    "\n",
    "    # merge on to main df\n",
    "    panel_df = panel_df.merge(cm_raw_panel_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    del cm_raw_panel_df\n",
    "\n",
    "    # ensure supply columns are all weakly positive, nonmissing, and bounded\n",
    "    supply_cols = ['char_circulating_supply_cmc', 'char_max_supply_cmc',\n",
    "        'char_total_supply_cmc', 'char_total_supply_san',\n",
    "        'char_circulation_san', 'char_circulation_5y_san', \n",
    "        'SplyAct10yr', 'SplyActEver', 'SplyCur', 'SplyFF']\n",
    "    for col in supply_cols:\n",
    "        panel_df.loc[panel_df[col]<=0, col] = np.nan\n",
    "        panel_df[col] = panel_df.groupby('asset')[col].fillna(method='ffill')\n",
    "        assert 0 == (panel_df[col]<=0).sum()\n",
    "        assert 0 == (panel_df[col]>1e18).sum()\n",
    "\n",
    "    # create circulating supply column\n",
    "    panel_df['char_supply_circ_t'] = panel_df[['char_circulating_supply_cmc',\n",
    "        'SplyAct10yr', 'SplyActEver', 'SplyCur', 'SplyFF',\n",
    "        'char_circulation_san', 'char_circulation_5y_san']].mean(axis=1)\n",
    "    panel_df.loc[(panel_df.asset=='chr')\n",
    "        & (panel_df.char_supply_circ_t.isnull()), 'char_supply_circ_t'] = 8e8\n",
    "    panel_df.loc[(panel_df.asset=='dia')\n",
    "        & (panel_df.char_supply_circ_t.isnull()), 'char_supply_circ_t'] = 1.5e8\n",
    "    panel_df.loc[(panel_df.asset=='unfi')\n",
    "        & (panel_df.char_supply_circ_t.isnull()), 'char_supply_circ_t'] = 2.5e6\n",
    "    panel_df.loc[(panel_df.asset=='orca')\n",
    "        & (panel_df.char_supply_circ_t.isnull()), 'char_supply_circ_t'] = 1.5e7\n",
    "    panel_df.loc[(panel_df.asset=='t')\n",
    "        & (panel_df.char_supply_circ_t.isnull()), 'char_supply_circ_t'] = 3e9\n",
    "    panel_df.loc[(panel_df.asset=='gmt')\n",
    "        & (panel_df.char_supply_circ_t.isnull()), 'char_supply_circ_t'] = 6e8\n",
    "\n",
    "    # create total supply column by averaging cmc and san\n",
    "    panel_df['char_supply_max_t'] = panel_df[['char_max_supply_cmc',\n",
    "                                                'char_total_supply_cmc',\n",
    "                                                'char_total_supply_san']].mean(axis=1)\n",
    "    panel_df.loc[panel_df.char_supply_max_t.isnull(), 'char_supply_max_t'] = panel_df.char_supply_circ_t\n",
    "\n",
    "    # ensure for all weakly positive, nonmissing, and bounded\n",
    "    new_cols = ['char_supply_circ_t', 'char_supply_max_t']\n",
    "    for col in new_cols:\n",
    "        assert 0 == panel_df[col].isnull().sum()\n",
    "        assert 0 == (panel_df[col]<=0).sum()\n",
    "        assert 0 == (panel_df[col]>1e18).sum()\n",
    "\n",
    "    # drop old columns\n",
    "    panel_df = panel_df.drop(supply_cols, axis=1)\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fefa89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formDevActivity(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Form single developer activity column from underlying column. \"\"\"\n",
    "    # set cols to combine\n",
    "    dev_cols = ['char_github_activity_cg', 'char_github_activity_san',\n",
    "        'char_github_activity_contributors_count_san', 'char_dev_activity_san',\n",
    "        'char_dev_activity_contributors_count_san']\n",
    "    \n",
    "    # fill missing\n",
    "    panel_df.loc[panel_df.char_github_activity_cg.isnull(), 'char_github_activity_cg'] = -1\n",
    "\n",
    "    # ensure columns are all weakly positive, nonmissing, and bounded\n",
    "    for col in dev_cols:\n",
    "        panel_df[col] = panel_df.groupby('asset')[col].fillna(method='ffill')\n",
    "        panel_df.loc[panel_df[col].isnull(), col] = 0\n",
    "        assert 0 == panel_df[col].isnull().sum()\n",
    "        assert 0 == (panel_df[col]<-1).sum()\n",
    "        assert 0 == (panel_df[col]>1e18).sum()\n",
    "\n",
    "    # normalize all columns to -1 to 1\n",
    "    for col in dev_cols:\n",
    "        panel_df = Helper.xsecNormalizeToMinusOneOne(panel_df, target_col=col, asset_col='asset')\n",
    "\n",
    "    # form single dev column from underlying\n",
    "    panel_df['char_dev_activity_t'] = panel_df[dev_cols].mean(axis=1)\n",
    "    assert 0 == panel_df.char_dev_activity_t.isnull().sum()\n",
    "\n",
    "    # drop old columns\n",
    "    panel_df = panel_df.drop(dev_cols, axis=1)\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9192ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCmcColumns(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Clean the remaining few cmc columns. \"\"\"\n",
    "    # clean rank column with forward filling and then replacing any missing with crossectional min\n",
    "    panel_df['char_rank_cmc'] = panel_df.groupby('asset')['char_rank_cmc'].fillna(method='ffill')\n",
    "    panel_df['char_rank_cmc'] = panel_df.groupby('date')['char_rank_cmc'].apply(lambda x: x.fillna(x.min()))\n",
    "\n",
    "    # clean num market pairs and vc to be zero if missing\n",
    "    panel_df.loc[panel_df['char_num_market_pairs_cmc'].isnull(), 'char_num_market_pairs_cmc'] = 0\n",
    "    panel_df.loc[panel_df['char_vc_cmc'].isnull(), 'char_vc_cmc'] = 0\n",
    "\n",
    "    # confirm ranges and missingness\n",
    "    cmc_cols = ['char_num_market_pairs_cmc', 'char_rank_cmc', 'char_vc_cmc']\n",
    "    for col in cmc_cols:\n",
    "        assert 0 == panel_df[col].isnull().sum()\n",
    "        assert 0 == (panel_df[col]<0).sum()\n",
    "        assert 0 == (panel_df[col]>1e6).sum()\n",
    "\n",
    "    # rename\n",
    "    panel_df = panel_df.rename(columns={'char_num_market_pairs_cmc': 'char_num_pairs_t', \n",
    "                                        'char_rank_cmc': 'char_rank_cmc_t', \n",
    "                                        'char_vc_cmc': 'char_vc_t'})\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e268493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formSocial(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Form social columns. \"\"\"\n",
    "    # set social columns\n",
    "    social_cols = ['char_unique_social_volume_total_1h_san',\n",
    "        'char_sentiment_balance_reddit_san',\n",
    "        'char_sentiment_balance_total_san',\n",
    "        'char_sentiment_balance_twitter_san',\n",
    "        'char_sentiment_balance_twitter_crypto_san',\n",
    "        'char_sentiment_negative_reddit_san',\n",
    "        'char_sentiment_negative_total_san',\n",
    "        'char_sentiment_negative_twitter_san',\n",
    "        'char_sentiment_negative_twitter_crypto_san',\n",
    "        'char_sentiment_positive_reddit_san',\n",
    "        'char_sentiment_positive_total_san',\n",
    "        'char_sentiment_positive_twitter_san',\n",
    "        'char_sentiment_positive_twitter_crypto_san',\n",
    "        'char_sentiment_volume_consumed_reddit_san',\n",
    "        'char_sentiment_volume_consumed_total_san',\n",
    "        'char_sentiment_volume_consumed_twitter_san',\n",
    "        'char_sentiment_volume_consumed_twitter_crypto_san',\n",
    "        'char_social_dominance_reddit_san',\n",
    "        'char_social_dominance_total_san',\n",
    "        'char_social_dominance_twitter_san',\n",
    "        'char_social_dominance_twitter_crypto_san',\n",
    "        'char_social_volume_reddit_san',\n",
    "        'char_social_volume_total_san',\n",
    "        'char_social_volume_twitter_san',\n",
    "        'char_social_volume_twitter_crypto_san',\n",
    "        'char_reddit_activity_cg',\n",
    "        'char_twitter_followers_cg']\n",
    "\n",
    "    # fill missings with ffill or zeros.\n",
    "    for col in social_cols:\n",
    "        panel_df[col] = panel_df.groupby('asset')[col].fillna(method='ffill')\n",
    "        panel_df.loc[panel_df[col].isnull(), col] = 0\n",
    "\n",
    "    # drop social columns not needed\n",
    "    drop_cols = ['char_sentiment_balance_total_san',\n",
    "        'char_sentiment_balance_twitter_crypto_san',\n",
    "        'char_sentiment_negative_total_san',\n",
    "        'char_sentiment_negative_twitter_crypto_san',\n",
    "        'char_sentiment_positive_total_san',\n",
    "        'char_sentiment_positive_twitter_crypto_san',\n",
    "        'char_sentiment_volume_consumed_reddit_san',\n",
    "        'char_sentiment_volume_consumed_twitter_san',\n",
    "        'char_sentiment_volume_consumed_twitter_crypto_san',\n",
    "        'char_social_dominance_reddit_san',\n",
    "        'char_social_dominance_twitter_san',\n",
    "        'char_social_dominance_twitter_crypto_san',\n",
    "        'char_social_volume_total_san',\n",
    "        'char_social_volume_twitter_crypto_san']\n",
    "    panel_df = panel_df.drop(drop_cols, axis=1)\n",
    "\n",
    "    # normalize columns to -1 to 1 x-sectionally before combining\n",
    "    cols_to_norm = ['char_social_volume_reddit_san',\n",
    "        'char_reddit_activity_cg',\n",
    "        'char_social_volume_twitter_san',\n",
    "        'char_twitter_followers_cg']\n",
    "    for col in cols_to_norm:\n",
    "        panel_df = Helper.xsecNormalizeToMinusOneOne(panel_df, target_col=col, asset_col='asset')\n",
    "\n",
    "    # combine columns\n",
    "    panel_df['char_social_volume_reddit_t'] = panel_df[['char_social_volume_reddit_san',\n",
    "                                                        'char_reddit_activity_cg']].mean(axis=1)\n",
    "    panel_df['char_social_volume_twitter_t'] = panel_df[['char_social_volume_twitter_san',\n",
    "                                                        'char_twitter_followers_cg']].mean(axis=1)\n",
    "    assert 0 == panel_df.char_social_volume_reddit_t.isnull().sum()\n",
    "    assert 0 == panel_df.char_social_volume_twitter_t.isnull().sum()\n",
    "\n",
    "    # drop combined columns\n",
    "    panel_df = panel_df.drop(cols_to_norm, axis=1)\n",
    "\n",
    "    # rename columns to drop the _san\n",
    "    cols_to_rename = list(set(social_cols).intersection(set(list(panel_df.columns))))\n",
    "    new_col_names = [col[:-4]+'_t' for col in cols_to_rename]\n",
    "    column_mapping = dict(zip(cols_to_rename, new_col_names))\n",
    "    panel_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edb72732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formAddrCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # set address cols\n",
    "    address_cols = ['char_daily_active_addresses_san',\n",
    "        'char_active_addresses_1h_san',\n",
    "        'char_network_growth_san',\n",
    "        'char_payments_count_san',\n",
    "        'char_transaction_volume_san',\n",
    "        'char_transactions_count_san']\n",
    "\n",
    "    # confirm ranges\n",
    "    for col in address_cols:\n",
    "        panel_df[col] = panel_df.groupby('asset')[col].fillna(method='ffill')\n",
    "        assert 0 == (panel_df[col]<0).sum()\n",
    "        assert 0 == (panel_df[col]>1e15).sum()\n",
    "\n",
    "    # combine active addresses into single column\n",
    "    panel_df['char_active_addr_t'] = (1/2)*(panel_df.char_daily_active_addresses_san/24 +\n",
    "                                            panel_df.char_active_addresses_1h_san)\n",
    "    panel_df.loc[panel_df.char_active_addr_t.isnull(), 'char_active_addr_t'] = 0\n",
    "\n",
    "    # clear network growth\n",
    "    panel_df.loc[panel_df['char_network_growth_san'].isnull(), 'char_network_growth_san'] = 0\n",
    "    panel_df = panel_df.rename(columns = {'char_network_growth_san': 'char_network_growth_t'})\n",
    "\n",
    "    # create transaction volume\n",
    "    panel_df['char_tx_volume_t'] = panel_df[['char_payments_count_san', \n",
    "                                            'char_transaction_volume_san',\n",
    "                                            'char_transactions_count_san']].mean(1)\n",
    "    panel_df.loc[panel_df['char_tx_volume_t'].isnull(), 'char_tx_volume_t'] = 0\n",
    "\n",
    "    # drop the old cols\n",
    "    address_cols.remove('char_network_growth_san')\n",
    "    panel_df.drop(address_cols, axis=1, inplace=True)\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e407079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDepositAndWithdrawCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # clean up deposit columns\n",
    "    panel_df = panel_df.drop(['char_active_deposits_san', 'char_active_deposits_per_exchange_san',\n",
    "        'char_deposit_transactions_per_exchange_san', 'char_deposit_balance_san'], axis=1)\n",
    "    panel_df = panel_df.rename(columns={'char_deposit_transactions_san': 'char_tx_deposit_t'})\n",
    "    panel_df['char_tx_deposit_t'] = panel_df.groupby('asset')['char_tx_deposit_t'].fillna(method='ffill')\n",
    "    panel_df.loc[panel_df['char_tx_deposit_t'].isnull(), 'char_tx_deposit_t'] = 0\n",
    "    assert 0 == panel_df[\"char_tx_deposit_t\"].isnull().sum()\n",
    "    assert 0 == (panel_df[\"char_tx_deposit_t\"]<0).sum()\n",
    "    assert 0 == (panel_df[\"char_tx_deposit_t\"]>1e6).sum()\n",
    "\n",
    "    # clean up withdraw columns\n",
    "    panel_df = panel_df.drop(['char_active_withdrawals_san',\n",
    "        'char_active_withdrawals_per_exchange_san', 'char_withdrawal_balance_san'], axis=1)\n",
    "    panel_df = panel_df.rename(columns={'char_withdrawal_transactions_san': 'char_tx_withdraw_t'})\n",
    "    panel_df['char_tx_withdraw_t'] = panel_df.groupby('asset')['char_tx_withdraw_t'].fillna(method='ffill')\n",
    "    panel_df.loc[panel_df['char_tx_withdraw_t'].isnull(), 'char_tx_withdraw_t'] = 0\n",
    "    assert 0 == panel_df[\"char_tx_withdraw_t\"].isnull().sum()\n",
    "    assert 0 == (panel_df[\"char_tx_withdraw_t\"]<0).sum()\n",
    "    assert 0 == (panel_df[\"char_tx_withdraw_t\"]>1e8).sum()\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "481bee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAge(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # clean age destroyed\n",
    "    panel_df = panel_df.drop('char_age_consumed_san', axis=1)\n",
    "    panel_df = panel_df.rename(columns={'char_age_destroyed_san': 'char_age_destroyed_t'})\n",
    "    panel_df['char_age_destroyed_t'] = panel_df.groupby('asset')['char_age_destroyed_t'].fillna(method='ffill')\n",
    "    panel_df.loc[panel_df['char_age_destroyed_t'].isnull(), 'char_age_destroyed_t'] = 0\n",
    "    assert 0 == panel_df[\"char_age_destroyed_t\"].isnull().sum()\n",
    "\n",
    "    # clean mean age\n",
    "    panel_df = panel_df.drop('char_mean_age_san', axis=1)\n",
    "    panel_df = panel_df.rename(columns={'char_mean_dollar_invested_age_san': 'char_age_mean_dollar_t'})\n",
    "    panel_df['char_age_mean_dollar_t'] = panel_df.groupby('asset')['char_age_mean_dollar_t'].fillna(method='ffill')\n",
    "    panel_df.loc[panel_df['char_age_mean_dollar_t'].isnull(), 'char_age_mean_dollar_t'] = 0\n",
    "    assert 0 == panel_df[\"char_age_mean_dollar_t\"].isnull().sum()\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e1fd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCryptoValuationCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # drop cols\n",
    "    panel_df = panel_df.drop('char_total_supply_in_profit_san', axis=1)\n",
    "\n",
    "    # clean cols\n",
    "    cols = ['char_stock_to_flow_san',\n",
    "        'char_realized_value_usd_san',\n",
    "        'char_mean_realized_price_usd_san',\n",
    "        'char_mvrv_long_short_diff_usd_san',\n",
    "        'char_mvrv_usd_san',\n",
    "        'char_nvt_san',\n",
    "        'char_nvt_transaction_volume_san',\n",
    "        'char_percent_of_total_supply_in_profit_san']\n",
    "    for col in cols:\n",
    "        new_col = col[:-4]+'_t'\n",
    "        panel_df = panel_df.rename(columns={col: new_col})\n",
    "        panel_df[new_col] = panel_df.groupby('asset')[new_col].fillna(method='ffill')\n",
    "        panel_df.loc[panel_df[new_col].isnull(), new_col] = 0\n",
    "        assert 0 == panel_df[new_col].isnull().sum()\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e02ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFlowCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = ['char_cexes_to_defi_flow_san', 'char_cexes_to_dex_flow_san',\n",
    "            'char_cexes_to_dex_traders_flow_san', 'char_cexes_to_traders_flow_san',\n",
    "            'char_cexes_to_whale_flow_san', 'char_defi_to_cexes_flow_san',\n",
    "            'char_defi_to_dex_traders_flow_san', 'char_defi_to_dexes_flow_san',\n",
    "            'char_defi_to_exchanges_flow_san', 'char_defi_to_traders_flow_san',\n",
    "            'char_defi_to_whale_flow_san', 'char_dex_traders_to_cexes_flow_san',\n",
    "            'char_dex_traders_to_defi_flow_san', 'char_dex_traders_to_dexes_flow_san',\n",
    "            'char_dex_traders_to_exchanges_flow_san', 'char_dex_traders_to_whale_flow_san',\n",
    "            'char_dexes_to_defi_flow_san', 'char_dexes_to_dex_traders_flow_san',\n",
    "            'char_dexes_to_traders_flow_san', 'char_dexes_to_whale_flow_san',\n",
    "            'char_dex_to_cexes_flow_san', 'char_exchange_inflow_san',\n",
    "            'char_exchange_inflow_usd_san', 'char_exchange_outflow_san',\n",
    "            'char_exchange_outflow_usd_san', 'char_exchanges_to_defi_flow_san',\n",
    "            'char_exchanges_to_dex_traders_flow_san', 'char_exchanges_to_genesis_flow_san',\n",
    "            'char_exchanges_to_traders_flow_san', 'char_exchanges_to_whales_flow_san',\n",
    "            'char_traders_to_cexes_flow_san', 'char_traders_to_defi_flow_san',\n",
    "            'char_traders_to_dexes_flow_san', 'char_traders_to_exchanges_flow_san',\n",
    "            'char_traders_to_whale_flow_san', 'char_whale_to_cexes_flow_san',\n",
    "            'char_whale_to_defi_flow_san', 'char_whale_to_dex_traders_flow_san',\n",
    "            'char_whale_to_dexes_flow_san', 'char_whale_to_traders_flow_san',\n",
    "            'char_whales_to_exchanges_flow_san']\n",
    "    for col in cols:\n",
    "        new_col = col[:-4]+'_t'\n",
    "        panel_df = panel_df.rename(columns={col: new_col})\n",
    "        panel_df[new_col] = panel_df.groupby('asset')[new_col].fillna(method='ffill')\n",
    "        panel_df.loc[panel_df[new_col].isnull(), new_col] = 0\n",
    "        assert 0 == panel_df[new_col].isnull().sum()\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62c6d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCirculationCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = ['char_circulation_1d_san', 'char_circulation_2y_san',\n",
    "            'char_circulation_30d_san', 'char_circulation_365d_san',\n",
    "            'char_circulation_3y_san', 'char_circulation_7d_san',\n",
    "            'char_circulation_90d_san', 'char_dormant_circulation_180d_san', \n",
    "            'char_dormant_circulation_365d_san', 'char_dormant_circulation_90d_san']\n",
    "    for col in cols:\n",
    "        new_col = col[:-4]+'_t'\n",
    "        panel_df = panel_df.rename(columns={col: new_col})\n",
    "        panel_df[new_col] = panel_df.groupby('asset')[new_col].fillna(method='ffill')\n",
    "        panel_df.loc[panel_df[new_col].isnull(), new_col] = 0\n",
    "        assert 0 == panel_df[new_col].isnull().sum()\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8825718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanBalanceCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # drop cols\n",
    "    cols_to_drop = ['char_all_known_balance_san',\n",
    "                    'char_amount_in_exchange_top_holders_san',\n",
    "                    'char_amount_in_non_exchange_top_holders_san',\n",
    "                    'char_dex_trader_balance_san',\n",
    "                    'char_dex_traders_cex_balance_san',\n",
    "                    'char_dex_traders_defi_balance_san',\n",
    "                    'char_dex_traders_dex_balance_san',\n",
    "                    'char_dex_traders_exchange_balance_san',\n",
    "                    'char_dex_traders_whale_balance_san',\n",
    "                    'char_whales_exchange_balance_san']\n",
    "    panel_df = panel_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    # rename typo\n",
    "    panel_df = panel_df.rename(columns={'char_trader_balance_san': 'char_traders_balance_san'})\n",
    "\n",
    "    # clean cols\n",
    "    cols = ['char_amount_in_top_holders_san', 'char_defi_balance_san',\n",
    "        'char_defi_cex_balance_san', 'char_defi_dex_balance_san',\n",
    "        'char_defi_exchange_balance_san', 'char_cex_balance_san',\n",
    "        'char_dex_balance_san', 'char_dex_cex_balance_san',\n",
    "        'char_exchange_balance_san', 'char_percent_of_total_supply_on_exchanges_san',\n",
    "        'char_supply_on_exchanges_san', 'char_supply_outside_exchanges_san',\n",
    "        'char_traders_balance_san', 'char_traders_cex_balance_san',\n",
    "        'char_traders_defi_balance_san', 'char_traders_dex_balance_san',\n",
    "        'char_traders_exchange_balance_san', 'char_traders_whale_balance_san',\n",
    "        'char_whale_balance_san', 'char_whale_cex_balance_san',\n",
    "        'char_whale_defi_balance_san', 'char_whale_dex_balance_san']\n",
    "    for col in cols:\n",
    "        new_col = col[:-4]+'_t'\n",
    "        panel_df = panel_df.rename(columns={col: new_col})\n",
    "        panel_df[new_col] = panel_df.groupby('asset')[new_col].fillna(method='ffill')\n",
    "        panel_df.loc[panel_df[new_col].isnull(), new_col] = 0\n",
    "        assert 0 == panel_df[new_col].isnull().sum()\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4459535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formStablecoinDeviation(macro_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # form deviation column with coinapi data\n",
    "    macro_df.loc[macro_df.macro_usd_per_usdc_ca.isnull(), 'macro_stablecoin_dev_t'] = macro_df.macro_usd_per_usdc_ca - 1\n",
    "    macro_df.loc[macro_df.macro_usd_per_usdc_ca.notnull()\n",
    "        & (np.abs(macro_df.macro_usd_per_usdc_ca-1) > np.abs(macro_df.macro_usd_per_usdt_ca-1)), 'macro_stablecoin_dev_t'] = macro_df.macro_usd_per_usdt_ca-1\n",
    "    macro_df.loc[macro_df.macro_usd_per_usdc_ca.notnull()\n",
    "        & (np.abs(macro_df.macro_usd_per_usdc_ca-1) <= np.abs(macro_df.macro_usd_per_usdt_ca-1)), 'macro_stablecoin_dev_t'] = macro_df.macro_usd_per_usdt_ca-1\n",
    "\n",
    "    # avg with existing deviation column from coinmetrics\n",
    "    macro_df['macro_stablecoin_dev_t'] = macro_df[['macro_stablecoin_dev_t', 'macro_usdt_usdc_dev_from_one_cm']].mean(axis=1)\n",
    "\n",
    "    # ensure clean\n",
    "    assert 0 == macro_df.macro_stablecoin_dev_t.isnull().sum()\n",
    "    assert 0 == macro_df[(macro_df.macro_stablecoin_dev_t < -0.5) \n",
    "        | (macro_df.macro_stablecoin_dev_t > 0.5)].shape[0]\n",
    "\n",
    "    # drop old columns\n",
    "    macro_df = macro_df.drop(['macro_usd_per_usdc_ca', 'macro_usd_per_usdt_ca',\n",
    "        'macro_usdt_usdc_dev_from_one_cm'], axis=1)\n",
    "    \n",
    "    return macro_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51fb9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formICO(macro_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # rename\n",
    "    macro_df = macro_df.rename(columns={'macro_ico_sum_momtaz': 'macro_ico_count_t'})\n",
    "\n",
    "    # clean the col\n",
    "    macro_df.loc[macro_df.macro_ico_count_t.isnull(), 'macro_ico_count_t'] = 0\n",
    "\n",
    "    # ensure clean\n",
    "    assert 0 == macro_df.macro_ico_count_t.isnull().sum()\n",
    "    assert 0 == (macro_df.macro_ico_count_t<0).sum()\n",
    "\n",
    "    return macro_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b00e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formMcCrakenColumns(macro_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # final all fed columns\n",
    "    fed_cols = [col for col in macro_df.columns if '_fed' in col]\n",
    "\n",
    "    # rename messed up columns\n",
    "    macro_df = macro_df.rename(columns={'macro_ces0600000008_fed': 'macro_ces06_fed',\n",
    "        'macro_ces2000000008_fed': 'macro_ces20_fed',\n",
    "        'macro_ces3000000008_fed': 'macro_ces30_fed',\n",
    "        'macro_s&p 500_fed': 'macro_snp500_fed',\n",
    "        'macro_s&p pe ratio_fed': 'macro_snp_pe_fed',\n",
    "        'macro_s&p div yield_fed': 'macro_snp_div_yield_fed',\n",
    "        'macro_s&p: indust_fed': 'macro_snp_indust_fed'})\n",
    "\n",
    "    # make list of columns to clean and keep\n",
    "    fed_cols_to_clean = ['macro_aaa_fed', 'macro_acogno_fed', 'macro_amdmnox_fed', \n",
    "        'macro_andenox_fed', 'macro_awhman_fed', 'macro_awotman_fed',\n",
    "        'macro_baa_fed', 'macro_businvx_fed', 'macro_busloans_fed', \n",
    "        'macro_ce16ov_fed', 'macro_claimsx_fed', 'macro_clf16ov_fed',\n",
    "        'macro_cmrmtsplx_fed', 'macro_compapffx_fed', 'macro_conspi_fed', \n",
    "        'macro_cp3mx_fed', 'macro_cpiaucsl_fed', 'macro_cusr0000sac_fed',\n",
    "        'macro_cusr0000sad_fed', 'macro_cusr0000sas_fed',\n",
    "        'macro_ddurrg3m086sbea_fed', 'macro_dndgrg3m086sbea_fed',\n",
    "        'macro_dpcera3m086sbea_fed', 'macro_dserrg3m086sbea_fed',\n",
    "        'macro_dtcthfnm_fed', 'macro_excausx_fed', 'macro_exjpusx_fed',\n",
    "        'macro_exszusx_fed', 'macro_exusukx_fed', 'macro_fedfunds_fed',\n",
    "        'macro_gs1_fed', 'macro_gs10_fed', 'macro_gs5_fed', \n",
    "        'macro_houst_fed', 'macro_hwiuratio_fed', 'macro_indpro_fed',\n",
    "        'macro_invest_fed', 'macro_m1sl_fed', 'macro_m2real_fed', \n",
    "        'macro_m2sl_fed', 'macro_bogmbase_fed', 'macro_manemp_fed',\n",
    "        'macro_nonrevsl_fed', 'macro_oilpricex_fed', 'macro_payems_fed', \n",
    "        'macro_pcepi_fed', 'macro_permit_fed', 'macro_realln_fed',\n",
    "        'macro_rpi_fed', 'macro_tb3ms_fed', 'macro_tb6ms_fed', \n",
    "        'macro_totresns_fed', 'macro_twexafegsmthx_fed', 'macro_uempmean_fed',\n",
    "        'macro_umcsentx_fed', 'macro_unrate_fed', 'macro_uscons_fed', \n",
    "        'macro_usfire_fed', 'macro_usgood_fed', 'macro_vixclsx_fed',\n",
    "        'macro_ces06_fed', 'macro_ces20_fed', 'macro_ces30_fed', 'macro_snp500_fed',\n",
    "        'macro_snp_pe_fed', 'macro_snp_div_yield_fed', 'macro_snp_indust_fed']\n",
    "\n",
    "    # drop fed columns not used\n",
    "    fed_cols_to_drop = list(set(fed_cols).difference(set(fed_cols_to_clean)))\n",
    "    fed_cols_to_drop = list(set(fed_cols_to_drop).intersection(set(macro_df.columns)))\n",
    "    macro_df = macro_df.drop(fed_cols_to_drop, axis=1)\n",
    "\n",
    "    # rename columns and ensure clean\n",
    "    for col in fed_cols_to_clean:\n",
    "        new_col = col[:-4]+'_t'\n",
    "        macro_df = macro_df.rename(columns={col: new_col})\n",
    "        assert 0 == macro_df[new_col].isnull().sum()\n",
    "\n",
    "    return macro_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5749d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCustomMacroColumns(macro_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # final all fred columns\n",
    "    fred_cols = [col for col in macro_df.columns if ('_fred' in col) | ('_ui' in col)]\n",
    "\n",
    "    # rename messed up columns\n",
    "    macro_df = macro_df.rename(columns={'macro_teu-sca_ui': 'macro_teu_sca_ui',\n",
    "        'macro_tmu-sca_ui': 'macro_tmu_sca_ui'})\n",
    "\n",
    "    # make list of columns to clean and keep\n",
    "    fred_cols_to_clean = ['macro_dgs1mo_fred', 'macro_expinf10yr_fred',\n",
    "        'macro_expinf1yr_fred', 'macro_expinf20yr_fred',\n",
    "        'macro_expinf2yr_fred', 'macro_expinf30yr_fred',\n",
    "        'macro_expinf3yr_fred', 'macro_expinf5yr_fred',\n",
    "        'macro_t10yie_fred', 'macro_t20yiem_fred',\n",
    "        'macro_t30yiem_fred', 'macro_t5yie_fred']\n",
    "    ui_cols_to_clean = ['macro_teu_sca_ui', 'macro_tmu_sca_ui', 'macro_emv_ui', \n",
    "        'macro_emv_inflation_ui', 'macro_gepu_ui', 'macro_us_mpu_ui']\n",
    "\n",
    "    # drop fred columns not used\n",
    "    cols_to_drop = list(set(fred_cols).difference(set(fred_cols_to_clean)))\n",
    "    cols_to_drop = list(set(cols_to_drop).difference(set(ui_cols_to_clean)))\n",
    "    cols_to_drop = list(set(cols_to_drop).intersection(set(macro_df.columns)))\n",
    "    macro_df = macro_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    # rename columns and ensure clean\n",
    "    for col in fred_cols_to_clean:\n",
    "        new_col = col[:-5]+'_t'\n",
    "        macro_df = macro_df.rename(columns={col: new_col})\n",
    "        macro_df[new_col] = macro_df[new_col].ffill()\n",
    "        assert 0 == macro_df[new_col].isnull().sum()\n",
    "    for col in ui_cols_to_clean:\n",
    "        new_col = col[:-3]+'_t'\n",
    "        macro_df = macro_df.rename(columns={col: new_col})\n",
    "        macro_df[new_col] = macro_df[new_col].ffill()\n",
    "        assert 0 == macro_df[new_col].isnull().sum()\n",
    "\n",
    "    return macro_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2acd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanRemainingMacroColumns(macro_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # drop cols\n",
    "    cols_to_drop = ['macro_btc_adr_act_rec_cnt_cm', 'macro_btc_adr_act_sent_cnt_cm',\n",
    "        'macro_eth_fee_rev_pct_cm', 'macro_btc_fee_rev_pct_cm',\n",
    "        'macro_eth_avg_fee_san', 'macro_btc_puell_mul_tot_cm',\n",
    "        'macro_eth_puell_mul_tot_cm', 'macro_altcoin_usd_volume_24h_cmc',\n",
    "        'macro_btc_dominance_cmc', 'macro_total_adr_act_rec_cnt_cm',\n",
    "        'macro_total_adr_act_sent_cnt_cm']\n",
    "    macro_df = macro_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    # adjust columns before avg\n",
    "    macro_df['macro_total_usd_volume_24h_cmc'] = macro_df['macro_total_usd_volume_24h_cmc']/24\n",
    "    macro_df['macro_ex_usd_volume_24h_dex_cmc'] = macro_df['macro_ex_usd_volume_24h_dex_cmc']/24\n",
    "\n",
    "    # set up lists for averaging\n",
    "    col_pairs_to_avg = [['macro_btc_cap_mvrv_cur_cm', 'macro_btc_mvrv_san'],\n",
    "                        ['macro_eth_fee_med_usd_cm', 'macro_eth_median_fee_san'],\n",
    "                        ['macro_eth_mvrv_san', 'macro_eth_cap_mvrv_cur_cm'],\n",
    "                        ['macro_eth_total_fee_san', 'macro_eth_fee_tot_usd_cm'],\n",
    "                        ['macro_ex_volume_spot_usd_cm', 'macro_total_usd_volume_24h_cmc'],\n",
    "                        ['macro_total_dex_volume_san', 'macro_ex_usd_volume_24h_dex_cmc']]\n",
    "    new_col_names = ['macro_btc_mvrv_t', 'macro_eth_fee_med_t', 'macro_eth_mvrv_t', \n",
    "                    'macro_eth_total_fee_t', 'macro_ex_volume_t', 'macro_dex_volume_t']\n",
    "    for new_col, col_pair in zip(new_col_names, col_pairs_to_avg):\n",
    "        macro_df[new_col] = macro_df[col_pair].mean(axis=1)\n",
    "        macro_df[new_col] = macro_df[new_col].ffill()\n",
    "        assert 0 == macro_df[new_col].isnull().sum()\n",
    "        macro_df = macro_df.drop(col_pair, axis=1)\n",
    "\n",
    "    # obtaining remaining macro columns \n",
    "    cols_end_in_t = [col for col in macro_df.columns if col[-2:]=='_t']\n",
    "    remaining_cols = list(set(macro_df.columns).difference(set(cols_end_in_t)))\n",
    "    remaining_cols.remove('date')\n",
    "\n",
    "    # clean remaining cols\n",
    "    for col in remaining_cols:\n",
    "        last_underscore_index = col.rfind('_')\n",
    "        new_col = col[:last_underscore_index]+'_t'\n",
    "        macro_df = macro_df.rename(columns={col: new_col})\n",
    "        macro_df[new_col] = macro_df[new_col].ffill()\n",
    "        macro_df.loc[macro_df[new_col].isnull(), new_col] = 0\n",
    "        assert 0 == macro_df[new_col].isnull().sum()\n",
    "    \n",
    "    return macro_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89ced420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineAndCleanPanelAndMacro(panel_df: pd.DataFrame, macro_df: pd.DataFrame,\n",
    "    asset_universe_dict: Dict[str, list]) -> pd.DataFrame:\n",
    "    \"\"\" Perform various checks and combine the panel and macro data. \"\"\"\n",
    "    # merge on macro variables\n",
    "    panel_df = panel_df.merge(macro_df, on='date', how='left', validate='many_to_one')\n",
    "\n",
    "    # ensure i have all hours\n",
    "    min_dt, max_dt = panel_df.date.min(), panel_df.date.max()\n",
    "    full_date_range = pd.date_range(start=min_dt, end=max_dt, freq='1H')\n",
    "    assert len(panel_df.date.unique()) == len(full_date_range)\n",
    "\n",
    "    # ensure no missing values\n",
    "    assert 0 == panel_df.isnull().sum().sum()\n",
    "\n",
    "    # ensure date column is clean\n",
    "    assert len(panel_df) == panel_df[panel_df.date.dt.minute==0].shape[0]\n",
    "\n",
    "    # ensure asset column is clean\n",
    "    asset_universe = Helper.findUniqueAssets(asset_universe_dict)\n",
    "    assert len(panel_df) == panel_df[panel_df.asset.isin(asset_universe)].shape[0]\n",
    "\n",
    "    # ensure no duplicates\n",
    "    assert not panel_df.duplicated(subset=['date', 'asset']).any()\n",
    "\n",
    "    # sort by date then asset and reset index\n",
    "    panel_df = panel_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "\n",
    "    # sort columns\n",
    "    cols = list(panel_df.columns.values)\n",
    "    first_cols = ['date', 'asset', 'r_ex_tp1', 'r_ex_tp24', \n",
    "        'char_price_t', 'char_volume_t', 'char_trades_t', 'char_mcap_t']\n",
    "    asset_usage_cols = [col for col in panel_df.columns if 'char_asset_usage' in col]\n",
    "    asset_usage_cols.sort()\n",
    "    static_cols = [col for col in panel_df.columns if 'char_industry' in col]\n",
    "    static_cols += ['char_pow', 'char_pos']\n",
    "    static_cols += asset_usage_cols\n",
    "    static_cols += ['char_ico_price', 'char_ico', 'char_ico_days_since_t']\n",
    "    cols_to_remove = first_cols + static_cols\n",
    "    for col in cols_to_remove:\n",
    "        cols.remove(col)\n",
    "    cols.sort()\n",
    "    cols_before = panel_df.shape[1]\n",
    "    panel_df = panel_df[cols_to_remove+cols]\n",
    "    assert panel_df.shape[1] == cols_before\n",
    "\n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    CW_IN_FP            = '../data/clean/cw.pkl'\n",
    "    ASSET_IN_FP         = '../data/derived/asset_universe_dict.pickle'\n",
    "    ASSET_OUT_FP        = '../data/clean/asset_universe_dict.pickle'\n",
    "    CA_PANEL_IN_FP      = '../data/derived/ca_panel.pkl'\n",
    "    CA_MACRO_IN_FP      = '../data/derived/ca_macro.pkl'\n",
    "    CM_PANEL_IN_FP      = \"../data/derived/cm_panel.pkl\"\n",
    "    CM_MACRO_IN_FP      = '../data/derived/cm_macro.pkl'\n",
    "    CG_PANEL_IN_FP      = '../data/derived/cg_panel.pkl'\n",
    "    CMC_PANEL_IN_FP     = '../data/derived/cmc_panel.pkl'\n",
    "    CMC_MACRO_IN_FP     = '../data/derived/cmc_macro.pkl'\n",
    "    SAN_PANEL_IN_FP     = \"../data/derived/san_panel.pkl\"\n",
    "    SAN_MACRO_IN_FP     = '../data/derived/san_macro.pkl'\n",
    "    MACRO_IN_FP         = '../data/derived/macro.pkl'\n",
    "    ASSET_ICO_IN_FP     = '../data/derived/momtaz_ico_asset.pkl' \n",
    "    MACRO_ICO_IN_FP     = '../data/derived/momtaz_ico_macro.pkl' \n",
    "    MESSARI_IN_FP       = '../data/derived/messari.pkl'\n",
    "    CM_RAW_PANEL_IN_FP  = '../data/raw/coinmetrics_panel_hourly.pkl'\n",
    "    PANEL_OUT_FP        = '../data/clean/panel.pkl'\n",
    "\n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    \n",
    "    # import and form panel and macro data\n",
    "    panel_df = formPanel(CW_IN_FP, CA_PANEL_IN_FP, CM_PANEL_IN_FP, CMC_PANEL_IN_FP,\n",
    "                CG_PANEL_IN_FP, SAN_PANEL_IN_FP, ASSET_ICO_IN_FP, MESSARI_IN_FP)\n",
    "    macro_df = formMacro(MACRO_IN_FP, CA_MACRO_IN_FP, CM_MACRO_IN_FP,\n",
    "        CMC_MACRO_IN_FP, SAN_MACRO_IN_FP, MACRO_ICO_IN_FP)\n",
    "    \n",
    "    # update asset universe\n",
    "    panel_df, asset_universe_dict = finalizeAssetUniverse(panel_df, asset_universe_dict)\n",
    "    with open(ASSET_OUT_FP, 'wb') as f:\n",
    "        pickle.dump(asset_universe_dict, f)\n",
    "\n",
    "    # clean panel columns\n",
    "    panel_df = formPrices(panel_df)\n",
    "    panel_df = formLHSs(panel_df, macro_df)\n",
    "    panel_df = formVolumes(panel_df)\n",
    "    panel_df = formMcap(panel_df)\n",
    "    panel_df = formStaticCharacteristics(panel_df)\n",
    "    panel_df = cleanDistributionCols(panel_df)\n",
    "    panel_df = formSupplyCols(panel_df, CM_RAW_PANEL_IN_FP) \n",
    "    panel_df = formDevActivity(panel_df)\n",
    "    panel_df = cleanCmcColumns(panel_df)\n",
    "    panel_df = formSocial(panel_df)\n",
    "    panel_df = formAddrCols(panel_df)\n",
    "    panel_df = cleanDepositAndWithdrawCols(panel_df)\n",
    "    panel_df = cleanAge(panel_df)\n",
    "    panel_df = cleanCryptoValuationCols(panel_df)\n",
    "    panel_df = cleanFlowCols(panel_df)\n",
    "    panel_df = cleanCirculationCols(panel_df)\n",
    "    panel_df = cleanBalanceCols(panel_df)\n",
    "    \n",
    "    # clean macro columns\n",
    "    macro_df = formStablecoinDeviation(macro_df)\n",
    "    macro_df = formICO(macro_df)\n",
    "    macro_df = formMcCrakenColumns(macro_df)\n",
    "    macro_df = cleanCustomMacroColumns(macro_df)\n",
    "    macro_df = cleanRemainingMacroColumns(macro_df)\n",
    "\n",
    "    # combine the panel and macro data\n",
    "    panel_df = combineAndCleanPanelAndMacro(panel_df, macro_df, asset_universe_dict)\n",
    "\n",
    "    # output\n",
    "    panel_df.to_pickle(PANEL_OUT_FP)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
