{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "944cf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# IMPORT PACKAGES\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dot\n",
    "from keras.models import Model\n",
    "from typing import Dict, List\n",
    "from tensorflow import keras\n",
    "from tools import QuantTools\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ed6bbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropRowsColsAndNormalizeForCA(df: pd.DataFrame, lhs_col: str) -> pd.DataFrame:\n",
    "   # drop 2018-2019 and 2022 data\n",
    "   # - '18-'19 does not have enough assets\n",
    "   # - 2022 is oos for now\n",
    "   df = df[~df.date.dt.year.isin([2018, 2019])].reset_index(drop=True)\n",
    "\n",
    "   # Set characteristics of interest\n",
    "   selected_rhs = ['char_addr_new_log_delta_tm2_tm1',\n",
    "      'char_delta_flow_dist_tm1h',\n",
    "      'char_exchange_inflow_tm1h',\n",
    "      'char_exchange_prct_circ_supply_t',\n",
    "      'char_sent_volume_consumed_tm1',\n",
    "      'char_r_tm1h',\n",
    "      'char_r_tm2h',\n",
    "      'char_r_industry_tm6h',\n",
    "      'char_var5_tm1',\n",
    "      'char_var5_tm7',\n",
    "      'char_shortfall5_tm90',\n",
    "      'char_vol_tm6h']\n",
    "\n",
    "   # Cut to characteristics columns of interest\n",
    "   df = df[['date', 'asset', lhs_col]+selected_rhs]\n",
    "\n",
    "   # Normalize characteristics to be between 0 and 1.\n",
    "   for col in selected_rhs:\n",
    "      df[col] = (df.groupby('date')[col].rank() - 1) / (df.groupby('date')[col].transform('count') - 1)\n",
    "\n",
    "   # Assert range \n",
    "   assert 0 == df[selected_rhs].min().min()\n",
    "   assert 1 == df[selected_rhs].max().max()\n",
    "   assert -1 < df[lhs_col].min()\n",
    "   assert 2 >= df[lhs_col].max()\n",
    "\n",
    "   # NOTE: keep obs to RHS ratio roughly 4e4:1\n",
    "\n",
    "   # NOTE: for any macro, take cartesian product with characteristics to make it characteritisc level\n",
    "   # -or do the reg thing to reduce it down to same dim as number of assets\n",
    "\n",
    "   return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8e12308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPortfolioReturnCovariates(df: pd.DataFrame, lhs_col: str) -> pd.DataFrame:\n",
    "    # Obtain the datetimes of the dataframe\n",
    "    df = df.sort_values(by = 'date')\n",
    "    datetimes = np.unique(df.date.values)\n",
    "\n",
    "    # Form new covariate names\n",
    "    characteristics = list(df.columns.values)\n",
    "    characteristics.remove('date')\n",
    "    characteristics.remove('asset')\n",
    "    characteristics.remove(lhs_col)\n",
    "    new_covars = ['x_' + char[5:] for char in characteristics]\n",
    "\n",
    "    # Loop over all datetimes\n",
    "    for current_dt in datetimes: \n",
    "        # Obtain the datetime's LHS \"tomorrow\" returns and the covariates\n",
    "        r_tp1 = df[df.date == current_dt].r_ex_tp1.values\n",
    "        z_t   = df[df.date == current_dt][characteristics].values\n",
    "        \n",
    "        # Calculate the characteristic managed portfolio returns\n",
    "        design = np.linalg.inv(np.matmul(np.transpose(z_t), z_t))\n",
    "        x_tp1  = np.matmul(np.matmul(design, np.transpose(z_t)), r_tp1)\n",
    "        \n",
    "        # Set the new columns to this week's vector's value\n",
    "        df.loc[df.date == current_dt, new_covars] = x_tp1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4df66ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAutoencoder(b_covars, x_covars, \n",
    "    number_hidden_layer, l1_penalty, weight_initializer, bias_initializer, number_factor, learning_rate):\n",
    "    # Build the betas model from the time t covariates\n",
    "    model_b = tf.keras.models.Sequential()\n",
    "    model_b.add(tf.keras.Input(shape=(len(b_covars),)))\n",
    "    for j in range(number_hidden_layer):\n",
    "        model_b.add(Dense(16*1/(2**(j)), activation='relu',\n",
    "                        kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                        kernel_initializer=weight_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "        model_b.add(BatchNormalization())\n",
    "    model_b.add(Dense(number_factor, activation='linear',\n",
    "                    kernel_initializer=weight_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "\n",
    "    # Form the x model from time t plus 1 returns\n",
    "    model_x = tf.keras.models.Sequential()\n",
    "    model_x.add(tf.keras.Input(shape=(len(x_covars),)))\n",
    "    model_x.add(Dense(number_factor, activation='linear',\n",
    "                    kernel_initializer=weight_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "\n",
    "    # Form the dot product output for the combination of the two neurals\n",
    "    mergedOut = Dot(axes=(1,1))([model_b.output, model_x.output])\n",
    "\n",
    "    # Form the entire model\n",
    "    model = Model([model_b.input, model_x.input], mergedOut)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fitAutoencoder(train_df: pd.DataFrame, \n",
    "    hps_dict: dict, lhs_col: str='r_ex_tp1', rhs_cols: list=[], \n",
    "    val_df: pd.DataFrame=None, return_train_r2: bool=False) -> list:\n",
    "    # Obtain beta and factor side covariates\n",
    "    b_covars = [covar for covar in rhs_cols if covar[:4] == 'char']\n",
    "    x_covars = [covar for covar in rhs_cols if covar[:2] == 'x_']\n",
    "    assert set(rhs_cols) == (set(b_covars).union(set(x_covars)))\n",
    "\n",
    "    # Extract the hyperparameters\n",
    "    number_hidden_layer = hps_dict['num_hidden_layers']\n",
    "    number_factor       = hps_dict['number_factors']\n",
    "    learning_rate       = hps_dict['learning_rates']\n",
    "    l1_penalty          = hps_dict['l1_penalties']\n",
    "    batch_size          = hps_dict['batch_sizes']\n",
    "    number_ensemble     = hps_dict['num_ensemble']\n",
    "    epoch               = hps_dict['epochs']\n",
    "    early_stopping      = hps_dict['early_stopping']\n",
    "\n",
    "    # Loop over the ensembles to build models for each\n",
    "    models = []\n",
    "    num_epochs_trained = []\n",
    "    assert(number_ensemble <= 10), 'whatcha think you got infinite come pew ters'\n",
    "    for i in range(number_ensemble):\n",
    "        # Obtain the training input and output data and, if passed, validation data\n",
    "        train_b = train_df[b_covars]\n",
    "        train_x = train_df[x_covars]  \n",
    "        train_y = train_df[[lhs_col]]\n",
    "        if val_df is not None:\n",
    "            val_b = val_df[b_covars]\n",
    "            val_x = val_df[x_covars]  \n",
    "            val_y = val_df[[lhs_col]]\n",
    "\n",
    "        # According to which model in the ensemble it is, initialize parameters.\n",
    "        random.seed(i*42)\n",
    "        initializer_list = [initializers.HeNormal(seed=i), \n",
    "                            initializers.GlorotUniform(seed=i), \n",
    "                            initializers.RandomUniform(seed=i)]\n",
    "        initializer_pair = random.sample(initializer_list, 2)\n",
    "        weight_initializer = initializer_pair[0]\n",
    "        bias_initializer   = initializer_pair[1]\n",
    "\n",
    "        # Build the model\n",
    "        model = buildAutoencoder(b_covars, x_covars, number_hidden_layer, \n",
    "            l1_penalty, weight_initializer, bias_initializer, number_factor, learning_rate)\n",
    "\n",
    "        # Fit the model\n",
    "        with tf.device('/GPU:0'):\n",
    "            if early_stopping == True:\n",
    "                es = EarlyStopping(monitor='val_mse', mode='min', verbose=0, patience = 5) \n",
    "                \n",
    "                model.fit(x=[train_b, train_x], y=train_y, \n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=([val_b, val_x], val_y), \n",
    "                            epochs=epoch, verbose=0, callbacks=[es])\n",
    "                \n",
    "                num_epochs = es.stopped_epoch\n",
    "                num_epochs_trained.append(num_epochs)\n",
    "            else:\n",
    "                model.fit(x=[train_b, train_x], y=train_y, \n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epoch, verbose=0)\n",
    "\n",
    "                num_epochs_trained.append(epoch)\n",
    "        models.append(model)\n",
    "\n",
    "    if return_train_r2:\n",
    "        # build the time window for the training data to predict on\n",
    "        first_datetime = np.min(train_df.date.values)\n",
    "        month = np.datetime64(first_datetime, 'M').astype(int) % 12 + 1\n",
    "        year = np.datetime64(first_datetime, 'Y').astype(int) + 1970\n",
    "        oos_start_date = np.min(train_df[(train_df.date.dt.year==year) \n",
    "                            & (train_df.date.dt.month==month+1)].date.values)\n",
    "        oos_end_date   = np.max(train_df.date.values)\n",
    "\n",
    "        # fit on the training data for all models to report the r2_pred\n",
    "        train_yhats = genAutoencoderYhats(models, train_df, rhs_cols, oos_start_date, oos_end_date, number_factor)\n",
    "        train_ys    = train_df[train_df.date>=oos_start_date][lhs_col].values\n",
    "        train_r2_pred = 1-(np.mean(np.square(train_ys-train_yhats)))/(np.mean(np.square(train_ys)))\n",
    "    else:\n",
    "        train_r2_pred = 0\n",
    "    \n",
    "    return models, np.mean(num_epochs_trained), train_r2_pred\n",
    "\n",
    "def genAutoencoderYhats(models, in_df, rhs_cols, oos_start_date, oos_end_date, number_factor) -> np.array:\n",
    "    # Obtain beta and factor side covariates\n",
    "    b_covars = [covar for covar in rhs_cols if covar[:4] == 'char']\n",
    "    x_covars = [covar for covar in rhs_cols if covar[:2] == 'x_']\n",
    "    assert set(rhs_cols) == (set(b_covars).union(set(x_covars)))\n",
    "\n",
    "    # Obtain the oos data\n",
    "    oos_df = in_df[(in_df.date >= oos_start_date) & (in_df.date <= oos_end_date)].copy()\n",
    "    oos_b  = oos_df[b_covars]\n",
    "\n",
    "    # Form each model's results\n",
    "    b_hats = np.zeros((oos_df.shape[0], number_factor, len(models)))\n",
    "    lambda_hats = np.zeros((oos_df.shape[0], number_factor, len(models)))\n",
    "    oos_dates = np.unique(oos_df.date.values)\n",
    "    for i in range(len(models)):\n",
    "        # Update the model to use\n",
    "        model = models[i]\n",
    "\n",
    "        # Form the beta hats\n",
    "        layer_name = model.layers[-3]._name  \n",
    "        assert(model.layers[-3].output_shape[1] == number_factor)\n",
    "        b_hat_layer = Model(inputs=model.input[0],\n",
    "                            outputs=model.get_layer(layer_name).output)\n",
    "        b_hat = b_hat_layer.predict(oos_b, verbose=0) \n",
    "        b_hats[:,:,i] = b_hat\n",
    "\n",
    "        # Form the sample average of the estimated factors up to each oos date\n",
    "        # build this model's mapping from input to f hat\n",
    "        model = models[i]\n",
    "        layer_name = model.layers[-2]._name \n",
    "        assert(model.layers[-2].output_shape[1] == number_factor)\n",
    "        f_hat_layer = Model(inputs=model.input[1],\n",
    "                            outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "        # estimate this model's f hats for dates in month before the oos dates\n",
    "        x = in_df[(in_df.date >= (oos_start_date-np.timedelta64(30, 'D'))) \n",
    "                & (in_df.date < oos_start_date)][x_covars]\n",
    "        f_hat = f_hat_layer.predict(x, verbose=0).astype('float32')\n",
    "        assert(all(np.isclose(f_hat[0,:], f_hat[1,:])))\n",
    "        f_hats = np.sum(f_hat, axis=0)\n",
    "\n",
    "        # obtain the f hats for the entire oos period\n",
    "        x = in_df[in_df.date >= oos_start_date][x_covars]\n",
    "        f_hat = f_hat_layer.predict(x, verbose=0).astype('float32')\n",
    "        assert(all(np.isclose(f_hat[0,:], f_hat[1,:])))\n",
    "\n",
    "        # determine the lambda hat for each oos date\n",
    "        lambda_hat_index_start = 0\n",
    "        for t in range(len(oos_dates)):\n",
    "            # update this oos date \n",
    "            oos_date = oos_dates[t]\n",
    "\n",
    "            # update the fhats with the appropriate f_hat values\n",
    "            # -update start index in here so this is skipped on first run but occurs\n",
    "            #  on every other before we update the end index two lines below\n",
    "            if t != 0:\n",
    "                f_hats += np.sum(f_hat[lambda_hat_index_start:lambda_hat_index_end, :], axis=0)\n",
    "                lambda_hat_index_start = lambda_hat_index_end\n",
    "\n",
    "            # determine how many obs are in this oos date\n",
    "            num_rows_in_oos_dt = in_df[in_df.date==oos_date].shape[0]\n",
    "            \n",
    "            # update the end index given the number of oos obs for this date\n",
    "            lambda_hat_index_end = lambda_hat_index_start + num_rows_in_oos_dt\n",
    "\n",
    "            # divide by total number of f_hats added together to figure out TS average for this oos_date\n",
    "            #     save as this time period's and this model's lambda hat\n",
    "            lambda_hats[lambda_hat_index_start:lambda_hat_index_end, :, i] = (\n",
    "                np.tile(f_hats / in_df[(in_df.date >= (oos_start_date-np.timedelta64(30, 'D'))) \n",
    "                                        & (in_df.date<oos_date)].shape[0], \n",
    "                        (num_rows_in_oos_dt, 1)))\n",
    "\n",
    "    # Form model predictions of beta hats times lambda hats where\n",
    "    #     we take dot product between two factor length vectors for all time periods and models\n",
    "    #     and then average each model's forecast to return a vector of length of oos dataframe\n",
    "    yhats = np.mean(np.sum(b_hats * lambda_hats, axis=1), axis=1)\n",
    "\n",
    "    return yhats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c9f5a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df: pd.DataFrame, asset_universe_dict: Dict[str, List],\n",
    "    val_start_date: str, val_end_date: str, test_start_date: str, lhs_col: str,\n",
    "    hp_grid: Dict[str, list], periods_in_year: int, \n",
    "    cv_out_fp: str, arch_name: str) -> List[dict]:\n",
    "    # Subset to relevant data\n",
    "    df = df[df.date < test_start_date].copy()\n",
    "\n",
    "    # Initialize cv result objects\n",
    "    results_list = []\n",
    "\n",
    "    # Determine RHS columns\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove('date')\n",
    "    rhs_cols.remove('asset')\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # Determine validation datetimes to loop over and datetimes to refit at\n",
    "    val_dts_dict = {}\n",
    "    val_datetimes = np.unique(df[df.date>=val_start_date].date.values)\n",
    "    val_sun_midnights = np.unique(df[(df.date>=val_start_date) \n",
    "        & (df.date.dt.hour==0) & (df.date.dt.day_of_week==6)].date.values)\n",
    "\n",
    "    # Check if first val date is sunday midnight, if not then add the dates\n",
    "    first_val_date = np.min(df[(df.date==val_start_date)].date.values)\n",
    "    day_of_week_of_first_val_datetime = (first_val_date.astype('datetime64[D]').view('int64') - 4) % 7\n",
    "    if day_of_week_of_first_val_datetime != 6:\n",
    "        val_dts_dict[first_val_date] = np.unique(df[(df.date>=first_val_date) & (df.date<val_sun_midnights[0])].date.values)\n",
    "\n",
    "    # Complete the dictionary with all the sundays as keys as the dates until the next sunday as the values\n",
    "    for val_sun_midnight in val_sun_midnights:\n",
    "        next_sun_midnight = val_sun_midnight + np.timedelta64(7, 'D')\n",
    "        val_dts_dict[val_sun_midnight] = np.unique(df[(df.date>=val_sun_midnight) \n",
    "                                            & (df.date<next_sun_midnight)\n",
    "                                            & (df.date<test_start_date)].date.values)\n",
    "\n",
    "    # Loop over hp combinations\n",
    "    keys = hp_grid.keys()\n",
    "    values = hp_grid.values()\n",
    "    hp_combos = list(itertools.product(*values))\n",
    "    for hps in hp_combos:\n",
    "\n",
    "        # Start the timer\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        # Create hp dictionary and other objects for this iteration\n",
    "        hps_dict = dict(zip(keys, hps))\n",
    "        hps_results_dict = hps_dict.copy()\n",
    "        val_y_yhats_df = pd.DataFrame()\n",
    "\n",
    "        # Report on progress\n",
    "        print(hps_dict)\n",
    "\n",
    "        # Initiate lists for results and start the loop over the val dates to fit and predict\n",
    "        avg_num_epochs_trained_list = []\n",
    "        train_r2_pred_list = []\n",
    "        for val_datetime_start in list(val_dts_dict.keys()): \n",
    "            print(val_datetime_start)\n",
    "            # form end of this window\n",
    "            val_datetime_end = np.max(val_dts_dict[val_datetime_start])\n",
    "\n",
    "            # form appropriate asset universe\n",
    "            first_day_of_month_for_current_val_dt = np.datetime_as_string(val_datetime_start, unit='M')+'-01'\n",
    "            asset_universe = asset_universe_dict[first_day_of_month_for_current_val_dt]\n",
    "\n",
    "            # form relevant date-assets given asset universe and the train and val dataframes\n",
    "            rel_df = df[(df.date<=val_datetime_end) & (df.asset.isin(asset_universe))].copy()\n",
    "            train_df = rel_df[rel_df.date<val_datetime_start].copy()\n",
    "            val_df = rel_df[(rel_df.date>=val_datetime_start) & (rel_df.date<=val_datetime_end)].copy()\n",
    "\n",
    "            # fit and predict\n",
    "            models, avg_num_epochs_trained, train_r2_pred = fitAutoencoder(\n",
    "                train_df, hps_dict, lhs_col, rhs_cols, val_df, return_train_r2=False)\n",
    "            yhats = genAutoencoderYhats(models, rel_df, rhs_cols,\n",
    "                val_datetime_start, val_datetime_end, hps_dict['number_factors'])\n",
    "            del rel_df, models\n",
    "            gc.collect()\n",
    "\n",
    "            # save the results\n",
    "            avg_num_epochs_trained_list.append(avg_num_epochs_trained)\n",
    "            train_r2_pred_list.append(train_r2_pred)\n",
    "            temp_yhats_df = val_df[['date', 'asset', lhs_col]].copy()\n",
    "            temp_yhats_df['yhats'] = yhats\n",
    "            val_y_yhats_df = pd.concat([val_y_yhats_df, temp_yhats_df])\n",
    "\n",
    "            # output this week r2_pred and return\n",
    "            val_week_df = val_y_yhats_df[(val_y_yhats_df.date>=val_datetime_start) \n",
    "                                        & (val_y_yhats_df.date<=val_datetime_end)].copy()\n",
    "            val_week_df = val_week_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "            val_week_df = QuantTools.formPortfolioPositionsQuantileLongShort(val_week_df, 5)\n",
    "            val_week_df['returns'] = val_week_df.position*val_week_df[lhs_col]\n",
    "            val_week_y = val_week_df[lhs_col].values\n",
    "            val_week_yhats = val_week_df['yhats'].values\n",
    "            val_week_returns = val_week_df[val_week_df.position !=0].groupby('date')['returns'].sum().values\n",
    "            val_week_r_2_pred = 1-np.mean(np.square(val_week_y - val_week_yhats))/np.mean(np.square(val_week_y))\n",
    "            print(f'this week r 2 pred: {val_week_r_2_pred}')\n",
    "            print(f'this week geom avg ret {QuantTools.calcGeomAvg(val_week_returns)}')\n",
    "\n",
    "        # Stop the timer\n",
    "        toc = time.perf_counter()\n",
    "\n",
    "        # For this hp, add metadata to results dict\n",
    "        hps_results_dict['avg_epochs_trained'] = np.mean(avg_num_epochs_trained_list)\n",
    "        hps_results_dict['val_start_date'] = val_start_date\n",
    "        hps_results_dict['val_end_date'] = val_end_date\n",
    "        hps_results_dict['arch_name'] = arch_name\n",
    "        hps_results_dict['runtime'] = round((toc - tic)/60, 0) \n",
    "\n",
    "        # For this hp, add training period statistics\n",
    "        hps_results_dict['train_r2_pred_min'] = np.min(train_r2_pred_list)\n",
    "        hps_results_dict['train_r2_pred_mean'] = np.mean(train_r2_pred_list)\n",
    "        hps_results_dict['train_r2_pred_max'] = np.max(train_r2_pred_list)\n",
    "\n",
    "        # For this hp, obtain the yhats and ys and positions\n",
    "        val_y_yhats_df = val_y_yhats_df.dropna()\n",
    "        val_y_yhats_df = val_y_yhats_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "        val_y_yhats_pos_df = QuantTools.formPortfolioPositionsQuantileLongShort(val_y_yhats_df, 5)\n",
    "        val_yhats = val_y_yhats_pos_df.yhats.values\n",
    "        val_ys    = val_y_yhats_pos_df[lhs_col].values\n",
    "        val_y_yhats_pos_df['returns'] = val_y_yhats_pos_df.position*val_y_yhats_pos_df[lhs_col]\n",
    "        returns = val_y_yhats_pos_df[val_y_yhats_pos_df.position != 0].groupby('date')['returns'].sum().values\n",
    "        assert len(val_yhats) == len(val_ys)\n",
    "\n",
    "        # For this hp, form validation period statistics\n",
    "        hps_results_dict['val_mse']       = np.mean(np.square(val_ys-val_yhats))\n",
    "        hps_results_dict['val_r2_pred']   = 1-np.mean(np.square(val_ys-val_yhats))/np.mean(np.square(val_ys))\n",
    "        hps_results_dict['val_yhat_min']  = np.min(val_yhats)\n",
    "        hps_results_dict['val_yhat_q1']   = np.quantile(val_yhats, q=0.25)\n",
    "        hps_results_dict['val_yhat_q2']   = np.quantile(val_yhats, q=0.5)\n",
    "        hps_results_dict['val_yhat_mean'] = np.mean(val_yhats)\n",
    "        hps_results_dict['val_yhat_q3']   = np.quantile(val_yhats, q=0.75)\n",
    "        hps_results_dict['val_yhat_max']  = np.max(val_yhats)\n",
    "        hps_results_dict['geom_mean_1h']  = QuantTools.calcGeomAvg(returns)\n",
    "        hps_results_dict['sharpe_annual'] = QuantTools.calcSharpe(returns, periods_in_year=periods_in_year)\n",
    "        hps_results_dict['sd_annual']     = QuantTools.calcSD(returns, periods_in_year=periods_in_year)\n",
    "        hps_results_dict['max_dd']        = QuantTools.calcMaxDrawdown(returns)\n",
    "        hps_results_dict['avg_turnover']  = QuantTools.calcTSAvgTurnover(val_y_yhats_pos_df)\n",
    "\n",
    "        # Save results to return\n",
    "        results_list.append(hps_results_dict)\n",
    "\n",
    "        # For this hp, save results to csv\n",
    "        cv_df = pd.DataFrame(results_list)\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = cv_out_fp + '-' + arch_name + '-' + timestr + '.csv'\n",
    "        cv_df.to_csv(fp, index=False)\n",
    "\n",
    "    # Return cv results\n",
    "    return results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8c9f8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTestPeriod(df: pd.DataFrame, asset_universe_dict: Dict[str, list],\n",
    "    test_start_date: str, lhs_col: str, opt_hps_dict: dict) -> pd.DataFrame:\n",
    "    # Confirm separate df formed\n",
    "    df = df.copy()\n",
    "\n",
    "    # Determine RHS columns\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove('date')\n",
    "    rhs_cols.remove('asset')\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # Determine test period datetimes to loop over and datetimes to refit at\n",
    "    test_dts_dict = {}\n",
    "    test_sun_midnights = np.unique(df[(df.date>=test_start_date) \n",
    "        & (df.date.dt.hour==0) & (df.date.dt.day_of_week==6)].date.values)\n",
    "\n",
    "    # Check if first test date is sunday midnight, if not then add the dates\n",
    "    first_test_datetime = np.min(df[(df.date==test_start_date)].date.values)\n",
    "    day_of_week_of_first_test_datetime = (first_test_datetime.astype('datetime64[D]').view('int64') - 4) % 7\n",
    "    if day_of_week_of_first_test_datetime != 6:\n",
    "        test_dts_dict[first_test_datetime] = np.unique(df[(df.date>=first_test_datetime) & (df.date<test_sun_midnights[0])].date.values)\n",
    "\n",
    "    # Complete the dictionary with all the sundays as keys as the dates until the next sunday as the testues\n",
    "    for test_sun_midnight in test_sun_midnights:\n",
    "        next_sun_midnight = test_sun_midnight + np.timedelta64(7, 'D')\n",
    "        test_dts_dict[test_sun_midnight] = np.unique(df[(df.date>=test_sun_midnight) \n",
    "                                            & (df.date<next_sun_midnight)].date.values)\n",
    "        \n",
    "    # Create dataframe of results to return\n",
    "    test_y_yhats_df = pd.DataFrame()\n",
    "\n",
    "    # Loop over all the datetimes in the test period where we want to refit the model\n",
    "    for test_datetime_start in list(test_dts_dict.keys()):\n",
    "        # Monitor progress\n",
    "        print('Currently fitting and predicting for the week starting: ')\n",
    "        print(test_datetime_start)\n",
    "\n",
    "        # Form end of this window\n",
    "        test_datetime_end = np.max(test_dts_dict[test_datetime_start])\n",
    "\n",
    "        # Form appropriate asset universe\n",
    "        first_day_of_month_for_current_test_dt = np.datetime_as_string(test_datetime_start, unit='M')+'-01'\n",
    "        asset_universe = asset_universe_dict[first_day_of_month_for_current_test_dt]\n",
    "\n",
    "        # From relevant date-assets given asset universe and the train and oos dataframes\n",
    "        rel_df   = df[(df.date<=test_datetime_end) & (df.asset.isin(asset_universe))].copy()\n",
    "        train_df = rel_df[rel_df.date<test_datetime_start].copy()\n",
    "        oos_df   = rel_df[(rel_df.date>=test_datetime_start) & (rel_df.date<=test_datetime_end)].copy()\n",
    "\n",
    "        # Fit and predict\n",
    "        models, _, _ = fitAutoencoder(train_df, opt_hps_dict, lhs_col, rhs_cols)\n",
    "        yhats = genAutoencoderYhats(models, rel_df, rhs_cols, test_datetime_start, test_datetime_end, opt_hps_dict['number_factors'])\n",
    "        del rel_df, train_df\n",
    "        gc.collect()\n",
    "        temp_yhats_df = oos_df[['date', 'asset', lhs_col]].copy()\n",
    "        temp_yhats_df['yhats'] = yhats\n",
    "        test_y_yhats_df = pd.concat([test_y_yhats_df, temp_yhats_df])\n",
    "\n",
    "    return test_y_yhats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    TRAIN_IN_FP     = '../data/clean/panel_train.pkl'\n",
    "    TEST_IN_FP      = '../data/clean/panel_test.pkl'\n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    CV_OUT_FP       = '../output/high_dim_fm/cv_results'\n",
    "    TEST_OUT_FP     = '../data/clean/test_yhats_autoencoder.pkl'\n",
    "    LHS_COL         = 'r_ex_tp1'\n",
    "    VAL_START_DATE  = '2021-01-01'\n",
    "    VAL_END_DATE    = '2021-06-30'\n",
    "    TEST_START_DATE = '2021-07-01'\n",
    "    NUM_CPUS        = 22 \n",
    "    PERIODS_IN_YEAR = 365*24\n",
    "    ARCH_NAME       = 'autoencoder-12rhs'\n",
    "    HP_GRID         = {'number_factors': [1],\n",
    "        'num_hidden_layers': [1],\n",
    "        'learning_rates': [5e-5], \n",
    "        'batch_sizes': [2048],\n",
    "        'l1_penalties': [1e-4],\n",
    "        'num_ensemble': [10],\n",
    "        'early_stopping': [True],\n",
    "        'epochs': [100]} \n",
    "\n",
    "    # read in data\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    all_df = pd.read_pickle(TRAIN_IN_FP)\n",
    "    test_df = pd.read_pickle(TEST_IN_FP)\n",
    "    all_df = pd.concat([all_df, test_df])\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "\n",
    "    # drop rows and columns such that data will work for conditional autoencoder (CA)\n",
    "    all_df = dropRowsColsAndNormalizeForCA(all_df, LHS_COL) # NOTE: ~6 min RUNTIME\n",
    "\n",
    "    # form the char-sorted portfolios for factor side of CA input\n",
    "    all_df = formPortfolioReturnCovariates(all_df, LHS_COL) # NOTE: ~12 min runtime\n",
    "\n",
    "    # run CV\n",
    "    cv_results_list = runCV(all_df, asset_universe_dict,\n",
    "        VAL_START_DATE, VAL_END_DATE, TEST_START_DATE,\n",
    "        LHS_COL, HP_GRID, PERIODS_IN_YEAR, CV_OUT_FP, ARCH_NAME)\n",
    "\n",
    "    # choose optimal hyperparameter combination based on validation period predictive R^2\n",
    "    opt_val_r2_pred = -1e6\n",
    "    for cv_result in cv_results_list:\n",
    "        if cv_result['val_r2_pred'] > opt_val_r2_pred:\n",
    "            opt_val_r2_pred = cv_result['val_r2_pred']\n",
    "            opt_cv_result = cv_result\n",
    "    opt_hps_dict = dict(itertools.islice(opt_cv_result.items(), len(HP_GRID)+1))\n",
    "    opt_hps_dict['epochs'] = int(opt_hps_dict['avg_epochs_trained'])\n",
    "    opt_hps_dict['early_stopping'] = False\n",
    "    del opt_hps_dict['avg_epochs_trained']\n",
    "\n",
    "    # this was the best in first half 2022\n",
    "    opt_hps_dict = {'number_factors': 1,\n",
    "        'num_hidden_layers': 1,\n",
    "        'learning_rates': 5e-5, \n",
    "        'batch_sizes': 2048,\n",
    "        'l1_penalties': 1e-4,\n",
    "        'num_ensemble': 10,\n",
    "        'early_stopping': False,\n",
    "        'epochs': 38}\n",
    "\n",
    "    # predict in test period with the optimal model\n",
    "    test_y_yhats_df = predictTestPeriod(all_df, asset_universe_dict, TEST_START_DATE, LHS_COL, opt_hps_dict)\n",
    "    test_y_yhats_df.to_pickle(TEST_OUT_FP)\n",
    "\n",
    "    # TODO ALSO NEED TO TRY A MODEL WHERE WE SET MISSING ASSETS LHS \n",
    "    # AND RHS TO -2 AND THEN WE HAVE BALANCED PANEL SO WE FIT ON THE \n",
    "    # MATRIX OF RHS ACROSS ASSETS AS OPPOSED OT INDIVIDUAL DATE-ASSET \n",
    "    # AND THEN THE NETWORK LEARNS ASSET-SPECIFIC PARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9255c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "\n",
    "# Autoencoder parameter count is 2e2 to 3e3.\n",
    "\n",
    "# Transaction costs: 6.6 bps per hour (i.e. turnover (e.g. 16%) on 41 bps).\n",
    "# -If ventiles, then we are turning over about 16% based on autoencoder results across 2h 2021 thru 2022.\n",
    "# -Trading $2k per asset per hour then this is $64k an hour or $46MM per month total volume.\n",
    "# -Taker fees at that level for $COIN are 16 bps, which on 16% of the portfolio is 2.6 bps.\n",
    "# -Bid ask spread is something like 50 bps so 25 bps on 16% is 4 bps. \n",
    "# -Makes 6.6 bps total per hour.\n",
    "# -Could add leverage at 2 bps to open and .5 bps every hour so call it 2.5 bps per position per hour, \n",
    "#      which would make the results highly tasty.\n",
    "\n",
    "# Benchmark linear regression in 2021 2h: 5 bps r2pred; 10 bps tertile spread.\n",
    "# Benchmark single factor: 15 bps.\n",
    "\n",
    "# For positive r2pred, need:\n",
    "# -in 2021 1h: val mse's 2e-4 to 5e-4\n",
    "\n",
    "# For boot strap stat sig:\n",
    "# -need geom return above 1 bps, easy.\n",
    "# -need mse improvement over zero to be 2e-3. seems basically impossible given noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VENTILE RESULTS:\n",
    "\n",
    "# -2022 2h:\n",
    "# -- -1.47\n",
    "# --8.2 bps over a 6.6 bps cost\n",
    "# --16.8 sharpe after taking out 6.6 bps cost\n",
    "\n",
    "# money printer go brrr?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f14a76dcff07d5b93f2c0fc65ce65c8ac7788ddb1ef5c63daa3feefa016fa519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
