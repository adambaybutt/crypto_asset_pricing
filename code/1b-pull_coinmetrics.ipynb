{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "60883846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from helper_functions import Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "61e46fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullExchangeInfo(base_url: str, base_params: dict, target_exchanges: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing information about cryptocurrency exchanges.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): A string representing the base URL of the Coinmetrics service.\n",
    "        base_params (dict): A dictionary representing the parameters to be sent with the API request.\n",
    "        target_exchanges (List[str]): A list of strings with the target exchanges for this study.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame containing information about cryptocurrency exchanges.\n",
    "    \"\"\"\n",
    "    # Build target URL and headers\n",
    "    target_url = \"catalog-all/exchanges\"\n",
    "    url        = f\"{base_url}{target_url}\"\n",
    "    params = base_params.copy()\n",
    "\n",
    "    # Call API and convert to DataFrame\n",
    "    response_json = Helper.makeApiCall(url, headers={}, params=params)\n",
    "    df = pd.DataFrame(response_json['data'])\n",
    "\n",
    "    # Subset to relevant exchanges\n",
    "    df = df[df.exchange.isin(target_exchanges)].reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "c32dc043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullAssetInfo(base_url: str, base_params: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing information about cryptocurrency assets.\n",
    "\n",
    "    Args:\n",
    "        base_url: A string representing the base URL of the Coinmetrics service.\n",
    "        base_params: A dictionary representing the parameters to be sent with the API request.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame containing information about cryptocurrency assets.\n",
    "    \"\"\"\n",
    "    # Build target URL and headers\n",
    "    target_url = \"catalog-all/assets\"\n",
    "    url        = f\"{base_url}{target_url}\"\n",
    "    params = base_params.copy()\n",
    "\n",
    "    # Call API and convert to DataFrame\n",
    "    response_json = Helper.makeApiCall(url, headers={}, params=params)\n",
    "    df = pd.DataFrame(response_json['data'])\n",
    "\n",
    "    # Subset to assets with trading data\n",
    "    df = df[~df.markets.isnull()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "5af626ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullAndFormRelevantMarkets(exchanges_df: pd.DataFrame, assets_df: pd.DataFrame,\n",
    "        base_url: str, base_params: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a pandas DataFrame containing information about coinmetrics markets that are on a target exchange with\n",
    "    USD or stablecoin quote asset.\n",
    "\n",
    "    Args:\n",
    "    - exchanges_df (pd.DataFrame): a DataFrame containing exchange data\n",
    "    - assets_df (pd.DataFrame): a DataFrame containing asset data\n",
    "    - base_url (str): a string containing the base url for the API\n",
    "    - base_params (Dict[str, str]): a dictionary containing the base parameters for the API\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: a pandas DataFrame containing information about relevant markets\n",
    "    \"\"\"\n",
    "    # form dataframe of all markets\n",
    "    markets_list = []\n",
    "    for markets in list(exchanges_df.markets.values):\n",
    "        markets_list.extend(markets)\n",
    "    df = pd.DataFrame(data={'market': markets_list})\n",
    "\n",
    "    # remove duplicates\n",
    "    df = df.drop_duplicates(subset='market')\n",
    "\n",
    "    # form market info\n",
    "    df['exchange'] = df['market'].str.split('-', n=4, expand=True)[0]\n",
    "    df['asset'] = df['market'].str.split('-', n=4, expand=True)[1]\n",
    "    df['quote'] = df['market'].str.split('-', n=4, expand=True)[2]\n",
    "    df['type'] = df['market'].str.split('-', n=4, expand=True)[3]\n",
    "\n",
    "    # subset to spot markets\n",
    "    df = df[df.type == 'spot']\n",
    "    df = df.drop(columns='type', axis=1)\n",
    "\n",
    "    # subset to quote asset is USD, USDC, or USDT\n",
    "    df = df[df.quote.isin(['usd', 'usdt', 'usdc'])]\n",
    "\n",
    "    # remove assets that are derivatives of other symbols or stablecoins\n",
    "    assets_to_remove = ['wbtc', 'wluna', 'wnxm', 'tbtc', 'cusd', 'musd', 'nusd', 'dai', 'busd', \n",
    "                        'cusdt', 'gusd', 'lusd', 'ousd', 'usdj', 'usdk', 'usdn', 'usdt', 'usdc', \n",
    "                        'aoa', 'ausd', 'ern', 'krw', 'mtl', 'tusd', 'susd', 'usdd', 'ust', 'ustc', \n",
    "                        'eur', 'aud', 'gbp', 'cad', 'cbeth', 'lbp', 'sos', 'usdp', '00', 'bifi_beef', \n",
    "                        'bifi_bifr', 'btcauction', 'cix100']\n",
    "    df = df[~df.asset.isin(assets_to_remove)]\n",
    "    df = df[~df['asset'].str.contains('3l|3s|2s|2l')]\n",
    "\n",
    "    # remove assets if they have no coinmetrics metrics\n",
    "    df = df[df.asset.isin(list(assets_df[~assets_df.metrics.isnull()].asset.values))]\n",
    "\n",
    "    # build target url and headers for call for market meta data\n",
    "    target_url = \"catalog-all/markets\"\n",
    "    url        = f\"{base_url}{target_url}\"\n",
    "    params = base_params.copy()\n",
    "\n",
    "    # call API and convert to DataFrame\n",
    "    response_json = Helper.makeApiCall(url, headers={}, params=params)\n",
    "    markets_df = pd.DataFrame(response_json['data'])\n",
    "\n",
    "    # subset to markets and columnns of interest\n",
    "    markets_df = markets_df[markets_df.market.isin(df.market.values)].reset_index(drop=True)\n",
    "    markets_df = markets_df[['market', 'exchange', 'base', 'quote', 'quotes']]\n",
    "    markets_df = markets_df.drop('quotes', axis=1).join(pd.json_normalize(markets_df.quotes))\n",
    "\n",
    "    # drop markets without quote data\n",
    "    markets_df = markets_df[~markets_df.min_time.isnull()]\n",
    "\n",
    "    return markets_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "e1535f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullUSDTandUSDCexchangeRates(base_url: str, base_params: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing Coinmetrics reference exchange rates for USDT and USDC.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): Base URL for the API.\n",
    "        base_params (Dict[str, str]): Base parameters for the API.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing USDT and USDC price timeserieses.\n",
    "    \"\"\"\n",
    "    # Define API parameters\n",
    "    api_params = {\n",
    "        'page_size': 10000,\n",
    "        'metrics': 'ReferenceRateUSD',\n",
    "        'assets': 'usdt,usdc',\n",
    "        'frequency': '1d',\n",
    "        'limit_per_asset': 5000\n",
    "    }\n",
    "\n",
    "    # Merge base parameters with API parameters\n",
    "    params = {**base_params, **api_params}\n",
    "    \n",
    "    # Build API URL\n",
    "    api_endpoint = 'timeseries/asset-metrics'\n",
    "    url = f\"{base_url}{api_endpoint}\"\n",
    "\n",
    "    # Call API and convert response to DataFrame\n",
    "    response_json = Helper.makeApiCall(url, headers={}, params=params)\n",
    "    df = pd.DataFrame(response_json['data'])\n",
    "\n",
    "    # Clean the data\n",
    "    df['date'] = pd.to_datetime(df.time, format='%Y-%m-%d').dt.date\n",
    "    df['price'] = df.ReferenceRateUSD.astype(float)\n",
    "    df = df.sort_values('date', ignore_index=True)\n",
    "\n",
    "    # Ensure that all prices are within expected range\n",
    "    assert 0 == df[(df.price > 2) | (df.price < 0.8)].shape[0]\n",
    "\n",
    "    # Split data into USDC and USDT DataFrames\n",
    "    usdc_df = df[df.asset == 'usdc'][['date', 'price']]\n",
    "    usdc_df = usdc_df.rename(columns={'price': 'usd_per_usdc'})\n",
    "    usdt_df = df[df.asset == 'usdt'][['date', 'price']]\n",
    "    usdt_df = usdt_df.rename(columns={'price': 'usd_per_usdt'})\n",
    "\n",
    "    # Ensure that the DataFrames contain consecutive dates\n",
    "    expected_dates_usdc = pd.Series(pd.date_range(usdc_df['date'].iloc[0], usdc_df['date'].iloc[-1]))\n",
    "    expected_dates_usdt = pd.Series(pd.date_range(usdt_df['date'].iloc[0], usdt_df['date'].iloc[-1]))\n",
    "    assert usdc_df.shape[0] == (expected_dates_usdc.values == pd.to_datetime(usdc_df['date'])).sum()\n",
    "    assert usdt_df.shape[0] == (expected_dates_usdt.values == pd.to_datetime(usdt_df['date'])).sum()\n",
    "    \n",
    "    # Merge the DataFrames\n",
    "    df = usdc_df.merge(usdt_df, on='date', how='outer', validate='one_to_one')\n",
    "    df = df.sort_values(by='date', ignore_index=True)\n",
    "    df['date'] = pd.to_datetime(df.date).dt.date\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "7facdc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullOHLCV(base_url: str, base_params: Dict[str, str], \n",
    "              markets_df: pd.DataFrame, usd_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a panel DataFrame containing market prices, volumes, and trade counts.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): Base URL for the API.\n",
    "        base_params (Dict[str, str]): Base parameters for the API.\n",
    "        markets_df (pd.DataFrame): A pandas DataFrame containing information about relevant markets.\n",
    "        usd_df (pd.DataFrame): A pandas DataFrame containing USDT and USDC price timeserieses.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame panel of dates and markets with their usd_per_token prices, usd_volume_per_24h, and trades.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up object to store data\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Form list of markets\n",
    "    assert markets_df.market.is_unique\n",
    "    markets_list = list(markets_df.market.values)\n",
    "\n",
    "    # Define API parameters\n",
    "    api_params = {\n",
    "        'frequency': '1d',\n",
    "        'start_time': '2015-01-01',\n",
    "        'end_time': '2023-02-02',\n",
    "        'page_size': 10000,\n",
    "        'limit_per_market': 5000\n",
    "    }\n",
    "    \n",
    "    # Build API URL\n",
    "    api_endpoint = \"timeseries/market-candles\"\n",
    "    url = f\"{base_url}{api_endpoint}\"\n",
    "\n",
    "    # Pull all markets\n",
    "    for i in range(len(markets_list)):\n",
    "        # update market to pull\n",
    "        market = markets_list[i]\n",
    "        params = {**base_params, **api_params, 'markets': market}\n",
    "\n",
    "        # monitor progress\n",
    "        print(f\"Processing market #{i+1} ({(i+1)/len(markets_list)*100:.2f}%): {market}\")\n",
    "\n",
    "        # call API and convert response to DataFrame\n",
    "        response_json = Helper.makeApiCall(url, headers={}, params=params)\n",
    "\n",
    "        # catch if there is no data\n",
    "        try:\n",
    "            # clean the data\n",
    "            result_df = pd.DataFrame(response_json['data'])\n",
    "            result_df = result_df[['market', 'time', 'price_close', 'candle_usd_volume', 'candle_trades_count']]\n",
    "\n",
    "            # save\n",
    "            df = pd.concat((df, result_df))\n",
    "        except:\n",
    "            print(f\"{market} did not have data\")\n",
    "            continue\n",
    "\n",
    "    # Confirm no missing obs\n",
    "    assert 0==df.isnull().sum().sum()\n",
    "\n",
    "    # Add market meta data for exchange, base asset, and quote asset\n",
    "    df = df.merge(markets_df, on='market', how='inner', validate='many_to_one')\n",
    "\n",
    "    # Form date column\n",
    "    df['date'] = pd.to_datetime(df.time, format='%Y-%m-%d').dt.date\n",
    "    df = df.drop(columns='time', axis=1)\n",
    "\n",
    "    # Merge on USDT and USDC prices\n",
    "    min_usdt_date = np.min(df[df.quote=='usdt'].date)\n",
    "    min_usdc_date = np.min(df[df.quote=='usdc'].date)\n",
    "    assert min_usdt_date >= np.min(usd_df[~usd_df.usd_per_usdt.isnull()].date)\n",
    "    assert min_usdc_date >= np.min(usd_df[~usd_df.usd_per_usdc.isnull()].date)\n",
    "    df = df.merge(usd_df, on='date', how='left', validate='many_to_one')\n",
    "\n",
    "    # Form price column\n",
    "    df['price_close'] = df.price_close.astype(float)\n",
    "    df.loc[df.quote=='usd', 'usd_per_token_cm'] = df.loc[df.quote=='usd', 'price_close']\n",
    "    df.loc[df.quote=='usdc', 'usd_per_token_cm'] = df.loc[df.quote=='usdc', 'price_close']*df.loc[df.quote=='usdc', 'usd_per_usdc']\n",
    "    df.loc[df.quote=='usdt', 'usd_per_token_cm'] = df.loc[df.quote=='usdt', 'price_close']*df.loc[df.quote=='usdt', 'usd_per_usdt']\n",
    "    assert 0 == df.usd_per_token_cm.isnull().sum()\n",
    "\n",
    "    # Form volume columns\n",
    "    df['usd_volume_per_24h_cm'] = df.candle_usd_volume.astype(float)\n",
    "    df['trades_cm'] = df.candle_trades_count.astype(int)\n",
    "    assert 0 == df.usd_volume_per_24h_cm.isnull().sum()\n",
    "\n",
    "    # collapse to the asset date level\n",
    "    df.loc[df.usd_volume_per_24h_cm==0, 'usd_volume_per_24h_cm'] = 1\n",
    "    grouped = df.groupby(['date', 'base'])\n",
    "    weighted_avg = grouped.apply(lambda x: (x['usd_per_token_cm'] * x['usd_volume_per_24h_cm']).sum() / x['usd_volume_per_24h_cm'].sum())\n",
    "    total_volume = grouped['usd_volume_per_24h_cm'].sum()\n",
    "    total_trades = grouped['trades_cm'].sum()\n",
    "    df = pd.DataFrame({'usd_per_token_cm': weighted_avg, \n",
    "                    'usd_volume_per_24h_cm': total_volume, \n",
    "                    'trades_cm': total_trades}).reset_index()\n",
    "    df.loc[df.usd_volume_per_24h_cm==1, 'usd_volume_per_24h_cm'] = 0\n",
    "\n",
    "    # Check for valid ranges and dtypes\n",
    "    assert 0 == df.usd_per_token_cm.isnull().sum()\n",
    "    assert 0 == df.usd_volume_per_24h_cm.isnull().sum()\n",
    "    df = df[(df['usd_per_token_cm'] >= 0) & (df['usd_per_token_cm'] < 1e9)]\n",
    "    df = df[(df['usd_volume_per_24h_cm'] >= 0) & (df['usd_volume_per_24h_cm'] < 1e11)]\n",
    "    df = df[(df['trades_cm'] >= 0) & (df['trades_cm'] < 1e9)]\n",
    "\n",
    "    # Ensure dtypes are set\n",
    "    df['usd_per_token_cm'] = df['usd_per_token_cm'].astype('float32')\n",
    "    df['usd_volume_per_24h_cm'] = df['usd_volume_per_24h_cm'].astype('float32')\n",
    "    df['trades_cm'] = df['trades_cm'].astype('float32')\n",
    "\n",
    "    # ensure panel is sorted\n",
    "    df = df.rename(columns={'base': 'asset'})\n",
    "    df = df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "    # Initial a final dataframe to return\n",
    "    final_df = pd.DataFrame(data={'date': [], 'asset': [], 'usd_per_token_cm': [], \n",
    "                                'usd_volume_per_24h_cm': [], 'trades_cm': []})\n",
    "\n",
    "    # Loop over all assets to add any missing days\n",
    "    assets = list(np.unique(df.asset.values))\n",
    "    for asset in assets:\n",
    "        # subset to asset of interest\n",
    "        asset_df = df[df.asset==asset].copy()\n",
    "\n",
    "        # determine the date gaps\n",
    "        date_gaps = []\n",
    "        dates = asset_df.date.values\n",
    "        for i in range(1, len(dates)):\n",
    "            date_gaps.append(np.timedelta64(dates[i]-dates[i-1], 'D').astype(int))\n",
    "\n",
    "        # determine new days to add\n",
    "        indices_to_expand = [i for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32)]\n",
    "        num_days_to_add = [date_gaps[i] for i in range(len(date_gaps)) if (date_gaps[i] > 1) & (date_gaps[i] < 32)]\n",
    "        start_days = dates[indices_to_expand]\n",
    "        new_days = []\n",
    "        for i in range(len(start_days)):\n",
    "            start_day = start_days[i]\n",
    "            days_to_add = num_days_to_add[i]\n",
    "            for j in range(1, days_to_add):\n",
    "                new_days.append(start_day+np.timedelta64(24*(j), 'h'))\n",
    "\n",
    "        # add the new days to the asset df\n",
    "        new_asset_df = pd.DataFrame(data={'date': new_days})\n",
    "        new_asset_df['asset'] = asset\n",
    "        asset_df = pd.concat((asset_df, new_asset_df))\n",
    "        asset_df = asset_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # forward fill the price column\n",
    "        asset_df['usd_per_token_cm'] = asset_df.usd_per_token_cm.ffill()\n",
    "\n",
    "        # replace volume and trades with zeros\n",
    "        asset_df.loc[asset_df.usd_volume_per_24h_cm.isnull(), 'usd_volume_per_24h_cm'] = 0\n",
    "        asset_df.loc[asset_df.trades_cm.isnull(), 'trades_cm'] = 0\n",
    "\n",
    "        # add data to master df\n",
    "        final_df = pd.concat((final_df, asset_df))\n",
    "\n",
    "    # Final clean\n",
    "    df = final_df.copy()\n",
    "    df = df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "    assert not df.duplicated(subset=['date', 'asset']).any()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "cce88aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullAssetMetrics(base_url: str, base_params: Dict[str, str], \n",
    "                     asset_universe: List[str], target_asset_metrics: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pulls asset metrics for assets in the given asset universe using the given base URL and parameters.\n",
    "\n",
    "    Args:\n",
    "    - base_url (str): Base URL for the API.\n",
    "    - base_params (Dict[str, str]): Base parameters for the API.\n",
    "    - asset_universe (List[str]): list of strings representing the assets to pull metrics for.\n",
    "    - target_asset_metrics (List[str]): list of strings of asset metrics of interest.\n",
    "\n",
    "    Returns:\n",
    "    - results_df: Pandas DataFrame containing the asset metrics as panel data.\n",
    "    \"\"\"\n",
    "    # Initialize DataFrame to return results\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    # Define API parameters\n",
    "    api_params = {\n",
    "        'start_time': '2015-01-01',\n",
    "        'end_time': '2023-02-02',\n",
    "        'page_size': 10000,\n",
    "        'limit_per_asset': 10000\n",
    "    }\n",
    "\n",
    "    # Build API URL\n",
    "    api_endpoint = \"timeseries/asset-metrics\"\n",
    "    url = f\"{base_url}{api_endpoint}\"\n",
    "\n",
    "    # Loop over every asset\n",
    "    for i in range(len(asset_universe)):\n",
    "        # update asset\n",
    "        asset = asset_universe[i]\n",
    "\n",
    "        # monitor progress\n",
    "        print(f\"Processing the {i+1}th asset ({(i+1)/len(asset_universe)*100:.2f}%): {asset}\")\n",
    "\n",
    "        # initialize object for this asset results\n",
    "        asset_results_df = pd.DataFrame(data={'asset': [], 'time': []})\n",
    "\n",
    "        # update params for this asset\n",
    "        params = {**base_params, **api_params, 'assets': asset}\n",
    "\n",
    "        # determine metrics to pull\n",
    "        metrics = assets_df[assets_df.asset==asset].metrics.values[0]\n",
    "        if type(metrics) is not list:\n",
    "            continue\n",
    "        metrics_df = pd.DataFrame(metrics)\n",
    "        metrics_df = metrics_df[metrics_df.metric.isin(target_asset_metrics)]\n",
    "\n",
    "        # pull data for each metric\n",
    "        assert metrics_df.metric.is_unique\n",
    "        for metric in list(metrics_df.metric.values):\n",
    "            # form dataframe of different freq options for this metric\n",
    "            metric_options_df = pd.DataFrame(metrics_df[metrics_df.metric==metric].frequencies.values[0])\n",
    "\n",
    "            # set frequency\n",
    "            if '1d' in list(metric_options_df.frequency.values):\n",
    "                params['frequency'] = '1d'\n",
    "            else:\n",
    "                print(metric_options_df)\n",
    "                print(f\"The metric {metric} for asset {asset} does not have a 1d frequency option.\")\n",
    "                continue\n",
    "\n",
    "            # make the API call\n",
    "            params['metrics'] = metric\n",
    "            response_json = Helper.makeApiCall(url, headers={}, params=params)\n",
    "            try:\n",
    "                asset_df = pd.DataFrame(response_json['data'])\n",
    "            except:\n",
    "                print(f'The metric {metric} was not available.')\n",
    "\n",
    "            # add data to asset results data frame\n",
    "            asset_results_df = asset_results_df.merge(asset_df, on=['asset', 'time'], how='outer', validate='one_to_one')\n",
    "\n",
    "        # add this data to results df\n",
    "        results_df = pd.concat((results_df, asset_results_df))\n",
    "\n",
    "    # form date column\n",
    "    results_df['date'] = pd.to_datetime(results_df.time, format='%Y-%m-%d')\n",
    "    results_df = results_df.drop(columns='time', axis=1)\n",
    "\n",
    "    # convert other columns to float32\n",
    "    columns = list(results_df.columns.values)\n",
    "    columns.remove('asset')\n",
    "    columns.remove('date')\n",
    "    for col in columns:\n",
    "        results_df[col] = results_df[col].astype('float32')\n",
    "\n",
    "    # ensure not duplicated\n",
    "    assert not results_df.duplicated(subset=['date', 'asset']).any()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6c14804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    CM_API_FP = '../../admin/coinmetrics.txt'\n",
    "    BASE_URL = 'https://api.coinmetrics.io/v4/'\n",
    "    PANEL_FP = '../data/raw/coinmetrics_panel_initial.pkl'\n",
    "    TARGET_US_EXCHANGES = ['binance.us',  'bitstamp', 'coinbase', 'crypto.com', 'ftx.us', \n",
    "        'gemini', 'kraken', 'kucoin']\n",
    "    MCAP_METRICS = ['SplyAct1yr', 'SplyActEver', 'SplyCur', 'SplyFF', \n",
    "                      'CapMrktCurUSD', 'CapMrktEstUSD', 'CapMrktFFUSD', 'CapRealUSD']\n",
    "    \n",
    "    # import api key\n",
    "    with open(CM_API_FP) as f:\n",
    "        API_KEY = f.readlines()\n",
    "        API_KEY = API_KEY[0].strip()\n",
    "    BASE_PARAMS = {'api_key': API_KEY}\n",
    "\n",
    "    # pull meta data on target exchanges\n",
    "    exchanges_df = pullExchangeInfo(BASE_URL, BASE_PARAMS, TARGET_US_EXCHANGES)\n",
    "\n",
    "    # pull meta data on coinmetrics assets\n",
    "    assets_df = pullAssetInfo(BASE_URL, BASE_PARAMS)\n",
    "\n",
    "    # pull meta data markets and subset down to target markets\n",
    "    markets_df = pullAndFormRelevantMarkets(exchanges_df, assets_df, BASE_URL, BASE_PARAMS)\n",
    "\n",
    "    # pull usdt and usdc exchange rates\n",
    "    usd_df = pullUSDTandUSDCexchangeRates(BASE_URL, BASE_PARAMS)\n",
    "\n",
    "    # pull ohlcv data\n",
    "    panel_df = pullOHLCV(BASE_URL, BASE_PARAMS, markets_df, usd_df)\n",
    "\n",
    "    # pull mcap data\n",
    "    asset_universe = list(np.unique(panel_df.asset.values))\n",
    "    results_df = pullAssetMetrics(BASE_URL, BASE_PARAMS, asset_universe, MCAP_METRICS)\n",
    "\n",
    "    # merge and save the panel\n",
    "    panel_df = panel_df.merge(results_df, \n",
    "                          on=['asset', 'date'],\n",
    "                          how='outer',\n",
    "                          validate='one_to_one')\n",
    "    panel_df = panel_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "    panel_df.to_pickle(PANEL_FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
