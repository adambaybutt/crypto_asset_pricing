{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE WITH NEW PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "959a7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "28d653fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formFFQuintiles(test_df):\n",
    "    # Build quintiles\n",
    "    # -mcap is low to high preferable\n",
    "    # -while r_t_2 is high to low is preferable\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 3) &\n",
    "                (test_df.tertile_mcap_t_1 == 1), 'quintile'] = 5\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 3) &\n",
    "                (test_df.tertile_mcap_t_1 == 2), 'quintile'] = 4\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 2) &\n",
    "                (test_df.tertile_mcap_t_1 == 1), 'quintile'] = 4\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 3) &\n",
    "                (test_df.tertile_mcap_t_1 == 3), 'quintile'] = 3\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 2) &\n",
    "                (test_df.tertile_mcap_t_1 == 2), 'quintile'] = 3\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 1) &\n",
    "                (test_df.tertile_mcap_t_1 == 1), 'quintile'] = 3\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 2) &\n",
    "                (test_df.tertile_mcap_t_1 == 3), 'quintile'] = 2\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 1) &\n",
    "                (test_df.tertile_mcap_t_1 == 2), 'quintile'] = 2\n",
    "    test_df.loc[(test_df.tertile_r_t_2 == 1) &\n",
    "                (test_df.tertile_mcap_t_1 == 3), 'quintile'] = 1\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "08afcf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formFFPortfolioResults(quintile_df, sheet_name):\n",
    "    # Calculate value-weighted average returns for each quintile\n",
    "    quintile_df['mcap_sum']     = quintile_df.groupby(['date', 'quintile'])['mcap_t_1'].transform('sum')\n",
    "    quintile_df['weight']       = quintile_df.mcap_t_1 / quintile_df.mcap_sum\n",
    "    quintile_df['quintile_r_t'] = quintile_df.weight * quintile_df.r_t\n",
    "    quintile_df['quintile_r_t'] = quintile_df.groupby(['date', 'quintile'])['quintile_r_t'].transform('sum')\n",
    "    results_df = quintile_df[['quintile', 'quintile_r_t']].drop_duplicates()\n",
    "\n",
    "    # Form the output table\n",
    "    output_df = pd.DataFrame(data = {'quintile': [1, 2, 3, 4, 5]})\n",
    "    for quintile in [1, 2, 3, 4, 5]:\n",
    "        weekly_returns = results_df[results_df.quintile == quintile].quintile_r_t.values\n",
    "        output_df.loc[output_df.quintile == quintile, 'Real'] = np.product(weekly_returns+1)**(1/52)-1\n",
    "        output_df.loc[output_df.quintile == quintile, 'Std'] = np.std(weekly_returns)\n",
    "        output_df.loc[output_df.quintile == quintile, 'SR'] = np.sqrt(52)*np.mean(weekly_returns)/np.std(weekly_returns)\n",
    "\n",
    "    # Output to Excel without overwriting the file\n",
    "    book          = load_workbook('../4-output/portfolio_results.xlsx')\n",
    "    writer        = pd.ExcelWriter('../4-output/portfolio_results.xlsx', engine='openpyxl') \n",
    "    writer.book   = book\n",
    "    writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "    output_df.to_excel(writer, sheet_name=sheet_name)\n",
    "    writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a9fa9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formQuintiles(test_df): \n",
    "    test_df = test_df.sort_values(by=['date', 'y_hat_t'])\n",
    "    test_df['ranking'] = test_df.groupby(['date']).cumcount()+1\n",
    "    test_df['counts'] = 1\n",
    "    test_df['coins_per_week'] = test_df.groupby(['date']).counts.sum()\n",
    "    test_df['ranking'] = test_df.ranking / test_df.coins_per_week\n",
    "    test_df.loc[test_df.ranking <= 0.2, 'quintile'] = 1\n",
    "    test_df.loc[(test_df.ranking > 0.2) & (test_df.ranking <= 0.4), 'quintile'] = 2\n",
    "    test_df.loc[(test_df.ranking > 0.4) & (test_df.ranking <= 0.6), 'quintile'] = 3\n",
    "    test_df.loc[(test_df.ranking > 0.6) & (test_df.ranking <= 0.8), 'quintile'] = 4\n",
    "    test_df.loc[(test_df.ranking > 0.8), 'quintile'] = 5\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3c56ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPortfolioResults(quintile_df, sheet_name):\n",
    "    # Calculate equal-weighted average returns for each quintile\n",
    "    quintile_df['quintile_y_hat_t'] = quintile_df.groupby(['date', 'quintile'])['y_hat_t'].transform('mean')\n",
    "    quintile_df['quintile_r_t'] = quintile_df.groupby(['date', 'quintile'])['r_t'].transform('mean')\n",
    "    results_df = quintile_df[['quintile', 'quintile_y_hat_t', 'quintile_r_t']].drop_duplicates()\n",
    "\n",
    "    # Form the output table\n",
    "    output_df = pd.DataFrame(data = {'quintile': [1, 2, 3, 4, 5]})\n",
    "    for quintile in [1, 2, 3, 4, 5]:\n",
    "        pred_returns = results_df[results_df.quintile == quintile].quintile_y_hat_t.values\n",
    "        weekly_returns = results_df[results_df.quintile == quintile].quintile_r_t.values\n",
    "        output_df.loc[output_df.quintile == quintile, 'Pred'] = np.product(pred_returns+1)**(1/52)-1\n",
    "        output_df.loc[output_df.quintile == quintile, 'Real'] = np.product(weekly_returns+1)**(1/52)-1\n",
    "        output_df.loc[output_df.quintile == quintile, 'Std'] = np.std(weekly_returns)\n",
    "        output_df.loc[output_df.quintile == quintile, 'SR'] = np.sqrt(52)*np.mean(weekly_returns)/np.std(weekly_returns)\n",
    "\n",
    "    # Output to Excel without overwriting the file\n",
    "    book          = load_workbook('../4-output/portfolio_results.xlsx')\n",
    "    writer        = pd.ExcelWriter('../4-output/portfolio_results.xlsx', engine='openpyxl') \n",
    "    writer.book   = book\n",
    "    writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "    output_df.to_excel(writer, sheet_name=sheet_name)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4be58a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputBenchmarks(test_df):\n",
    "    # Calculate equal weighted return statistics\n",
    "    eql_wght_weekly_returns = test_df.groupby('date').r_t.mean().values\n",
    "    eql_wght_real = np.product(eql_wght_weekly_returns+1)**(1/52)-1\n",
    "    eql_wght_std = np.std(eql_wght_weekly_returns)\n",
    "    eql_wght_sr = np.sqrt(52)*np.mean(eql_wght_weekly_returns)/eql_wght_std\n",
    "\n",
    "    # Calculate mcap weighted return statistics\n",
    "    test_df['mcap_sum'] = test_df.groupby(['date'])['mcap_t_1'].transform('sum')\n",
    "    test_df['weight']   = test_df.mcap_t_1 / test_df.mcap_sum\n",
    "    test_df['mcap_r_t'] = test_df.weight * test_df.r_t\n",
    "    test_df['mcap_r_t'] = test_df.groupby(['date'])['mcap_r_t'].transform('sum')\n",
    "    mcap_wght_weekly_returns = test_df[['mcap_r_t']].drop_duplicates().mcap_r_t.values\n",
    "    mcap_wght_real = np.product(mcap_wght_weekly_returns+1)**(1/52)-1\n",
    "    mcap_wght_std = np.std(mcap_wght_weekly_returns)\n",
    "    mcap_wght_sr = np.sqrt(52)*np.mean(mcap_wght_weekly_returns)/mcap_wght_std\n",
    "\n",
    "    # Form output dataframe\n",
    "    output_df = pd.DataFrame(data={'weights': ['equal', 'mcap']})\n",
    "    output_df.loc[output_df.weights == 'equal', 'Real'] = eql_wght_real \n",
    "    output_df.loc[output_df.weights == 'equal', 'Std'] = eql_wght_std\n",
    "    output_df.loc[output_df.weights == 'equal', 'SR'] = eql_wght_sr\n",
    "    output_df.loc[output_df.weights == 'mcap', 'Real'] = mcap_wght_real \n",
    "    output_df.loc[output_df.weights == 'mcap', 'Std'] = mcap_wght_std\n",
    "    output_df.loc[output_df.weights == 'mcap', 'SR'] = mcap_wght_sr\n",
    "\n",
    "    # Output to Excel without overwriting the file\n",
    "    book          = load_workbook('../4-output/portfolio_results.xlsx')\n",
    "    writer        = pd.ExcelWriter('../4-output/portfolio_results.xlsx', engine='openpyxl') \n",
    "    writer.book   = book\n",
    "    writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "    output_df.to_excel(writer, sheet_name='benchmarks')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "b80ec612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FF and CA yhats\n",
    "pik_ff = '../3-data/clean/ff-rankings-returns.pkl' \n",
    "with open(pik_ff, \"rb\") as f:\n",
    "    ff_data_in = pickle.load(f)\n",
    "    \n",
    "ff_yhats_df, ff_test_df, ff_return_df = ff_data_in\n",
    "\n",
    "pik_ca = '../3-data/clean/autoencoders-yhats-returns.pkl' \n",
    "with open(pik_ca, \"rb\") as f:\n",
    "    ca_data_in = pickle.load(f)\n",
    "    \n",
    "opt_hps_list, test_dfs_list, returns_dfs_list = ca_data_in\n",
    "\n",
    "# Import other benchmarks\n",
    "\n",
    "# TODO: CMC 200\n",
    "# TODO: BTC\n",
    "# TODO: ETH\n",
    "# TODO: S&P 500\n",
    "\n",
    "# Output FF results\n",
    "test_df = ff_test_df.copy()\n",
    "quintile_df = formFFQuintiles(test_df)\n",
    "formFFPortfolioResults(quintile_df, sheet_name = 'raw_ff')\n",
    "\n",
    "# Output autoencoder results\n",
    "for i in range(len(test_dfs_list)):\n",
    "    test_df = test_dfs_list[i]\n",
    "    opt_hps = opt_hps_list[i]\n",
    "    num_hidden_layer = opt_hps['number_hidden_layer']\n",
    "    num_factor = opt_hps['number_factor']\n",
    "    sheet_name = 'raw_autoencoder-hl_' + str(num_hidden_layer) + '-fac_' + str(num_factor)\n",
    "    \n",
    "    quintile_df = formQuintiles(test_df)\n",
    "    formPortfolioResults(quintile_df, sheet_name)\n",
    "\n",
    "# Output benchmarks\n",
    "outputBenchmarks(ff_test_df)\n",
    "\n",
    "# TODO: ENSURE I REPORT THE RETURN AND SHARPE AND OTHER METRICS FOR ALL BENCHMARKS:\n",
    "# -FF, CA, CMC 200, EQUAL WEIGHTS FROM SAME UNIVERSE, MCAP WEIGHTS FROM SAME UNIVERSE, BTC, ETH, S&P 500\n",
    "\n",
    "# Report out for OOS: (maybe do some of these in a separate script?)\n",
    "# --return weighted by mcap and equal weights\n",
    "# --sharpe for both equal and mcap weights\n",
    "# --source of excess return e.g. distri of each week-asset holding return with naming top returns asset-weeks\n",
    "# --max DD\n",
    "# --fees\n",
    "# --min/Q1-Q3/max portfolio weight each week plotted\n",
    "# --number of transactions per week\n",
    "# --portfolio turnover\n",
    "# --portfolio return per month\n",
    "# --fees per week/month/overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
