{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d8a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO NOTEBOOK IMPORTING AND FORMING OF DATA NEEDS TO BE CLEANED; IT'S WAY MESSY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a21a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# Import packages\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import statsmodels.formula.api as smf\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import decomposition\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from typing import Dict, List\n",
    "from tools import QuantTools\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# set color map\n",
    "viridis = matplotlib.colormaps['viridis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9f2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importYahoo(ticker: str, start_date: str, end_date: str, rf_df: pd.DataFrame, \n",
    "            new_ret_col: str, resample_freq: str='W', rf_col: str='r_rf_tm7') -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Import Yahoo Finance data for given ticker, time period, taking our risk free rate in rf_df. \n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): The ticker symbol to get data for.\n",
    "    start_date (str): The start date of the data retrieval period.\n",
    "    end_date (str): The end date of the data retrieval period.\n",
    "    rf_df (DataFrame): A DataFrame containing risk-free rates.\n",
    "    new_ret_col (str): The new column name for returns.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the date and return data.\n",
    "\n",
    "    Note: this is done at weekly frequency; would need adjustment for different.\n",
    "    \"\"\"\n",
    "    # import the data\n",
    "    df = yf.Ticker(ticker).history(period='1d', start=start_date, end=end_date).reset_index()\n",
    "    \n",
    "    # reformat\n",
    "    df['Date'] = pd.to_datetime(df.Date).to_numpy(dtype='datetime64[D]')\n",
    "    df = df[['Date', 'Close']].rename(columns={'Date': 'date', 'Close': new_ret_col}).set_index('date')\n",
    "    df = df.resample(resample_freq).last().pct_change().dropna()\n",
    "\n",
    "    # adjust if resampling monthly\n",
    "    if resample_freq == 'M':\n",
    "        df.index = df.index + pd.Timedelta(days=1)\n",
    "\n",
    "    # take out rf rate\n",
    "    df = df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    df[new_ret_col] = df[new_ret_col] - df[rf_col]\n",
    "\n",
    "    # check for NaN values in new_ret_col\n",
    "    if df[new_ret_col].isnull().values.any():\n",
    "        print(\"Warning: NaN values found in return data\")\n",
    "    \n",
    "    return df[['date', new_ret_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8347f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniverse(df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset a DataFrame based on a dictionary of asset universes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame. Must contain columns \"date\" and \"asset\".\n",
    "    asset_universe_dict : Dict[str, List[str]]\n",
    "        A dictionary where keys are dates in 'YYYY-MM-DD' format and values are lists of asset names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The subsetted DataFrame.\n",
    "    \"\"\"\n",
    "    # Check that the required columns are present in the DataFrame\n",
    "    if not set(['date', 'asset']).issubset(df.columns):\n",
    "        raise ValueError('Input DataFrame must contain \"date\" and \"asset\" columns.')\n",
    "\n",
    "    # Ensure that the 'date' column is of datetime type\n",
    "    if df['date'].dtype != 'datetime64[ns]':\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Loop over all months with their relevant assets\n",
    "    for key, values in asset_universe_dict.items():\n",
    "        # Extract the year and month from the key\n",
    "        year, month = key.split('-')[:2]\n",
    "\n",
    "        # Drop rows from the dataframe which match the year and month but not the assets\n",
    "        df = df[~((df.date.dt.year == int(year)) \n",
    "                    & (df.date.dt.month == int(month)) \n",
    "                    & (~df.asset.isin(values)))]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9bd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotReturnHistograms(df: pd.DataFrame, out_fp: str):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame containing time series data for returns of\n",
    "    btc, eth, and the cmkt and saves a histogram plot for all three to given fp.\n",
    "    Each histogram also includes a normal distribution fit.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): A DataFrame containing columns 'date', 'asset', 'char_r_tm7', 'macro_cmkt_tm7'.\n",
    "    out_fp (str): A string specifying the filepath where the plot should be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # extract relevant returns\n",
    "    btc_df  = df[df.asset=='btc'][['date', 'char_r_tm7']]\n",
    "    btc_df  = btc_df.rename(columns={'char_r_tm7': 'btc'})\n",
    "    eth_df  = df[df.asset=='eth'][['date', 'char_r_tm7']]\n",
    "    eth_df  = eth_df.rename(columns={'char_r_tm7': 'eth'})\n",
    "    cmkt_df = df.groupby('date')[['macro_cmkt_tm7']].mean().reset_index()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'cmkt'})\n",
    "\n",
    "    # form single dataframe\n",
    "    hist_df = cmkt_df.merge(btc_df, on='date', how='inner', validate='one_to_one')\n",
    "    hist_df = hist_df.merge(eth_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    # initiate the plot with given colors and columns\n",
    "    fig, axs = plt.subplots(3, sharex=True, sharey=True, figsize=(6.4,4), facecolor='none')\n",
    "    colors = plt.get_cmap('viridis')(np.linspace(0, 10))\n",
    "    data_columns = ['cmkt', 'btc', 'eth']\n",
    "\n",
    "    # plot the data with the normal dist fit\n",
    "    for idx, ax in enumerate(axs):\n",
    "        data = hist_df[data_columns[idx]]\n",
    "        n, bins, patches = ax.hist(data, bins=30, color=colors[idx], alpha=1)\n",
    "\n",
    "        # Fit a normal distribution\n",
    "        mu, std = norm.fit(data)\n",
    "\n",
    "        # Scale normal distribution to histogram\n",
    "        scale = n.max() / norm.pdf(mu, mu, std).max()\n",
    "        \n",
    "        # Plot the PDF\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = norm.pdf(x, mu, std) * scale\n",
    "        ax.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    # tighen up the plot\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # adjust x axis labels\n",
    "    plt.xticks(np.arange(-.5, 0.7, 0.1))\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "\n",
    "    # close the figure\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15c837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCumulativeReturns(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot the time series of cumulative returns to the given output filepath.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data\n",
    "        out_fp (str): a relative filepath to save the figure to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # initialize df with timeserieses to plot\n",
    "    plot_df = pd.DataFrame(data={'date': []})\n",
    "\n",
    "    # find all assets present in the panel\n",
    "    assets = list(np.unique(df.asset.values))\n",
    "\n",
    "    # form each asset's cumulative return\n",
    "    for asset in assets:\n",
    "        # extract asset's returns\n",
    "        temp_df = df[df.asset==asset][['date', 'char_r_tm7']]\n",
    "\n",
    "        # ensure it is sorted\n",
    "        temp_df = temp_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # form cumulative return\n",
    "        temp_df[asset] = (1 + temp_df['char_r_tm7']).cumprod()\n",
    "\n",
    "        # merge on results\n",
    "        plot_df = plot_df.merge(temp_df[['date', asset]], on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # form the cmkt return\n",
    "    temp_df = df[df.asset=='btc'][['date', 'macro_cmkt_tm7']]\n",
    "    temp_df = temp_df.sort_values(by='date', ignore_index=True)\n",
    "    temp_df['cmkt'] = (1 + temp_df['macro_cmkt_tm7']).cumprod()\n",
    "    plot_df = plot_df.merge(temp_df[['date', 'cmkt']], on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # resort\n",
    "    plot_df = plot_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # set index\n",
    "    plot_df.set_index('date', inplace=True)\n",
    "\n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(4*1.61, 4), facecolor='none')\n",
    "\n",
    "    # Form column list\n",
    "    columns = list(plot_df.columns)\n",
    "    columns.remove('btc')\n",
    "    columns.remove('eth')\n",
    "    columns.remove('cmkt')\n",
    "    columns = columns + ['eth', 'btc', 'cmkt']\n",
    "\n",
    "    # Iterate over the columns and plot each time series\n",
    "    for column in columns:\n",
    "        if column == 'btc':\n",
    "            color = '#FDE725FF'\n",
    "            linewidth = 2\n",
    "        elif column == 'eth':\n",
    "            color = '#2D708EFF'\n",
    "            linewidth = 2\n",
    "        elif column == 'cmkt':\n",
    "            color = '#482677FF'\n",
    "            linewidth = 2\n",
    "        else:\n",
    "            color = 'gray'\n",
    "            linewidth = 0.5\n",
    "        plt.plot(plot_df.index, plot_df[column], color=color, linewidth=linewidth)\n",
    "\n",
    "    # Set y-axis to logarithmic scale\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Remove y-axis minor ticks\n",
    "    plt.gca().yaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.grid(visible=True, which='major', axis='y', linewidth=0.5)\n",
    "    plt.box(False)\n",
    "\n",
    "    # Add custom labels for important time series\n",
    "    plt.text(plot_df.index[-45], plot_df['cmkt'].iloc[-52]+5, 'cmkt', color='#482677FF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "    plt.text(plot_df.index[-1], plot_df['btc'].iloc[-1]-0.1, 'btc', color='#FDE725FF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "    plt.text(plot_df.index[-1], plot_df['eth'].iloc[-1]+0.3, 'eth', color='#2D708EFF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac7195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSummaryStatistics(df: pd.DataFrame, lhs_col: str, out_fp: str, periods_in_year: int) -> None:\n",
    "    \"\"\"\n",
    "    Generates summary statistics for the panel and saves them to an Excel file.\n",
    "\n",
    "    :param df: DataFrame containing asset return data.\n",
    "    :param lhs_col: Column in df that contains the return data.\n",
    "    :param out_fp: Output file path for the Excel file.\n",
    "    :param periods_in_year: integer number of observations in a calendar year.\n",
    "    \"\"\"\n",
    "    # define function for calculating return statistics\n",
    "    def calcReturnStats(temp_df: pd.DataFrame, asset: str, return_col: str, periods_in_year: int) -> dict:\n",
    "        mean_return = QuantTools.calcTSAvgReturn(temp_df[return_col].values, annualized=True, periods_in_year=periods_in_year)\n",
    "        std_dev = QuantTools.calcSD(temp_df[return_col].values, annualized=True, periods_in_year=periods_in_year)\n",
    "        sharpe_ratio = QuantTools.calcSharpe(temp_df[return_col].values, periods_in_year=periods_in_year)\n",
    "        skewness = stats.skew(temp_df[return_col].values) / np.sqrt(52)\n",
    "        kurtosis = stats.kurtosis(temp_df[return_col].values) / 52\n",
    "        perc_return_above_zero = np.sum(temp_df[return_col]>0) / len(temp_df)\n",
    "        \n",
    "        return {'asset': asset,\n",
    "            'Mean': mean_return,\n",
    "            'SD': std_dev,\n",
    "            'Sharpe': sharpe_ratio,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'Pct pos': perc_return_above_zero}\n",
    "\n",
    "    # drop to only necessary columns\n",
    "    df = df[['date', 'asset', lhs_col, 'char_size_t', 'char_volume_sum_tm7', \n",
    "        'macro_snp500_t', 'macro_dgs1mo_t']].copy()\n",
    "\n",
    "    # form btc and eth returns\n",
    "    btc_df = df[df.asset=='btc'].set_index('date')[[lhs_col]]\n",
    "    eth_df = df[df.asset=='eth'].set_index('date')[[lhs_col]]\n",
    "\n",
    "    # form cmkt return\n",
    "    df['weighted_return'] = df[lhs_col] * df['char_size_t']\n",
    "    total_market_cap = df.groupby('date')['char_size_t'].sum()\n",
    "    cmkt_df = df.groupby('date')['weighted_return'].sum() / total_market_cap\n",
    "    cmkt_df = pd.DataFrame(cmkt_df).rename(columns={0: 'return'})\n",
    "\n",
    "    # import nasdaq data and take out risk free rate\n",
    "    rf_df = df[['date', 'macro_dgs1mo_t']].drop_duplicates()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.macro_dgs1mo_t.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    nsdq_df = importYahoo('^IXIC', '2017-12-29', '2022-12-31', rf_df, 'r_nsdq_tm7')    \n",
    "\n",
    "    # calc return statistics\n",
    "    cmkt_stats = calcReturnStats(cmkt_df, 'CMKT', 'return', periods_in_year)\n",
    "    btc_stats  = calcReturnStats(btc_df, 'Bitcoin', lhs_col, periods_in_year)\n",
    "    eth_stats  = calcReturnStats(eth_df, 'Ethereum', lhs_col, periods_in_year)\n",
    "    nsdq_stats = calcReturnStats(nsdq_df, 'Nasdaq', 'r_nsdq_tm7', periods_in_year)\n",
    "    ret_df = pd.DataFrame([cmkt_stats, btc_stats, eth_stats, nsdq_stats])\n",
    "    \n",
    "    # calc extreme event statistics\n",
    "    ext_data = {'threshold': [], 'count': [], 'percent': []}\n",
    "    num_obs  = len(cmkt_df)\n",
    "    for threshold in [-.3, -.2, -.1, -.05, .05, .1, .2, .3]:\n",
    "        ext_data['threshold'].append(threshold)\n",
    "        if threshold < 0:\n",
    "            count = (cmkt_df['return'] < threshold).sum()\n",
    "            ext_data['count'].append(count)\n",
    "            ext_data['percent'].append(count / num_obs)\n",
    "        else:\n",
    "            count = (cmkt_df['return'] > threshold).sum()\n",
    "            ext_data['count'].append(count)\n",
    "            ext_data['percent'].append(count / num_obs)\n",
    "    ext_df = pd.DataFrame(ext_data)\n",
    "    \n",
    "    # calculate yearly stats of unique assets and median mcap and volume\n",
    "    df['year'] = df['date'].dt.year\n",
    "    yr_df = pd.DataFrame({\n",
    "        'num_unique_assets': df.groupby(['year'])['asset'].nunique(),\n",
    "        'median_market_cap': df.groupby(['year'])['char_size_t'].median(),\n",
    "        'median_weekly_asset_volume': df.groupby(['year'])['char_volume_sum_tm7'].median()}).reset_index()\n",
    "    all_df = pd.DataFrame({\n",
    "        'num_unique_assets': [df['asset'].nunique()],\n",
    "        'median_market_cap': [df['char_size_t'].median()],\n",
    "        'median_weekly_asset_volume': [df['char_volume_sum_tm7'].median()]})\n",
    "    all_df['year'] = 'all'\n",
    "    yr_df = pd.concat([yr_df, all_df])\n",
    "\n",
    "    # calculate the total mcap in the last week of each year\n",
    "    max_dates = df.groupby('year')['date'].max()\n",
    "    filtered_df = df[df['date'].isin(max_dates)]\n",
    "    total_mcap_by_year = filtered_df.groupby('year')[['char_size_t']].sum().reset_index()\n",
    "    yr_df = yr_df.merge(total_mcap_by_year, on='year', how='outer', validate='one_to_one')\n",
    "\n",
    "    # extract yearly returns\n",
    "    cmkt_df = cmkt_df.reset_index()\n",
    "    cmkt_df['year'] = cmkt_df.date.dt.year\n",
    "    for year in [2018, 2019, 2020, 2021, 2022]:\n",
    "        yr_df.loc[yr_df.year==year, 'cmkt_ret'] = ((cmkt_df[cmkt_df.year==year]['return']+1).cumprod()-1).values[-1]\n",
    "    yr_df.loc[yr_df.year=='all', 'cmkt_ret'] = ((cmkt_df['return']+1).cumprod()-1).values[-1]\n",
    "    \n",
    "    # save results\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        ret_df.to_excel(writer, sheet_name='raw_ret_stats')\n",
    "        ext_df.to_excel(writer, sheet_name='raw_extreme_stats')\n",
    "        yr_df.to_excel(writer, sheet_name='raw_yearly_stats')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb9e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRollingSharpe(out_fp: str) -> None:\n",
    "    \"\"\" Plot rolling four year sharpe ratio with new data for the study period. \"\"\"\n",
    "    \n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-31').reset_index()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf_tm7'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "\n",
    "    # import other asset class data\n",
    "    start_date = '2013-12-29'\n",
    "    end_date   = '2022-12-31'\n",
    "    nsdq_df = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq')\n",
    "    snp_df  = importYahoo('^GSPC', start_date, end_date, rf_df, 'SnP 500')\n",
    "    vt_df   = importYahoo('VT', start_date, end_date, rf_df, 'Global Stocks')\n",
    "    bnd_df  = importYahoo('BND', start_date, end_date, rf_df, 'US Bonds')\n",
    "    real_df = importYahoo('VNQ', start_date, end_date, rf_df, 'US Real Estate')\n",
    "    emrg_df = importYahoo('EBND', start_date, end_date, rf_df, 'Emerging Currencies')\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold')\n",
    "\n",
    "    # import the btc data and form weekly excess returns\n",
    "    btc_df = pd.read_csv('../data/raw/XBTUSD_1440.csv',\n",
    "                        header=None)\n",
    "    btc_df[0] = pd.to_datetime(btc_df[0], unit='s')\n",
    "    btc_df = btc_df[[0, 4]]\n",
    "    btc_df = btc_df.rename(columns={0: 'date', 4: 'price'})\n",
    "    btc_df.set_index('date', inplace=True)\n",
    "    btc_df = btc_df.resample('W').last()\n",
    "    btc_df['Bitcoin'] = btc_df['price'].pct_change()\n",
    "    btc_df = btc_df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'] - btc_df.r_rf_tm7\n",
    "    btc_df = btc_df.drop(['price', 'r_rf_tm7'], axis=1)\n",
    "\n",
    "    # form single dataframe\n",
    "    df = btc_df.merge(nsdq_df, on='date', how='inner', validate='one_to_one')\n",
    "    for temp_df in [snp_df, vt_df, bnd_df, real_df, emrg_df, gld_df]:\n",
    "        df = df.merge(temp_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    # form sharpe ratios\n",
    "    window_size = 208\n",
    "    df.set_index('date', inplace=True)\n",
    "    columns = list(df.columns.values)\n",
    "    for col in columns:\n",
    "        df[col] = np.sqrt(52) * df[col].rolling(window_size).mean() / df[col].rolling(window_size).std()\n",
    "\n",
    "    # subset to relevant time period\n",
    "    df = df[(df.index.year >= 2018)\n",
    "        & (df.index.year <= 2022)]\n",
    "\n",
    "    # form sharpe ratio plot\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    df.plot(cmap='viridis')\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    #plt.title('Sharpe Ratios: Bitcoin vs Major Asset Classes$^{12}$')\n",
    "    plt.legend(labels=df.columns.values, \n",
    "                loc='lower center',\n",
    "                ncol=3,\n",
    "                bbox_to_anchor=(0.5, -0.37),\n",
    "                frameon=False)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35aee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTransactionStats(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Plot rolling transaction statistics for Bitcoin. \"\"\"\n",
    "    # Extract relevant transaction data\n",
    "    tx_df = df[['date', 'macro_btc_fee_med_usd_t', 'macro_btc_tx_tfr_val_adj_usd_t']].drop_duplicates().copy()\n",
    "\n",
    "    # reformat the data\n",
    "    tx_df.set_index('date', inplace=True)\n",
    "    temp1_df = tx_df[['macro_btc_tx_tfr_val_adj_usd_t']].resample('M').sum()\n",
    "    temp2_df = tx_df[['macro_btc_fee_med_usd_t']].resample('M').median()\n",
    "    r_df = temp1_df.merge(temp2_df, how='inner', left_index=True, right_index=True, validate='one_to_one')\n",
    "    r_df = r_df.rename(columns={'macro_btc_fee_med_usd_t': 'Median Fee (USD)',\n",
    "                                'macro_btc_tx_tfr_val_adj_usd_t': 'Monthly Volume (USD)'})\n",
    "    r_df['date'] = r_df.index\n",
    "    r_df['date'] = r_df.date.dt.strftime('%Y-%m')\n",
    "    r_df = r_df.set_index('date')\n",
    "\n",
    "    # plot the data\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    r_df.plot(color=[viridis.colors[0], viridis.colors[111]])\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    plt.legend(labels=r_df.columns.values, \n",
    "            loc='lower center',\n",
    "            ncol=3,\n",
    "            bbox_to_anchor=(0.5, -0.22),\n",
    "            frameon=False)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.xticks(np.array([0,12,24,36,48]), \n",
    "            ['2018', '2019', '2020', '2021', '2022'],\n",
    "            rotation=0, ha='center')\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a09b4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHodlingStats(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    # form the hodl data\n",
    "    utxo_df = df[['date', 'macro_btc_utxo_age_med_t']].copy()\n",
    "    utxo_df = utxo_df.drop_duplicates()\n",
    "    utxo_df = utxo_df.set_index('date')\n",
    "    utxo_df = utxo_df.rename(columns={'macro_btc_utxo_age_med_t':\n",
    "                                    'UTXO Median Age (Days)'})\n",
    "\n",
    "    # Plot the hodl data\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    utxo_df.plot(color=[viridis.colors[0]],\n",
    "                legend=None)\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518f2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genForkStatistics(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Generate event studies for BTC fork dates. \"\"\"\n",
    "    # PARAMETERS\n",
    "    num_bs_samples = int(1e6)\n",
    "    num_cpus       = 22\n",
    "    window_size    = 8\n",
    "\n",
    "    # DEFINE RELEVANT FORKS\n",
    "    forks = {'bitcoin-21': datetime.datetime(2016,4,17),\n",
    "            'zcash': datetime.datetime(2016,10,28),\n",
    "            'bitcoin-cash': datetime.datetime(2017,7,31),\n",
    "            'bitcoin-gold': datetime.datetime(2017,10,24),\n",
    "            'bitcoin-diamond': datetime.datetime(2017,11,24),\n",
    "            'lightning-bitcoin': datetime.datetime(2017,12,18),\n",
    "            'bitcoinfast': datetime.datetime(2017,12,26),\n",
    "            'bitcoin2': datetime.datetime(2017,12,28),\n",
    "            'bitcoin-plus': datetime.datetime(2018,1,2),\n",
    "            'bitcoin-interest': datetime.datetime(2018,1,22),\n",
    "            'bitcoin-atom': datetime.datetime(2018,1,24),\n",
    "            'bitcoin-private': datetime.datetime(2018,2,28),\n",
    "            'microbitcoin': datetime.datetime(2018,5,29),\n",
    "            'bitcoin-bep2': datetime.datetime(2018,6,29),\n",
    "            'bitcoin-sv': datetime.datetime(2018,11,11)}\n",
    "\n",
    "    # PULL IN OLD DATA TO BUILD RELEVANT TIMESERIES FOR BTC\n",
    "    df = pd.read_csv('../data/raw/cmc_price_vol_mcap_panel.csv')\n",
    "    df = df[df.cmc_id==1]\n",
    "    df = df[['date', 'usd_per_token', 'usd_volume_24h']]\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "    df = df.drop_duplicates(subset='date')\n",
    "    df = df.reset_index(drop=True)\n",
    "    san_df = pd.read_pickle('../data/raw/santiment_panel.pkl')\n",
    "    san_df = san_df[san_df.san_slug=='bitcoin'][['date', 'active_addresses_24h', 'github_activity',\n",
    "                                                'social_volume_total']].reset_index(drop=True)\n",
    "    hash_df = pd.read_excel('../data/raw/coinmetrics_btc_hashrate.xlsx')\n",
    "    hash_df = hash_df.rename(columns={'Time': 'date',\n",
    "                                    'BTC / Mean Hash Rate': 'hash_rate'})\n",
    "    hash_df['date'] = pd.to_datetime(hash_df['date'])\n",
    "    df = df.merge(san_df,\n",
    "                on=['date'],\n",
    "                how='left',\n",
    "                validate='one_to_one')\n",
    "    df = df.merge(hash_df,\n",
    "                on='date',\n",
    "                how='left',\n",
    "                validate='one_to_one')\n",
    "    del san_df, hash_df\n",
    "\n",
    "    # CLEAN UP THE PANEL\n",
    "\n",
    "    # ensure it has all days\n",
    "    sdate   = datetime.date(2015, 1, 1) \n",
    "    edate   = datetime.date(2021, 12, 31)  \n",
    "    delta   = edate - sdate \n",
    "    days    = []\n",
    "    for i in range(delta.days + 1):\n",
    "        days.append(sdate+timedelta(days=i))\n",
    "    days_df = pd.DataFrame(data={'date':days})\n",
    "    df['date'] = df.date.dt.date\n",
    "    df      = df.merge(days_df,\n",
    "                    on='date',\n",
    "                    how='outer',\n",
    "                    validate='one_to_one')\n",
    "    df = df.sort_values('date')\n",
    "    df = df.interpolate()\n",
    "\n",
    "    # form pct change in all columns and clean up improper values\n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    for col in cols:\n",
    "        df[col] = df[col].pct_change()\n",
    "    df = df.dropna()\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # subset to time period of interest \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df[df.date.dt.year <= 2021]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # rename columns\n",
    "    df = df.rename(columns = {'usd_per_token': 'return',\n",
    "                            'usd_volume_24h': 'usd_volume',\n",
    "                            'active_addresses_24h': 'active_addresses',\n",
    "                            'github_activity': 'developer_activity',\n",
    "                            'social_volume_total': 'social_volume'})\n",
    "\n",
    "    # INITIALIZE RESULTS \n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    results_df = pd.DataFrame(data={'stat': cols,\n",
    "                                    'window': np.repeat('7 days', len(cols)),\n",
    "                                    'est': np.zeros(len(cols)),\n",
    "                                    'se': np.zeros(len(cols))})\n",
    "\n",
    "    # CALC STAT\n",
    "    for col in cols:\n",
    "        diffs = []\n",
    "        for i in range(len(forks)):\n",
    "            fork_date = list(forks.values())[i]\n",
    "            pre_date  = fork_date-pd.Timedelta(window_size, unit=\"d\")\n",
    "            post_date = fork_date+pd.Timedelta(window_size, unit=\"d\")\n",
    "            pre_mean  = np.mean(df[(df.date >= pre_date) & (df.date < fork_date)][col])\n",
    "            post_mean = np.mean(df[(df.date > fork_date) & (df.date <= post_date)][col])\n",
    "            diff      = post_mean-pre_mean\n",
    "            diffs.append(diff)\n",
    "        results_df.loc[results_df.stat==col, 'est'] = np.mean(diffs)\n",
    "\n",
    "    # CALC STANDARD ERROR (NOTE: 6 MIN RUN TIME)\n",
    "    num_forks = len(forks)\n",
    "    rel_dates = list(df.date.values)[window_size:-window_size]\n",
    "    for col in cols:\n",
    "        # calc all diffs across the panel\n",
    "        diffs = []\n",
    "        for date in rel_dates:\n",
    "            pre_date  = date-pd.Timedelta(31, unit=\"d\")\n",
    "            post_date = date+pd.Timedelta(31, unit=\"d\")\n",
    "            pre_mean  = np.mean(df[(df.date >= pre_date) & (df.date < date)][col])\n",
    "            post_mean = np.mean(df[(df.date > date) & (df.date <= post_date)][col])\n",
    "            diff      = post_mean-pre_mean\n",
    "            diffs.append(diff)\n",
    "\n",
    "        # reorder the list to randomize\n",
    "        random.shuffle(diffs)\n",
    "\n",
    "        # calc bootstrap distribution\n",
    "        def loopOverNumberBootstrapSamples(i):\n",
    "            random_diffs = []\n",
    "            for j in range(num_forks):\n",
    "                index = np.random.randint(low=0,high=len(diffs))\n",
    "                diff  = diffs[index]\n",
    "                random_diffs.append(diff)\n",
    "            return np.mean(random_diffs)\n",
    "        bs_stat = Parallel(n_jobs=num_cpus)(delayed(loopOverNumberBootstrapSamples)(i) for i in range(num_bs_samples))\n",
    "\n",
    "        # calc standard error and add it to results\n",
    "        se = np.std(bs_stat)\n",
    "        results_df.loc[results_df.stat==col, 'se'] = se\n",
    "\n",
    "    # OUTPUT\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        results_df.to_excel(writer, sheet_name='raw_forks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f07237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genCorrStatistics(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Build table of correlation statistics. \"\"\"\n",
    "    # Set dates\n",
    "    start_date = '2013-12-01'\n",
    "    end_date   = '2023-07-01'\n",
    "\n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-31').reset_index()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf_tm7'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "\n",
    "    # Import other asset class data\n",
    "    nsdq_df = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq')\n",
    "    snp_df  = importYahoo('^GSPC', start_date, end_date, rf_df, 'SnP 500')\n",
    "    rus_df  = importYahoo('^RUT', start_date, end_date, rf_df, 'Russell 2000')\n",
    "    vt_df   = importYahoo('VT', start_date, end_date, rf_df, 'Global Stocks')\n",
    "    bnd_df  = importYahoo('BND', start_date, end_date, rf_df, 'US Bonds')\n",
    "    bndx_df = importYahoo('BNDX', start_date, end_date, rf_df, 'Ex-US Global Bonds')\n",
    "    real_df = importYahoo('VNQ', start_date, end_date, rf_df, 'US Real Estate')\n",
    "    emrg_df = importYahoo('EBND', start_date, end_date, rf_df, 'Emerging Currencies')\n",
    "    dbc_df  = importYahoo('DBC', start_date, end_date, rf_df, 'Commodities')\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold')\n",
    "\n",
    "    # Form crypto time series\n",
    "    btc_df = df[df.asset=='btc'][['date', 'char_r_tm7']].reset_index(drop=True).copy()\n",
    "    btc_df = btc_df.rename(columns={'char_r_tm7': 'Bitcoin'})\n",
    "\n",
    "    eth_df = df[df.asset=='eth'][['date', 'char_r_tm7']].reset_index(drop=True).copy()\n",
    "    eth_df = eth_df.rename(columns={'char_r_tm7': 'Ethereum'})\n",
    "\n",
    "    cmkt_df = df.groupby('date')[['macro_cmkt_tm7']].mean().reset_index()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'Crypto Market'})\n",
    "\n",
    "\n",
    "    # form single dataframe\n",
    "    t_df = cmkt_df.merge(btc_df, on='date', how='inner', validate='one_to_one')\n",
    "    for temp_df in [eth_df, nsdq_df, snp_df, rus_df, vt_df, bnd_df, bndx_df, real_df, emrg_df, dbc_df, gld_df]:\n",
    "        t_df = t_df.merge(temp_df, on='date', how='inner', validate='one_to_one')\n",
    "    t_df = t_df.set_index('date')\n",
    "\n",
    "    # Export\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        t_df[(t_df.index >= '2018-01-01') \n",
    "            & (t_df.index <= '2022-12-31')].corr().to_excel(writer, sheet_name='raw_corr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef6f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRollingCorrelations(out_fp: str) -> None:\n",
    "    \"\"\" Plot figure out rolling correlations between BTC and other asset classes. \"\"\"\n",
    "    # Parameters\n",
    "    start_date = '2013-11-01'\n",
    "    end_date   = '2023-07-02'\n",
    "    rolling_window = 48\n",
    "\n",
    "    # Pull and form exp inf data\n",
    "    ei_df = pdr.DataReader('EXPINF1YR', 'fred', start_date).reset_index()\n",
    "    ei_df = ei_df.rename(columns={'DATE': 'date'})\n",
    "    ei_df = ei_df.set_index('date')\n",
    "    ei_df = ei_df.pct_change().dropna().reset_index()\n",
    "    ei_df = ei_df[(ei_df.date >= '2014-01-01') & (ei_df.date <= '2023-07-01')]\n",
    "\n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-15').reset_index()\n",
    "    rf_df['r_rf'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / 12) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "    rf_df = rf_df[(rf_df.date >= '2014-01-01') & (rf_df.date <= '2023-07-01')]\n",
    "\n",
    "    # Import other asset class data\n",
    "    nsdq_df = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq', 'M', 'r_rf')\n",
    "    bnd_df  = importYahoo('BND', start_date, end_date, rf_df, 'US Bonds', 'M', 'r_rf')\n",
    "    real_df = importYahoo('VNQ', start_date, end_date, rf_df, 'US Real Estate', 'M', 'r_rf')\n",
    "    emrg_df = importYahoo('EBND', start_date, end_date, rf_df, 'Emerging Currencies', 'M', 'r_rf')\n",
    "    dbc_df  = importYahoo('DBC', start_date, end_date, rf_df, 'Commodities', 'M', 'r_rf')\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold', 'M', 'r_rf')\n",
    "\n",
    "    # import the btc data and form monthly excess returns\n",
    "    btc_df = pd.read_csv('../data/raw/cm_btc.csv')\n",
    "    btc_df['date'] = pd.to_datetime(btc_df.date)\n",
    "    btc_df = btc_df.rename(columns={'btc': 'Bitcoin'})\n",
    "    btc_df.set_index('date', inplace=True)\n",
    "    btc_df = btc_df.resample('M').last()\n",
    "    btc_df.index = btc_df.index + pd.Timedelta(days=1)\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'].pct_change()\n",
    "    btc_df = btc_df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'] - btc_df.r_rf\n",
    "    btc_df = btc_df.drop(['r_rf'], axis=1)\n",
    "\n",
    "    # form single dataframe\n",
    "    df = btc_df.merge(nsdq_df, on='date', how='inner', validate='one_to_one')\n",
    "    for temp_df in [bnd_df, real_df, emrg_df, dbc_df, gld_df, ei_df]:\n",
    "        df = df.merge(temp_df, on='date', how='inner', validate='one_to_one')\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    # Calc rolling correlations\n",
    "    results_df = pd.DataFrame(data = {'date': df.index.values})\n",
    "    results_df = results_df.set_index('date')\n",
    "    col_list = list(df.columns.values)[1:]\n",
    "    for col in col_list:\n",
    "        temp_df = df[['Bitcoin']].rolling(rolling_window).corr(df[col])\n",
    "        temp_df = temp_df.rename(columns = {'Bitcoin': col})\n",
    "        temp_df = temp_df.dropna()\n",
    "        results_df = results_df.merge(temp_df,\n",
    "                                    how='inner',\n",
    "                                    left_index=True,\n",
    "                                    right_index=True,\n",
    "                                    validate='one_to_one')\n",
    "    results_df = results_df[results_df.index < '2023-01-01']\n",
    "\n",
    "    # Form figure\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    results_df.plot(color=[viridis.colors[0], viridis.colors[44], viridis.colors[88],\n",
    "        viridis.colors[132], viridis.colors[178], viridis.colors[222], viridis.colors[255]])\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    plt.legend(labels=results_df.columns.values, \n",
    "                loc='lower center',\n",
    "                ncol=3,\n",
    "                bbox_to_anchor=(0.5, -0.37),\n",
    "                frameon=False)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d06f6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formRiskReturnScatterPlot(df: pd.DataFrame, out_fp: str, periods_in_year: int) -> None:\n",
    "    \"\"\" Plot all assets in return risk space. \"\"\"\n",
    "    # calc each asset's annualized return and risk\n",
    "    r_df = df[['date', 'asset', 'char_r_tm7', 'macro_dgs1mo_t']].copy()\n",
    "    r_df['char_r_tm7'] = r_df.char_r_tm7 + ((r_df.macro_dgs1mo_t.values/100)+1)**(1/52)-1\n",
    "    r_df = r_df.drop('macro_dgs1mo_t', axis=1)\n",
    "    r_df['annual_return'] = r_df.groupby('asset')['char_r_tm7'].transform(lambda x: \n",
    "                                QuantTools.calcCumulativeReturn(x, annualized=True, periods_in_year=periods_in_year))\n",
    "    r_df['annual_risk'] = r_df.groupby('asset')['char_r_tm7'].transform(lambda x: \n",
    "                                QuantTools.calcSD(x, annualized=True, periods_in_year=periods_in_year))\n",
    "    r_df = r_df[['asset', 'annual_return', 'annual_risk']].drop_duplicates()\n",
    "    r_df = r_df.sort_values(by='asset', ignore_index=True)\n",
    "\n",
    "    # drop assets with absurd returns and note what/how many we dropped\n",
    "    crazy_df = r_df[r_df.annual_return > 5].copy()\n",
    "    r_df = r_df[r_df.annual_return <= 5]\n",
    "    for asset in list(crazy_df.asset.values):\n",
    "        print(f'{asset} was dropped from risk return plot with annual return {np.round(crazy_df[crazy_df.asset==asset].annual_return.values[0], 1)}')\n",
    "\n",
    "    # add 1 month tbill to data\n",
    "    t_df = df[['date', 'macro_dgs1mo_t']].drop_duplicates().reset_index(drop=True)\n",
    "    t_weekly_returns = ((t_df.macro_dgs1mo_t.values/100)+1)**(1/52)-1\n",
    "    t_annual_return = QuantTools.calcCumulativeReturn(t_weekly_returns, annualized=True, periods_in_year=periods_in_year)\n",
    "    r_df = pd.concat([r_df, \n",
    "                    pd.DataFrame(data={'asset': ['1moTbill'], \n",
    "                                    'annual_return': [t_annual_return],\n",
    "                                    'annual_risk': [0]})]).reset_index(drop=True)\n",
    "\n",
    "    # Calc risk free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-31').reset_index()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf_tm7'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "\n",
    "    # Import other asset class data\n",
    "    nsdq_df = importYahoo('^IXIC', '2017-11-30', '2023-01-08', rf_df, 'Nasdaq')\n",
    "    nsdq_df = nsdq_df[(nsdq_df.date >= '2018-01-01') & (nsdq_df.date <= '2022-12-31')]\n",
    "\n",
    "    # Add nsdq\n",
    "    nsdq_annual_ret = QuantTools.calcCumulativeReturn(nsdq_df.Nasdaq.values, annualized=True, periods_in_year=periods_in_year)\n",
    "    nsdq_sd = QuantTools.calcSD(nsdq_df.Nasdaq.values, annualized=True, periods_in_year=periods_in_year)\n",
    "    r_df = pd.concat([r_df, \n",
    "                    pd.DataFrame(data={'asset': ['nsdq'], \n",
    "                            'annual_return': [nsdq_annual_ret],\n",
    "                            'annual_risk': [nsdq_sd]})]).reset_index(drop=True)\n",
    "\n",
    "    # Add cmkt\n",
    "    cmkt_df = df.groupby('date')[['macro_cmkt_tm7']].mean().reset_index()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'cmkt'})\n",
    "    cmkt_df['cmkt'] = cmkt_df.cmkt+t_weekly_returns\n",
    "    cmkt_annual_ret = QuantTools.calcCumulativeReturn(cmkt_df.cmkt.values, annualized=True, periods_in_year=periods_in_year)\n",
    "    cmkt_sd = QuantTools.calcSD(cmkt_df.cmkt.values, annualized=True, periods_in_year=periods_in_year)\n",
    "    r_df = pd.concat([r_df,\n",
    "                    pd.DataFrame(data={'asset': ['cmkt'], \n",
    "                            'annual_return': [cmkt_annual_ret],\n",
    "                            'annual_risk': [cmkt_sd]})]).reset_index(drop=True)\n",
    "\n",
    "    # Form optimal risk portfolio\n",
    "    nsdq_pcts = list(np.arange(0, 1, 0.05)) + [.99]\n",
    "    max_sharpe = 0\n",
    "    nsdq_opt   = 0\n",
    "    for nsdq_pct in nsdq_pcts:\n",
    "        cmkt_pct = 1 - nsdq_pct\n",
    "        d_ret = nsdq_df.Nasdaq.values*nsdq_pct + cmkt_df.cmkt.values*cmkt_pct\n",
    "        d_annual_ret = QuantTools.calcCumulativeReturn(d_ret, annualized=True, periods_in_year=periods_in_year)\n",
    "        d_sd = QuantTools.calcSD(d_ret, annualized=True, periods_in_year=periods_in_year)\n",
    "        print(f\"for nsdq pct {np.round(nsdq_pct, 2)}, the sharpe is {np.round(d_annual_ret / d_sd, 3)} and annual return is {np.round(d_annual_ret, 4)}\")\n",
    "        if (d_annual_ret / d_sd) > max_sharpe:\n",
    "            nsdq_opt = nsdq_pct\n",
    "            opt_return = d_annual_ret\n",
    "            opt_sd     = d_sd\n",
    "            max_sharpe = opt_return / opt_sd\n",
    "        \n",
    "    print(f\"\\n for a sharpe of {np.round(max_sharpe, 4)}, allocate {np.round(nsdq_opt, 4)} to nsdq and {np.round(1-nsdq_opt, 4)} to cmkt.\")\n",
    "\n",
    "    opt_col_name = 'nsdq_'+str(np.round(nsdq_opt, 2))+'_cmkt_'+str(np.round(1-nsdq_opt, 2))\n",
    "    r_df = pd.concat([r_df,\n",
    "                    pd.DataFrame(data={'asset': [opt_col_name], \n",
    "                            'annual_return': [opt_return],\n",
    "                            'annual_risk': [opt_sd]})]).reset_index(drop=True)\n",
    "\n",
    "    # Final edits\n",
    "    r_df = r_df.set_index('asset')\n",
    "    r_df = r_df[r_df.index != 'luna']\n",
    "\n",
    "    # Form figure\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    plt.scatter(r_df['annual_risk'], r_df['annual_return'], color=viridis.colors[111])\n",
    "\n",
    "    labels = [\"1moTbill\", \"btc\", \"cmkt\", \"eth\", \"nsdq\", opt_col_name]\n",
    "    colors = ['lightgrey', viridis.colors[255], viridis.colors[33], \n",
    "        viridis.colors[177], 'grey', 'black']\n",
    "    for label, color in zip(labels, colors):\n",
    "        asset_df = r_df[r_df.index==label]\n",
    "        plt.scatter(asset_df['annual_risk'], asset_df['annual_return'], color=color)\n",
    "\n",
    "    point_rf = r_df[r_df.index == \"1moTbill\"].iloc[0]\n",
    "    point_nsdq = r_df[r_df.index == \"nsdq\"].iloc[0]\n",
    "    point_cmkt = r_df[r_df.index == \"cmkt\"].iloc[0]\n",
    "    point_opt  = r_df[r_df.index == opt_col_name].iloc[0]\n",
    "    slope_nsdq = (point_nsdq['annual_return'] - point_rf['annual_return']) / (point_nsdq['annual_risk'] - point_rf['annual_risk'])\n",
    "    slope_cmkt = (point_cmkt['annual_return'] - point_rf['annual_return']) / (point_cmkt['annual_risk'] - point_rf['annual_risk'])\n",
    "    slope_opt = (point_opt['annual_return'] - point_rf['annual_return']) / (point_opt['annual_risk'] - point_rf['annual_risk'])\n",
    "    max_x = r_df['annual_risk'].max()\n",
    "    max_y_nsdq = slope_nsdq * (max_x - point_rf['annual_risk']) + point_rf['annual_return']\n",
    "    max_y_cmkt = slope_cmkt * (max_x - point_rf['annual_risk']) + point_rf['annual_return']\n",
    "    max_y_opt  = slope_opt  * (max_x - point_rf['annual_risk']) + point_rf['annual_return']\n",
    "    plt.plot([point_rf['annual_risk'], max_x], [point_rf['annual_return'], max_y_nsdq], ':', color='grey')\n",
    "    plt.plot([point_rf['annual_risk'], max_x], [point_rf['annual_return'], max_y_cmkt], ':', color=viridis.colors[33])\n",
    "    plt.plot([point_rf['annual_risk'], max_x], [point_rf['annual_return'], max_y_opt], ':', color='black')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.xlim(0, max_x + 0.3)\n",
    "\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e824e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportICOStatistics(df: pd.DataFrame) -> None:\n",
    "    # Subset to relevant data\n",
    "    ico_df = df[['date', 'asset', 'r_ex_tp7', 'char_price_t', \n",
    "        'char_ico', 'char_ico_days_since_t', 'char_ico_price']].copy()\n",
    "\n",
    "    # Determine which assets are ico assets\n",
    "    ico_assets = np.unique(ico_df[ico_df.char_ico==1].asset.values)\n",
    "\n",
    "    # Subset to relevant assets\n",
    "    ico_df = ico_df[ico_df.asset.isin(ico_assets)]\n",
    "    ico_df = ico_df.drop(columns='char_ico', axis=1)\n",
    "\n",
    "    # Form dataframe of the last tradable data for all assets\n",
    "    last_df = ico_df.groupby('asset')[['date']].max().reset_index()\n",
    "    last_df = last_df.merge(ico_df[['date', 'asset', 'char_price_t', 'char_ico_price']], \n",
    "        on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Calc and report return since ICO\n",
    "    last_df['r_from_ico'] = last_df.char_price_t / last_df.char_ico_price - 1\n",
    "    return_from_ico_dates = last_df.r_from_ico.mean()\n",
    "    print(f\"Return on ICO assets from ICO date to end of panel is: {int(return_from_ico_dates)}x.\")\n",
    "\n",
    "    # Calc return of ico assets from first tradable data to end of panel\n",
    "    min_df = ico_df[(ico_df.char_ico_days_since_t>0)].groupby('asset')[['date']].min().reset_index()\n",
    "    min_df = min_df.merge(ico_df[['date', 'asset', 'char_price_t']],\n",
    "        on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    min_df = min_df.rename(columns={'char_price_t': 'first_trade_price'})\n",
    "    min_df = min_df.drop('date', axis=1)\n",
    "\n",
    "    max_df = ico_df.groupby('asset')[['date']].max().reset_index()\n",
    "    max_df = max_df.merge(ico_df[['date', 'asset', 'char_price_t']], \n",
    "        on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    max_df = max_df.rename(columns={'char_price_t': 'last_trade_price'})\n",
    "    max_df = max_df.drop('date', axis=1)\n",
    "    trade_df = max_df.merge(min_df, on='asset', how='inner', validate='one_to_one')\n",
    "    trade_df['r'] = trade_df.last_trade_price / trade_df.first_trade_price - 1\n",
    "\n",
    "    return_from_first_tradable = trade_df.r.mean()\n",
    "    print(f\"Return on ICO assets from first tradable date to end of panel is: {np.round(100*return_from_first_tradable, 1)}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6dd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflationStatistics(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    # Copy the raw data for use later as this func is terribly written\n",
    "    temp_df = df.copy()\n",
    "\n",
    "    # Params\n",
    "    start_date = '2013-11-01'\n",
    "    end_date   = '2023-07-02'\n",
    "\n",
    "    # Obtain monthly excess returns for cmkt\n",
    "    cmkt_df = df[['date', 'macro_cmkt_tm7']].drop_duplicates().copy()\n",
    "    cmkt_df.set_index('date', inplace=True)\n",
    "    cmkt_df['cmkt'] = cmkt_df.macro_cmkt_tm7+1\n",
    "    cmkt_df = cmkt_df.drop('macro_cmkt_tm7', axis=1)\n",
    "    cmkt_df = cmkt_df.resample('M')[['cmkt']].prod()-1\n",
    "    cmkt_df.index = cmkt_df.index + pd.DateOffset(days=1) - pd.DateOffset(months=1)\n",
    "\n",
    "    # Pull and form exp inf data\n",
    "    inf_col = 'EXPINF10YR'\n",
    "    ei_df = pdr.DataReader(inf_col, 'fred', start_date).reset_index()\n",
    "    ei_df = ei_df.rename(columns={'DATE': 'date'})\n",
    "    ei_df = ei_df.set_index('date')\n",
    "    ei_df[inf_col] = (1+ei_df[inf_col])**(1/12)-1 # annual inflation rate to monthly rate\n",
    "    ei_df = ei_df.pct_change().dropna().reset_index()\n",
    "    ei_df = ei_df[(ei_df.date >= '2014-01-01') & (ei_df.date <= '2023-07-01')]\n",
    "\n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-15').reset_index()\n",
    "    rf_df['r_rf'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / 12) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "    rf_df = rf_df[(rf_df.date >= '2014-01-01') & (rf_df.date <= '2023-07-01')]\n",
    "\n",
    "    # Import other asset class data\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold', 'M', 'r_rf')\n",
    "    nsdq_df  = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq', 'M', 'r_rf')\n",
    "\n",
    "    # import the btc data and form monthly excess returns\n",
    "    btc_df = pd.read_csv('../data/raw/cm_btc.csv')\n",
    "    btc_df['date'] = pd.to_datetime(btc_df.date)\n",
    "    btc_df = btc_df.rename(columns={'btc': 'Bitcoin'})\n",
    "    btc_df.set_index('date', inplace=True)\n",
    "    btc_df = btc_df.resample('M').last()\n",
    "    btc_df.index = btc_df.index + pd.Timedelta(days=1)\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'].pct_change()\n",
    "    btc_df = btc_df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'] - btc_df.r_rf\n",
    "    btc_df = btc_df.drop(['r_rf'], axis=1)\n",
    "\n",
    "    # form single dataframe\n",
    "    df = cmkt_df.merge(btc_df, on='date', how='right', validate='one_to_one')\n",
    "    df = df.merge(ei_df, on='date', how='outer', validate='one_to_one')\n",
    "    df = df.merge(gld_df, on='date', how='outer', validate='one_to_one')\n",
    "    df = df.merge(nsdq_df, on='date', how='outer', validate='one_to_one')\n",
    "    df = df.set_index('date')\n",
    "    df = df[df.index <= '2023-01-01']\n",
    "\n",
    "    # OVERALL CORR OF MONTHLY RETURNS AND INF INNOV\n",
    "    print(df.index.min())\n",
    "    print(df.index.max())\n",
    "    print(df.drop('cmkt', axis=1).corr())\n",
    "\n",
    "    # OVERALL CORR OF MONTHLY RETURNS AND INF INNOV JUST 2018-2022\n",
    "    print(df[df.cmkt.notnull()].corr())\n",
    "\n",
    "    # CORR BETWEEN BTC AND EXPINF1YR AS WELL AS BTWN GOLD AND EXPINF1YR \n",
    "    # ON TOP 12 MONTHS IN 2014 Jan to Dec 2022 and Jan 2018 to Dec 2022 for cmkt\n",
    "    df['inf_abs'] = np.abs(df[inf_col])\n",
    "    print(df.drop('cmkt', axis=1).sort_values(by='inf_abs', ascending=False)[:12].corr())\n",
    "    print(df[df.cmkt.notnull()].sort_values(by='inf_abs', ascending=False)[:12].corr())\n",
    "    df = df.drop('inf_abs', axis=1)\n",
    "\n",
    "    # Regress BTC returns on CMKT and inf\n",
    "    df = df[df.cmkt.notnull()]\n",
    "    y = df.Bitcoin\n",
    "    X = df[['cmkt', inf_col]]\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    results_summary = results.summary2().tables\n",
    "    summary_df = pd.DataFrame(results_summary[1])\n",
    "    summary_df['N'] = len(y)\n",
    "    summary_df['R2'] = results.rsquared\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        summary_df.to_excel(writer, sheet_name='raw_inf_reg')\n",
    "\n",
    "    # Fama Macbeth\n",
    "    temp_df = temp_df[['date', 'asset', 'char_r_tm7']].copy()\n",
    "    asset_df = pd.DataFrame()\n",
    "    assets = list(np.unique(temp_df.asset.values))\n",
    "    for asset in assets:\n",
    "        t_df = temp_df[temp_df.asset==asset][['date', 'char_r_tm7']]\n",
    "        t_df.set_index('date', inplace=True)\n",
    "        t_df['r'] = t_df['char_r_tm7']+1\n",
    "        t_df = t_df.drop('char_r_tm7', axis=1)\n",
    "        t_df = t_df.resample('M')[['r']].prod()-1\n",
    "        t_df.index = t_df.index + pd.DateOffset(days=1) - pd.DateOffset(months=1)\n",
    "        t_df = t_df.reset_index()\n",
    "        t_df['asset'] = asset\n",
    "        asset_df = pd.concat([asset_df, t_df])\n",
    "\n",
    "    beta_hats = []\n",
    "    assets = list(np.unique(asset_df.asset.values))\n",
    "    assets.remove('inv')\n",
    "    for asset in assets:\n",
    "        a_df = asset_df[asset_df.asset==asset]\n",
    "        a_df = a_df.merge(df[[inf_col]].reset_index(), on='date', how='inner', validate='one_to_one')\n",
    "        y = a_df.r\n",
    "        X = a_df[[inf_col]]\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X)\n",
    "        results = model.fit()\n",
    "        beta_hats.append(results.params[1])\n",
    "\n",
    "    y = asset_df[asset_df.asset!='inv'].groupby('asset')['r'].mean().values\n",
    "    x = np.array(beta_hats)\n",
    "    X = sm.add_constant(x)\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    results_summary = results.summary2().tables\n",
    "    summary_df = pd.DataFrame(results_summary[1])\n",
    "    summary_df['N'] = len(y)\n",
    "    summary_df['R2'] = results.rsquared\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        summary_df.to_excel(writer, sheet_name='raw_fama_macbeth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26125bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genCharDescStatistics(df: pd.DataFrame, char_group_cols: List[list], lhs_col: str, out_fp: str) -> None:\n",
    "    char_cols = [col for sublist in char_group_cols for col in sublist]\n",
    "    char_df = df[[lhs_col]+char_cols].copy()\n",
    "\n",
    "    char_desc_stat_df = char_df.describe().T\n",
    "\n",
    "    percentiles = [1, 5, 95, 99]\n",
    "    for prcnt in percentiles:\n",
    "        char_desc_stat_df[str(prcnt)+'%'] = char_df.apply(lambda x: np.percentile(x, prcnt))\n",
    "\n",
    "    def formatter(x):\n",
    "        if x > 1000 or x < -1000:\n",
    "            return \"{:.1e}\".format(x)\n",
    "        elif 10 <= x <= 1000 or -1000 <= x <= -10:\n",
    "            return \"{:.0f}\".format(x)\n",
    "        elif 1 <= x < 10 or -10 < x <= -1:\n",
    "            return \"{:.1f}\".format(x)\n",
    "        elif 0.001 <= x < 1 or -1 < x <= -0.001:\n",
    "            return \"{:.3f}\".format(x)\n",
    "        else: # -0.001 < x < 0.001\n",
    "            return \"{:.1e}\".format(x)\n",
    "\n",
    "    for col in list(char_desc_stat_df.columns.values):\n",
    "        char_desc_stat_df[col] = char_desc_stat_df[col].apply(formatter)\n",
    "    \n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        char_desc_stat_df.to_excel(writer, sheet_name='raw_char_desc_stat')\n",
    "\n",
    "def generatePrincipalComponent(\n",
    "    df: pd.DataFrame, var: List[str], ncomponents: int = 1, pc_col_name: str = 'pca') -> pd.DataFrame:\n",
    "    \n",
    "    '''\n",
    "    Generate a column of the principle component among var and add the column to the dataframe\n",
    "    \n",
    "    Keyword arguments:\n",
    "        df -- input dataframe\n",
    "        var -- list of variables to inspect\n",
    "        ncomponents -- number of components of PCA\n",
    "        pc_col_name -- name of the PC column\n",
    "        \n",
    "    Return value:\n",
    "        df -- dataframe with the PC column\n",
    "    '''\n",
    "    \n",
    "    # pca\n",
    "    X = df[var]\n",
    "    pca = decomposition.PCA(n_components=ncomponents)\n",
    "    res = pca.fit_transform(X)\n",
    "\n",
    "    # add pc to dataframe\n",
    "    df[pc_col_name] = res\n",
    "    \n",
    "    return df\n",
    "\n",
    "def genPcCorrTable(df: pd.DataFrame, char_group_cols: List[list], out_fp: str) -> None:\n",
    "    # Form the first PC for each group of characteristics\n",
    "    pcs = []\n",
    "    for char_cols in char_group_cols:\n",
    "        char_df = df[char_cols]\n",
    "        pca = decomposition.PCA(n_components=1)\n",
    "        pc  = pca.fit_transform(char_df)\n",
    "        pcs.append(pc)\n",
    "\n",
    "    # Concatenate the PC's together\n",
    "    r_df = pd.DataFrame(np.concatenate(pcs, axis=1))\n",
    "\n",
    "    # Calc correlations\n",
    "    corr_df = r_df.corr()\n",
    "\n",
    "    # Output the results\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        corr_df.to_excel(writer, sheet_name='raw_pca_corr')\n",
    "\n",
    "def formRelCharDf(df: pd.DataFrame, lhs_col: str, char_cols: List[str], char_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function forms a DataFrame with relevant data, rearranges columns, drops data beyond a certain date, \n",
    "    and generates a Principal Component column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    lhs_col (str): The column name to be used as the left-hand side column.\n",
    "    char_cols (List[str]): A list of character column names.\n",
    "    char_name (str): The name to be used for the generated Principal Component column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The resulting DataFrame after all transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Form the relevant data\n",
    "    # -accounting for whether return tm7 is in char_cols to avoiding dup col\n",
    "    if 'char_r_tm7' in char_cols:\n",
    "        char_df = df[['date', 'asset', lhs_col]+char_cols].copy()\n",
    "        char_df['r_ex_tp0'] = char_df['char_r_tm7']\n",
    "    else:\n",
    "        char_df = df[['date', 'asset', lhs_col]+['char_r_tm7']+char_cols].copy()\n",
    "        char_df = char_df.rename(columns={'char_r_tm7': 'r_ex_tp0'})\n",
    "\n",
    "    # Form remaining LHS columns\n",
    "    new_lhs_cols = ['r_ex_tp0']\n",
    "    for horizon, day_delta in zip([14, 30, 60], [14, 28, 56]):\n",
    "        rhs_col = 'char_r_tm'+str(horizon)\n",
    "        new_lhs_col = 'r_ex_tp'+str(horizon)\n",
    "        new_lhs_cols.append(new_lhs_col)\n",
    "        temp_df = df[['date', 'asset', rhs_col]].copy()\n",
    "        temp_df = temp_df.rename(columns={rhs_col: new_lhs_col})\n",
    "        temp_df['date'] = temp_df.date - pd.DateOffset(days=day_delta)\n",
    "        char_df = char_df.merge(temp_df, \n",
    "            on=['date', 'asset'], how='left', validate='one_to_one') \n",
    "\n",
    "    # Rearrange cols\n",
    "    char_df = char_df[['date', 'asset', lhs_col] + new_lhs_cols + char_cols]\n",
    "\n",
    "    # Drop 2h 2022 data\n",
    "    char_df = char_df[char_df.date < '2022-07-01']\n",
    "\n",
    "    # Generate the PC column\n",
    "    char_df = generatePrincipalComponent(char_df, char_cols, 1, 'pc_'+char_name)\n",
    "\n",
    "    return char_df\n",
    "    \n",
    "def genAndOutputCharCorrTable(char_df: pd.DataFrame,\n",
    "    char_cols: List[str], char_name: str, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a correlation DataFrame and save it to an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    char_df: The DataFrame to calculate correlations on.\n",
    "    char_cols: The columns to include in the correlation calculation.\n",
    "    char_name: The name of the characteristic to include in the correlation calculation.\n",
    "    out_fp: The file path to save the correlation DataFrame to.\n",
    "    \"\"\"\n",
    "    # Generate correlation dataframe\n",
    "    corr_df = char_df[char_cols+['pc_'+char_name]].corr()\n",
    "\n",
    "    # Save the table\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        corr_df.to_excel(writer, sheet_name='raw_char_corr_'+char_name)\n",
    "\n",
    "def format_float(num):\n",
    "    str_num = str(num)\n",
    "    if \"e\" in str_num or num < 0.01:\n",
    "        return np.format_float_scientific(num, precision=1, unique=False, trim='k')\n",
    "    else:\n",
    "        decimal_part = str_num.split('.')[1] if '.' in str_num else ''\n",
    "        precision = 2 if len(decimal_part) >= 2 else len(decimal_part)\n",
    "        return np.format_float_positional(num, precision=precision, unique=False, trim='k')\n",
    "\n",
    "def genAndOutputCharRegTable(char_df: pd.DataFrame,\n",
    "    char_cols: List[str], char_name: str, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a regression DataFrame and save it to an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    char_df: The DataFrame to calculate regressions on.\n",
    "    char_cols: The columns to include in the regression calculation.\n",
    "    char_name: The name of the characteristic to include in the regression calculation.\n",
    "    out_fp: The file path to save the regression DataFrame to.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize DataFrame for the results\n",
    "    results_df = pd.DataFrame(data={'char': []})\n",
    "\n",
    "    # Generate results for all horizons and all characteristics\n",
    "    for horizon in [0, 7, 14, 30, 60]:\n",
    "        horizon_results_df = pd.DataFrame()\n",
    "        for char_col in char_cols+['pc_'+char_name]:\n",
    "            # calc the uni reg\n",
    "            y_col   = 'r_ex_tp'+str(horizon)\n",
    "            rel_df  = char_df[char_df[y_col].notnull()]\n",
    "            y       = rel_df[y_col]\n",
    "            x       = rel_df[char_col]\n",
    "            X       = sm.add_constant(x)\n",
    "            model   = sm.OLS(y, X)\n",
    "            results = model.fit()\n",
    "\n",
    "            # obtain the results\n",
    "            coef    = results.params[1]\n",
    "            se      = results.bse[1]\n",
    "            tstat   = results.tvalues[1]\n",
    "            r2      = results.rsquared\n",
    "\n",
    "            # format the results\n",
    "            coef    = format_float(coef)\n",
    "            se      = \"(\"+format_float(se)+\")\"\n",
    "            r2      = np.round(results.rsquared, 2)\n",
    "\n",
    "            # add significant\n",
    "            if np.abs(tstat) >= 2.326:\n",
    "                coef = coef+\"***\"\n",
    "            elif np.abs(tstat) >= 1.96:\n",
    "                coef = coef+\"**\"\n",
    "            elif np.abs(tstat) >= 1.645:\n",
    "                coef = coef+\"*\"\n",
    "\n",
    "            # build df of results\n",
    "            horizon_results_df = pd.concat([horizon_results_df,\n",
    "                pd.DataFrame(data={\n",
    "                    'char': [char_col+'_coef', \n",
    "                            char_col+'_se', \n",
    "                            char_col+'_r2'],\n",
    "                    'tp'+str(horizon): [coef, se, r2]})])\n",
    "        \n",
    "        # build the main df\n",
    "        results_df = results_df.merge(horizon_results_df, \n",
    "            on='char', how='outer', validate='one_to_one')\n",
    "\n",
    "    # Save the table\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        results_df.to_excel(writer, sheet_name='raw_char_reg_'+char_name)\n",
    "\n",
    "def calc_MI(temp_df: pd.DataFrame, lhs_col: str, feat_col: str):\n",
    "    ''' Calculate mutual information between two given columns in given df.\n",
    "    \n",
    "    Args:\n",
    "        temp_df (pd.DataFrame): dataframe to obtain x and y columns from.\n",
    "        lhs_col (str): name of first column to use to calc MI.\n",
    "        feat_col (str): name of second col to use to calc MI.\n",
    "\n",
    "    Returns:\n",
    "        mi (float): mutual information between two given vectors.\n",
    "    '''\n",
    "    # obtain the two data series\n",
    "    x    = temp_df[lhs_col].values\n",
    "    y    = temp_df[feat_col].values\n",
    "\n",
    "    # calc mutual information\n",
    "    bins = int(np.floor(np.sqrt(temp_df.shape[0]/5)))\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi   = mutual_info_score(None, None, contingency=c_xy)\n",
    "\n",
    "    return mi\n",
    "\n",
    "def calcMiByYear(df: pd.DataFrame, lhs_col: str, char_group_cols: List[list], out_fp: str) -> None:\n",
    "    # obtain all characteristics in same order\n",
    "    char_cols = [col for sublist in char_group_cols for col in sublist]\n",
    "        \n",
    "    # initialize object for results\n",
    "    results_df = pd.DataFrame(data={\n",
    "        'char': char_cols+['N'],\n",
    "        '2018': ['' for _ in char_cols]+[''],\n",
    "        '2019': ['' for _ in char_cols]+[''],\n",
    "        '2020': ['' for _ in char_cols]+[''],\n",
    "        '2021': ['' for _ in char_cols]+[''],\n",
    "        '2022': ['' for _ in char_cols]+[''],\n",
    "        'All': ['' for _ in char_cols]+['']\n",
    "    })\n",
    "\n",
    "    # loop over years to calc mutual info\n",
    "    for col in char_cols:\n",
    "        for year in [2018, 2019, 2020, 2021, 2022]:\n",
    "            mi = calc_MI(df[(df.date.dt.year==year)], lhs_col, col)\n",
    "            results_df.loc[results_df.char==col, str(year)] = mi\n",
    "        mi = calc_MI(df, lhs_col, col)\n",
    "        results_df.loc[results_df.char==col, 'All'] = mi\n",
    "\n",
    "    # Add ob count by year\n",
    "    for year in [2018, 2019, 2020, 2021, 2022]:\n",
    "        results_df.loc[results_df.char=='N', str(year)] = len(df[(df.date.dt.year==year)])\n",
    "\n",
    "    # Save the table\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        results_df.to_excel(writer, sheet_name='raw_char_mi_by_year')\n",
    "\n",
    "def genCharDescCorrAndRegStats(df: pd.DataFrame, lhs_col: str, out_fp: str) -> None:\n",
    "    # Build lists of characteristics to produce results for.\n",
    "    oc_cols = ['char_circulation_tm7',\n",
    "        'char_circulation_tm30',\n",
    "        'char_circulation_tm90',\n",
    "        'char_tx_volume_t',\n",
    "        'char_tx_volume_tm7',\n",
    "        'char_addr_new_tm1',\n",
    "        'char_addr_new_tm7',\n",
    "        'char_addr_active_tm1',\n",
    "        'char_addr_active_tm7',\n",
    "        'char_addr_new_log_delta_tm14_tm7',\n",
    "        'char_age_destroyed_tm7',\n",
    "        'char_age_mean_dollar_t',\n",
    "        'char_delta_flow_dist_tm7',\n",
    "        'char_delta_holders_dist_tm7',\n",
    "        'char_prct_supply_in_profit_t']\n",
    "    ex_cols = ['char_exchange_prct_circ_supply_t',\n",
    "        'char_cex_prct_circ_supply_t',\n",
    "        'char_dex_prct_circ_supply_t',\n",
    "        'char_defi_prct_circ_supply_t',\n",
    "        'char_traders_prct_circ_supply_t',\n",
    "        'char_exchange_inflow_tm7',\n",
    "        'char_exchange_outflow_tm7',\n",
    "        'char_rank_cmc_t',\n",
    "        'char_tradable_t']\n",
    "    social_cols = ['char_social_volume_tm7',\n",
    "        'char_social_volume_reddit_tm7',\n",
    "        'char_social_volume_twitter_tm7',\n",
    "        'char_sent_neg_reddit_tm7',\n",
    "        'char_sent_neg_twitter_tm7',\n",
    "        'char_sent_pos_reddit_tm7',\n",
    "        'char_sent_pos_twitter_tm7',\n",
    "        'char_sent_volume_consumed_tm7',\n",
    "        'char_social_dom_avg_tm7',\n",
    "        'char_dev_activity_tm7',\n",
    "        'char_vc_t']\n",
    "    mom_cols = ['char_r_tm7',\n",
    "        'char_r_tm14',\n",
    "        'char_r_tm14_tm7',\n",
    "        'char_r_tm30',\n",
    "        'char_r_tm60',\n",
    "        'char_r_tm90',\n",
    "        'char_r_max_tm7',\n",
    "        'char_r_max_tm30',\n",
    "        'char_r_ath_t',\n",
    "        'char_r_atl_t']\n",
    "    mic_cols = ['char_trades_t',\n",
    "        'char_trades_sum_tm7',\n",
    "        'char_trades_std_tm7',\n",
    "        'char_volume_sum_tm7',\n",
    "        'char_volume_std_tm7',\n",
    "        'char_ask_t',\n",
    "        'char_bid_t',\n",
    "        'char_bidask_t',\n",
    "        'char_ask_size_t',\n",
    "        'char_bid_size_t',\n",
    "        'char_illiq_tm7',\n",
    "        'char_turnover_tm7']\n",
    "    fin_cols = ['char_price_t',\n",
    "        'char_size_t',\n",
    "        'char_mvrv_t',\n",
    "        'char_alpha_tm7',\n",
    "        'char_alpha_tm30',\n",
    "        'char_beta_tm7',\n",
    "        'char_beta_tm30',\n",
    "        'char_beta_downside_tm30',\n",
    "        'char_coskew_tm30',\n",
    "        'char_iskew_tm30',\n",
    "        'char_shortfall5_tm7',\n",
    "        'char_shortfall5_tm90',\n",
    "        'char_var5_tm7',\n",
    "        'char_var5_tm90',\n",
    "        'char_vol_tm7',\n",
    "        'char_vol_tm30',\n",
    "        'char_ivol_tm7',\n",
    "        'char_ivol_tm30']\n",
    "\n",
    "    # Produce the name of these characteristic groups, in same order\n",
    "    char_groups = ['onchain', 'exchange', 'social',\n",
    "        'mom', 'micro', 'financial']\n",
    "    char_group_cols = [oc_cols, ex_cols, social_cols, \n",
    "        mom_cols, mic_cols, fin_cols]\n",
    "        \n",
    "    # Output the characteristic descriptive statistics\n",
    "    genCharDescStatistics(df, char_group_cols, lhs_col, out_fp)\n",
    "\n",
    "    # Output the correlation table among the pc's\n",
    "    genPcCorrTable(df, char_group_cols, out_fp)\n",
    "\n",
    "    # Generate output for each group of characteristics\n",
    "    for char_name, char_cols in zip(char_groups, char_group_cols):\n",
    "        # form the relevant data\n",
    "        char_df = formRelCharDf(df, lhs_col, char_cols, char_name)\n",
    "\n",
    "        # output the corr table\n",
    "        genAndOutputCharCorrTable(char_df, char_cols, char_name, out_fp)\n",
    "\n",
    "        # output the reg table\n",
    "        genAndOutputCharRegTable(char_df, char_cols, char_name, out_fp)\n",
    "\n",
    "    # Calculate mutual information measure\n",
    "    calcMiByYear(df, lhs_col, char_group_cols, out_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "724676b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMcaps(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    # Subset to relevant columns\n",
    "    industry_cols    = [col for col in df.columns if ('industry' in col) & ('industry_t' not in col)]\n",
    "    asset_usage_cols = [col for col in df.columns if 'asset_usage' in col]\n",
    "    mcap_df = df[['date', 'asset', 'char_size_t', 'char_pow', 'char_pos']+industry_cols+asset_usage_cols].copy()\n",
    "\n",
    "    # Create the three dataframes of mcap series to plot\n",
    "    total_df = mcap_df.groupby('date')[['char_size_t']].sum()\n",
    "    total_df = total_df.rename(columns={'char_size_t': 'total'})\n",
    "    asset_df = mcap_df[mcap_df.asset.isin(['btc', 'eth'])].groupby(['date', 'asset'])[['char_size_t']].sum().reset_index()\n",
    "    asset_df = asset_df.pivot(index='date', columns='asset', values='char_size_t')\n",
    "    asset_df = total_df.merge(asset_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    industry_dfs = []\n",
    "    for col in industry_cols:\n",
    "        temp_df = mcap_df[mcap_df[col]==1].groupby('date')[['char_size_t']].sum()\n",
    "        new_col_name = \"_\".join(col.split('_')[2:])\n",
    "        temp_df = temp_df.rename(columns={'char_size_t': new_col_name})\n",
    "        industry_dfs.append(temp_df)\n",
    "    industry_df = pd.concat(industry_dfs, axis=1)\n",
    "\n",
    "    usage_dfs = []\n",
    "    for col in asset_usage_cols:\n",
    "        temp_df = mcap_df[mcap_df[col]==1].groupby('date')[['char_size_t']].sum()\n",
    "        new_col_name = col.split('_')[3]\n",
    "        temp_df = temp_df.rename(columns={'char_size_t': new_col_name})\n",
    "        usage_dfs.append(temp_df)\n",
    "    usage_df = pd.concat(usage_dfs, axis=1)\n",
    "    usage_df = usage_df.drop('access', axis=1)\n",
    "\n",
    "    # Create a figure with 3 subplots\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14, 6))\n",
    "\n",
    "    # Plot the 'value' column from each dataframe against the 'date' column\n",
    "    asset_df.plot(ax=axs[0], color=[viridis.colors[0], viridis.colors[255], viridis.colors[177]])\n",
    "    industry_df.plot(ax=axs[1], color=viridis(np.linspace(0, 1, industry_df.shape[1])))\n",
    "    usage_df.plot(ax=axs[2], color=viridis(np.linspace(0, 1, usage_df.shape[1])))\n",
    "\n",
    "    # Set y-scale to logarithmic\n",
    "    axs[0].set_yscale('log')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[2].set_yscale('log')\n",
    "\n",
    "    # Remove minor ticks, add vertical grid lines, and remove x-axis labels\n",
    "    for ax in axs:\n",
    "        ax.minorticks_off()\n",
    "        ax.grid(axis='x', which='major')\n",
    "        ax.tick_params(axis='x', labelbottom=True)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_facecolor('none')\n",
    "\n",
    "    # Move legends to below the plots\n",
    "    axs[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3, frameon=False)\n",
    "    axs[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3, frameon=False)\n",
    "    axs[2].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3, frameon=False)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    fig.patch.set_visible(False)\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    PANEL_IN_FP      = '../data/clean/panel_weekly.pkl' \n",
    "    ASSET_IN_FP      = '../data/clean/asset_universe_dict.pickle'\n",
    "    MCAP_OUT_FP      = '../output/desc_stats/mcaps.png'\n",
    "    HIST_OUT_FP      = '../output/desc_stats/histograms.png'\n",
    "    CUM_RET_OUT_FP   = '../output/desc_stats/cumulative_returns.png'\n",
    "    SHARPE_OUT_FP    = '../output/desc_stats/sharpe.png'\n",
    "    TX_OUT_FP        = '../output/desc_stats/btc_tx.png'\n",
    "    HODL_OUT_FP      = '../output/desc_stats/hodl.png'\n",
    "    CORR_OUT_FP      = '../output/desc_stats/corr.png'\n",
    "    RISK_RTRN_OUT_FP = '../output/desc_stats/risk_return.png'\n",
    "    OUT_FP           = '../output/desc_stats/descriptive_statistics.xlsx'\n",
    "    PERIODS_IN_YEAR  = 52\n",
    "    LHS_COL          = 'r_ex_tp7'\n",
    "\n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # drop rows that are not in the asset universe\n",
    "    df = subsetToAssetUniverse(df, asset_universe_dict)\n",
    "\n",
    "    # generate plots\n",
    "    plotMcaps(df, MCAP_OUT_FP)\n",
    "    plotReturnHistograms(df, HIST_OUT_FP)\n",
    "    plotCumulativeReturns(df, CUM_RET_OUT_FP)\n",
    "    plotRollingSharpe(SHARPE_OUT_FP)\n",
    "    plotTransactionStats(df, TX_OUT_FP)\n",
    "    plotHodlingStats(df, HODL_OUT_FP)\n",
    "    plotRollingCorrelations(CORR_OUT_FP)\n",
    "    formRiskReturnScatterPlot(df, RISK_RTRN_OUT_FP, PERIODS_IN_YEAR)\n",
    "\n",
    "    # generate tables\n",
    "    genSummaryStatistics(df, LHS_COL, OUT_FP, PERIODS_IN_YEAR)\n",
    "    genForkStatistics(df, OUT_FP) # NOTE: run time is about 5 min.\n",
    "    genCorrStatistics(df, OUT_FP)\n",
    "    genCharDescCorrAndRegStats(df, LHS_COL, OUT_FP)\n",
    "\n",
    "    # report statistics\n",
    "    reportICOStatistics(df)\n",
    "    inflationStatistics(df, OUT_FP)\n",
    "    \n",
    "    # # TODO SCOPE IF RESULTS FOR ALL CHANGE MUCH AFTER A WINSOR\n",
    "    # p1 = df[LHS_COL].quantile(0.01)\n",
    "    # p99 = df[LHS_COL].quantile(0.99)\n",
    "    # df.loc[df[LHS_COL] < p1, LHS_COL] = p1 \n",
    "    # df.loc[df[LHS_COL] > p99, LHS_COL] = p1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
