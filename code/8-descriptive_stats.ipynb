{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a21a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "from tools import QuantTools\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8347f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniverse(df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset a DataFrame based on a dictionary of asset universes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame. Must contain columns \"date\" and \"asset\".\n",
    "    asset_universe_dict : Dict[str, List[str]]\n",
    "        A dictionary where keys are dates in 'YYYY-MM-DD' format and values are lists of asset names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The subsetted DataFrame.\n",
    "    \"\"\"\n",
    "    # Check that the required columns are present in the DataFrame\n",
    "    if not set(['date', 'asset']).issubset(df.columns):\n",
    "        raise ValueError('Input DataFrame must contain \"date\" and \"asset\" columns.')\n",
    "\n",
    "    # Ensure that the 'date' column is of datetime type\n",
    "    if df['date'].dtype != 'datetime64[ns]':\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Loop over all months with their relevant assets\n",
    "    for key, values in asset_universe_dict.items():\n",
    "        # Extract the year and month from the key\n",
    "        year, month = key.split('-')[:2]\n",
    "\n",
    "        # Drop rows from the dataframe which match the year and month but not the assets\n",
    "        df = df[~((df.date.dt.year == int(year)) \n",
    "                    & (df.date.dt.month == int(month)) \n",
    "                    & (~df.asset.isin(values)))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c9bd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotReturnHistograms(df: pd.DataFrame, out_fp: str):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame containing time series data for returns of\n",
    "    btc, eth, and the cmkt and saves a histogram plot for all three to given fp.\n",
    "    Each histogram also includes a normal distribution fit.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): A DataFrame containing columns 'date', 'asset', 'char_r_tm7', 'macro_cmkt_tm7'.\n",
    "    out_fp (str): A string specifying the filepath where the plot should be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # extract relevant returns\n",
    "    btc_df  = df[df.asset=='btc'][['date', 'char_r_tm7']]\n",
    "    btc_df  = btc_df.rename(columns={'char_r_tm7': 'btc'})\n",
    "    eth_df  = df[df.asset=='eth'][['date', 'char_r_tm7']]\n",
    "    eth_df  = eth_df.rename(columns={'char_r_tm7': 'eth'})\n",
    "    cmkt_df = df.groupby('date')[['macro_cmkt_tm7']].mean().reset_index()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'cmkt'})\n",
    "\n",
    "    # form single dataframe\n",
    "    hist_df = cmkt_df.merge(btc_df, on='date', how='inner', validate='one_to_one')\n",
    "    hist_df = hist_df.merge(eth_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    # initiate the plot with given colors and columns\n",
    "    fig, axs = plt.subplots(3, sharex=True, sharey=True, figsize=(6.4,4), facecolor='none')\n",
    "    colors = plt.get_cmap('viridis')(np.linspace(0, 10))\n",
    "    data_columns = ['cmkt', 'btc', 'eth']\n",
    "\n",
    "    # plot the data with the normal dist fit\n",
    "    for idx, ax in enumerate(axs):\n",
    "        data = hist_df[data_columns[idx]]\n",
    "        n, bins, patches = ax.hist(data, bins=30, color=colors[idx], alpha=1)\n",
    "        #  density=True, \n",
    "\n",
    "        # Fit a normal distribution\n",
    "        mu, std = norm.fit(data)\n",
    "\n",
    "        # Scale normal distribution to histogram\n",
    "        scale = n.max() / norm.pdf(mu, mu, std).max()\n",
    "        \n",
    "        # Plot the PDF\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = norm.pdf(x, mu, std) * scale\n",
    "        ax.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    # tighen up the plot\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # adjust x axis labels\n",
    "    plt.xticks(np.arange(-.5, 0.7, 0.1))\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "\n",
    "    # close the figure\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15c837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCumulativeReturns(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot the time series of cumulative returns to the given output filepath.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data\n",
    "        out_fp (str): a relative filepath to save the figure to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # initialize df with timeserieses to plot\n",
    "    plot_df = pd.DataFrame(data={'date': []})\n",
    "\n",
    "    # find all assets present in the panel\n",
    "    assets = list(np.unique(df.asset.values))\n",
    "\n",
    "    # form each asset's cumulative return\n",
    "    for asset in assets:\n",
    "        # extract asset's returns\n",
    "        temp_df = df[df.asset==asset][['date', 'char_r_tm7']]\n",
    "\n",
    "        # ensure it is sorted\n",
    "        temp_df = temp_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # form cumulative return\n",
    "        temp_df[asset] = (1 + temp_df['char_r_tm7']).cumprod()\n",
    "\n",
    "        # merge on results\n",
    "        plot_df = plot_df.merge(temp_df[['date', asset]], on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # form the cmkt return\n",
    "    temp_df = df[df.asset=='btc'][['date', 'macro_cmkt_tm7']]\n",
    "    temp_df = temp_df.sort_values(by='date', ignore_index=True)\n",
    "    temp_df['cmkt'] = (1 + temp_df['macro_cmkt_tm7']).cumprod()\n",
    "    plot_df = plot_df.merge(temp_df[['date', 'cmkt']], on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # resort\n",
    "    plot_df = plot_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # set index\n",
    "    plot_df.set_index('date', inplace=True)\n",
    "\n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(8, 5), facecolor='none')\n",
    "\n",
    "    # Form column list\n",
    "    columns = list(plot_df.columns)\n",
    "    columns.remove('btc')\n",
    "    columns.remove('eth')\n",
    "    columns.remove('cmkt')\n",
    "    columns = columns + ['eth', 'btc', 'cmkt']\n",
    "\n",
    "    # Iterate over the columns and plot each time series\n",
    "    for column in columns:\n",
    "        if column == 'btc':\n",
    "            color = '#FDE725FF'\n",
    "            linewidth = 2\n",
    "        elif column == 'eth':\n",
    "            color = '#2D708EFF'\n",
    "            linewidth = 2\n",
    "        elif column == 'cmkt':\n",
    "            color = '#482677FF'\n",
    "            linewidth = 2\n",
    "        else:\n",
    "            color = 'gray'\n",
    "            linewidth = 0.5\n",
    "        plt.plot(plot_df.index, plot_df[column], color=color, linewidth=linewidth)\n",
    "\n",
    "    # Set y-axis to logarithmic scale\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Remove y-axis minor ticks\n",
    "    plt.gca().yaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.grid(visible=True, which='major', axis='y', linewidth=0.5)\n",
    "    plt.box(False)\n",
    "\n",
    "    # Add custom labels for important time series\n",
    "    plt.text(plot_df.index[-45], plot_df['cmkt'].iloc[-52]+5, 'cmkt', color='#482677FF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "    plt.text(plot_df.index[-1], plot_df['btc'].iloc[-1]-0.1, 'btc', color='#FDE725FF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "    plt.text(plot_df.index[-1], plot_df['eth'].iloc[-1]+0.3, 'eth', color='#2D708EFF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac7195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSummaryStatistics(df: pd.DataFrame, lhs_col: str, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates summary statistics for the panel and saves them to an Excel file.\n",
    "\n",
    "    :param df: DataFrame containing asset return data.\n",
    "    :param lhs_col: Column in df that contains the return data.\n",
    "    :param out_fp: Output file path for the Excel file.\n",
    "    \"\"\"\n",
    "    # define function for calculating return statistics\n",
    "    def calcReturnStats(temp_df: pd.DataFrame, asset: str, return_col: str) -> dict:\n",
    "        mean_return = QuantTools.calcTSAvgReturn(temp_df[return_col].values, annualized=True, periods_in_year=52)\n",
    "        std_dev = QuantTools.calcSD(temp_df[return_col].values, annualized=True, periods_in_year=52)\n",
    "        sharpe_ratio = QuantTools.calcSharpe(temp_df[return_col].values, periods_in_year=52)\n",
    "        skewness = stats.skew(temp_df[return_col].values) / np.sqrt(52)\n",
    "        kurtosis = stats.kurtosis(temp_df[return_col].values) / 52\n",
    "        perc_return_above_zero = np.sum(temp_df[return_col]>0) / len(temp_df)\n",
    "        \n",
    "        return {'asset': asset,\n",
    "            'Mean': mean_return,\n",
    "            'SD': std_dev,\n",
    "            'Sharpe': sharpe_ratio,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'Pct pos': perc_return_above_zero}\n",
    "\n",
    "    # drop to only necessary columns\n",
    "    df = df[['date', 'asset', lhs_col, 'macro_snp500_t', 'char_size_t', 'char_volume_sum_tm7']].copy()\n",
    "\n",
    "    # form btc and eth returns\n",
    "    btc_df = df[df.asset=='btc'].set_index('date')[[lhs_col]]\n",
    "    eth_df = df[df.asset=='eth'].set_index('date')[[lhs_col]]\n",
    "\n",
    "    # form cmkt return\n",
    "    df['weighted_return'] = df[lhs_col] * df['char_size_t']\n",
    "    total_market_cap = df.groupby('date')['char_size_t'].sum()\n",
    "    cmkt_df = df.groupby('date')['weighted_return'].sum() / total_market_cap\n",
    "    cmkt_df = pd.DataFrame(cmkt_df).rename(columns={0: 'return'})\n",
    "\n",
    "    # obtain nasdaq weekly return\n",
    "    nsdq_df = yf.Ticker('^IXIC').history(period='1d', start='2017-12-29', end='2022-12-31').reset_index()\n",
    "    nsdq_df['Date'] = pd.to_datetime(nsdq_df.Date).to_numpy(dtype='datetime64[D]')\n",
    "    nsdq_df = nsdq_df[['Date', 'Close']].rename(columns={'Date': 'date', 'Close': 'return'}).set_index('date')\n",
    "    nsdq_df = nsdq_df.resample('W').last().pct_change().dropna()\n",
    "\n",
    "    # calc return statistics\n",
    "    cmkt_stats = calcReturnStats(cmkt_df, 'CMKT', 'return')\n",
    "    btc_stats  = calcReturnStats(btc_df, 'Bitcoin', lhs_col)\n",
    "    eth_stats  = calcReturnStats(eth_df, 'Ethereum', lhs_col)\n",
    "    nsdq_stats = calcReturnStats(nsdq_df, 'Nasdaq', 'return')\n",
    "    ret_df = pd.DataFrame([cmkt_stats, btc_stats, eth_stats, nsdq_stats])\n",
    "\n",
    "    # calc extreme event statistics\n",
    "    ext_data = {'threshold': [], 'count': [], 'percent': []}\n",
    "    num_obs  = len(cmkt_df)\n",
    "    for threshold in [-.3, -.2, -.1, -.05, .05, .1, .2, .3]:\n",
    "        ext_data['threshold'].append(threshold)\n",
    "        if threshold < 0:\n",
    "            count = (cmkt_df['return'] < threshold).sum()\n",
    "            ext_data['count'].append(count)\n",
    "            ext_data['percent'].append(count / num_obs)\n",
    "        else:\n",
    "            count = (cmkt_df['return'] > threshold).sum()\n",
    "            ext_data['count'].append(count)\n",
    "            ext_data['percent'].append(count / num_obs)\n",
    "    ext_df = pd.DataFrame(ext_data)\n",
    "\n",
    "    # calculate yearly stats of unique assets and median mcap and volume\n",
    "    df['year'] = df['date'].dt.year\n",
    "    yr_df = pd.DataFrame({\n",
    "        'num_unique_assets': df.groupby(['year'])['asset'].nunique(),\n",
    "        'median_market_cap': df.groupby(['year'])['char_size_t'].median(),\n",
    "        'median_weekly_asset_volume': df.groupby(['year'])['char_volume_sum_tm7'].median()}).reset_index()\n",
    "    all_df = pd.DataFrame({\n",
    "        'num_unique_assets': [df['asset'].nunique()],\n",
    "        'median_market_cap': [df['char_size_t'].median()],\n",
    "        'median_weekly_asset_volume': [df['char_volume_sum_tm7'].median()]})\n",
    "    all_df['year'] = 'all'\n",
    "    yr_df = pd.concat([yr_df, all_df])\n",
    "\n",
    "    # calculate the total mcap in the last week of each year\n",
    "    max_dates = df.groupby('year')['date'].max()\n",
    "    filtered_df = df[df['date'].isin(max_dates)]\n",
    "    total_mcap_by_year = filtered_df.groupby('year')[['char_size_t']].sum().reset_index()\n",
    "    yr_df = yr_df.merge(total_mcap_by_year, on='year', how='outer', validate='one_to_one')\n",
    "\n",
    "    # extract yearly returns\n",
    "    cmkt_df = cmkt_df.reset_index()\n",
    "    cmkt_df['year'] = cmkt_df.date.dt.year\n",
    "    for year in [2018, 2019, 2020, 2021, 2022]:\n",
    "        yr_df.loc[yr_df.year==year, 'cmkt_ret'] = ((cmkt_df[cmkt_df.year==year]['return']+1).cumprod()-1).values[-1]\n",
    "    yr_df.loc[yr_df.year=='all', 'cmkt_ret'] = ((cmkt_df['return']+1).cumprod()-1).values[-1]\n",
    "\n",
    "    # save results\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        ret_df.to_excel(writer, sheet_name='raw_ret_stats')\n",
    "        ext_df.to_excel(writer, sheet_name='raw_extreme_stats')\n",
    "        yr_df.to_excel(writer, sheet_name='raw_yearly_stats')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "093d7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    PANEL_IN_FP     = '../data/clean/panel_weekly.pkl' \n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    HIST_OUT_FP     = '../output/desc_stats/histograms.png'\n",
    "    CUM_RET_OUT_FP  = '../output/desc_stats/cumulative_returns.png'\n",
    "    OUT_FP          = '../output/desc_stats/descriptive_statistics.xlsx'\n",
    "    PERIODS_IN_YEAR = 52\n",
    "    TS_AVG_METHOD   = 'arithmetic'\n",
    "    LHS_COL         = 'r_ex_tp7'\n",
    "    ANNUALIZED      = False\n",
    "\n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # drop rows that are not in the asset universe\n",
    "    df = subsetToAssetUniverse(df, asset_universe_dict)\n",
    "\n",
    "    # generate plots\n",
    "    plotReturnHistograms(df, HIST_OUT_FP)\n",
    "    plotCumulativeReturns(df, CUM_RET_OUT_FP)\n",
    "\n",
    "    # generate tables\n",
    "    genSummaryStatistics(df, LHS_COL, OUT_FP)\n",
    "\n",
    "    # # TODO SCOPE IF RESULTS FOR ALL CHANGE MUCH AFTER A WINSOR\n",
    "    # p1 = df[LHS_COL].quantile(0.01)\n",
    "    # p99 = df[LHS_COL].quantile(0.99)\n",
    "    # df.loc[df[LHS_COL] < p1, LHS_COL] = p1 \n",
    "    # df.loc[df[LHS_COL] > p99, LHS_COL] = p1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
