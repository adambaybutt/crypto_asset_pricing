{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d8a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO NOTEBOOK IMPORTING AND FORMING OF DATA NEEDS TO BE CLEANED\n",
    "# WAY MESSY TO SAVE TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a21a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# Import packages\n",
    "from joblib import Parallel, delayed\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from typing import Dict, List\n",
    "from tools import QuantTools\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# set color map\n",
    "viridis = matplotlib.colormaps['viridis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9f2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importYahoo(ticker: str, start_date: str, end_date: str, rf_df: pd.DataFrame, \n",
    "            new_ret_col: str, resample_freq: str='W', rf_col: str='r_rf_tm7') -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Import Yahoo Finance data for given ticker, time period, taking our risk free rate in rf_df. \n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): The ticker symbol to get data for.\n",
    "    start_date (str): The start date of the data retrieval period.\n",
    "    end_date (str): The end date of the data retrieval period.\n",
    "    rf_df (DataFrame): A DataFrame containing risk-free rates.\n",
    "    new_ret_col (str): The new column name for returns.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the date and return data.\n",
    "\n",
    "    Note: this is done at weekly frequency; would need adjustment for different.\n",
    "    \"\"\"\n",
    "    # import the data\n",
    "    df = yf.Ticker(ticker).history(period='1d', start=start_date, end=end_date).reset_index()\n",
    "    \n",
    "    # reformat\n",
    "    df['Date'] = pd.to_datetime(df.Date).to_numpy(dtype='datetime64[D]')\n",
    "    df = df[['Date', 'Close']].rename(columns={'Date': 'date', 'Close': new_ret_col}).set_index('date')\n",
    "    df = df.resample(resample_freq).last().pct_change().dropna()\n",
    "\n",
    "    # adjust if resampling monthly\n",
    "    if resample_freq == 'M':\n",
    "        df.index = df.index + pd.Timedelta(days=1)\n",
    "\n",
    "    # take out rf rate\n",
    "    df = df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    df[new_ret_col] = df[new_ret_col] - df[rf_col]\n",
    "\n",
    "    # check for NaN values in new_ret_col\n",
    "    if df[new_ret_col].isnull().values.any():\n",
    "        print(\"Warning: NaN values found in return data\")\n",
    "    \n",
    "    return df[['date', new_ret_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8347f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniverse(df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset a DataFrame based on a dictionary of asset universes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame. Must contain columns \"date\" and \"asset\".\n",
    "    asset_universe_dict : Dict[str, List[str]]\n",
    "        A dictionary where keys are dates in 'YYYY-MM-DD' format and values are lists of asset names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The subsetted DataFrame.\n",
    "    \"\"\"\n",
    "    # Check that the required columns are present in the DataFrame\n",
    "    if not set(['date', 'asset']).issubset(df.columns):\n",
    "        raise ValueError('Input DataFrame must contain \"date\" and \"asset\" columns.')\n",
    "\n",
    "    # Ensure that the 'date' column is of datetime type\n",
    "    if df['date'].dtype != 'datetime64[ns]':\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Loop over all months with their relevant assets\n",
    "    for key, values in asset_universe_dict.items():\n",
    "        # Extract the year and month from the key\n",
    "        year, month = key.split('-')[:2]\n",
    "\n",
    "        # Drop rows from the dataframe which match the year and month but not the assets\n",
    "        df = df[~((df.date.dt.year == int(year)) \n",
    "                    & (df.date.dt.month == int(month)) \n",
    "                    & (~df.asset.isin(values)))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9bd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotReturnHistograms(df: pd.DataFrame, out_fp: str):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame containing time series data for returns of\n",
    "    btc, eth, and the cmkt and saves a histogram plot for all three to given fp.\n",
    "    Each histogram also includes a normal distribution fit.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): A DataFrame containing columns 'date', 'asset', 'char_r_tm7', 'macro_cmkt_tm7'.\n",
    "    out_fp (str): A string specifying the filepath where the plot should be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # extract relevant returns\n",
    "    btc_df  = df[df.asset=='btc'][['date', 'char_r_tm7']]\n",
    "    btc_df  = btc_df.rename(columns={'char_r_tm7': 'btc'})\n",
    "    eth_df  = df[df.asset=='eth'][['date', 'char_r_tm7']]\n",
    "    eth_df  = eth_df.rename(columns={'char_r_tm7': 'eth'})\n",
    "    cmkt_df = df.groupby('date')[['macro_cmkt_tm7']].mean().reset_index()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'cmkt'})\n",
    "\n",
    "    # form single dataframe\n",
    "    hist_df = cmkt_df.merge(btc_df, on='date', how='inner', validate='one_to_one')\n",
    "    hist_df = hist_df.merge(eth_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    # initiate the plot with given colors and columns\n",
    "    fig, axs = plt.subplots(3, sharex=True, sharey=True, figsize=(6.4,4), facecolor='none')\n",
    "    colors = plt.get_cmap('viridis')(np.linspace(0, 10))\n",
    "    data_columns = ['cmkt', 'btc', 'eth']\n",
    "\n",
    "    # plot the data with the normal dist fit\n",
    "    for idx, ax in enumerate(axs):\n",
    "        data = hist_df[data_columns[idx]]\n",
    "        n, bins, patches = ax.hist(data, bins=30, color=colors[idx], alpha=1)\n",
    "        #  density=True, \n",
    "\n",
    "        # Fit a normal distribution\n",
    "        mu, std = norm.fit(data)\n",
    "\n",
    "        # Scale normal distribution to histogram\n",
    "        scale = n.max() / norm.pdf(mu, mu, std).max()\n",
    "        \n",
    "        # Plot the PDF\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = norm.pdf(x, mu, std) * scale\n",
    "        ax.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    # tighen up the plot\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # adjust x axis labels\n",
    "    plt.xticks(np.arange(-.5, 0.7, 0.1))\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "\n",
    "    # close the figure\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15c837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCumulativeReturns(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot the time series of cumulative returns to the given output filepath.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data\n",
    "        out_fp (str): a relative filepath to save the figure to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # initialize df with timeserieses to plot\n",
    "    plot_df = pd.DataFrame(data={'date': []})\n",
    "\n",
    "    # find all assets present in the panel\n",
    "    assets = list(np.unique(df.asset.values))\n",
    "\n",
    "    # form each asset's cumulative return\n",
    "    for asset in assets:\n",
    "        # extract asset's returns\n",
    "        temp_df = df[df.asset==asset][['date', 'char_r_tm7']]\n",
    "\n",
    "        # ensure it is sorted\n",
    "        temp_df = temp_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # form cumulative return\n",
    "        temp_df[asset] = (1 + temp_df['char_r_tm7']).cumprod()\n",
    "\n",
    "        # merge on results\n",
    "        plot_df = plot_df.merge(temp_df[['date', asset]], on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # form the cmkt return\n",
    "    temp_df = df[df.asset=='btc'][['date', 'macro_cmkt_tm7']]\n",
    "    temp_df = temp_df.sort_values(by='date', ignore_index=True)\n",
    "    temp_df['cmkt'] = (1 + temp_df['macro_cmkt_tm7']).cumprod()\n",
    "    plot_df = plot_df.merge(temp_df[['date', 'cmkt']], on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # resort\n",
    "    plot_df = plot_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # set index\n",
    "    plot_df.set_index('date', inplace=True)\n",
    "\n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(4*1.61, 4), facecolor='none')\n",
    "\n",
    "    # Form column list\n",
    "    columns = list(plot_df.columns)\n",
    "    columns.remove('btc')\n",
    "    columns.remove('eth')\n",
    "    columns.remove('cmkt')\n",
    "    columns = columns + ['eth', 'btc', 'cmkt']\n",
    "\n",
    "    # Iterate over the columns and plot each time series\n",
    "    for column in columns:\n",
    "        if column == 'btc':\n",
    "            color = '#FDE725FF'\n",
    "            linewidth = 2\n",
    "        elif column == 'eth':\n",
    "            color = '#2D708EFF'\n",
    "            linewidth = 2\n",
    "        elif column == 'cmkt':\n",
    "            color = '#482677FF'\n",
    "            linewidth = 2\n",
    "        else:\n",
    "            color = 'gray'\n",
    "            linewidth = 0.5\n",
    "        plt.plot(plot_df.index, plot_df[column], color=color, linewidth=linewidth)\n",
    "\n",
    "    # Set y-axis to logarithmic scale\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Remove y-axis minor ticks\n",
    "    plt.gca().yaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.grid(visible=True, which='major', axis='y', linewidth=0.5)\n",
    "    plt.box(False)\n",
    "\n",
    "    # Add custom labels for important time series\n",
    "    plt.text(plot_df.index[-45], plot_df['cmkt'].iloc[-52]+5, 'cmkt', color='#482677FF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "    plt.text(plot_df.index[-1], plot_df['btc'].iloc[-1]-0.1, 'btc', color='#FDE725FF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "    plt.text(plot_df.index[-1], plot_df['eth'].iloc[-1]+0.3, 'eth', color='#2D708EFF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac7195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSummaryStatistics(df: pd.DataFrame, lhs_col: str, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates summary statistics for the panel and saves them to an Excel file.\n",
    "\n",
    "    :param df: DataFrame containing asset return data.\n",
    "    :param lhs_col: Column in df that contains the return data.\n",
    "    :param out_fp: Output file path for the Excel file.\n",
    "    \"\"\"\n",
    "    # define function for calculating return statistics\n",
    "    def calcReturnStats(temp_df: pd.DataFrame, asset: str, return_col: str) -> dict:\n",
    "        mean_return = QuantTools.calcTSAvgReturn(temp_df[return_col].values, annualized=True, periods_in_year=52)\n",
    "        std_dev = QuantTools.calcSD(temp_df[return_col].values, annualized=True, periods_in_year=52)\n",
    "        sharpe_ratio = QuantTools.calcSharpe(temp_df[return_col].values, periods_in_year=52)\n",
    "        skewness = stats.skew(temp_df[return_col].values) / np.sqrt(52)\n",
    "        kurtosis = stats.kurtosis(temp_df[return_col].values) / 52\n",
    "        perc_return_above_zero = np.sum(temp_df[return_col]>0) / len(temp_df)\n",
    "        \n",
    "        return {'asset': asset,\n",
    "            'Mean': mean_return,\n",
    "            'SD': std_dev,\n",
    "            'Sharpe': sharpe_ratio,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'Pct pos': perc_return_above_zero}\n",
    "\n",
    "    # drop to only necessary columns\n",
    "    df = df[['date', 'asset', lhs_col, 'char_size_t', 'char_volume_sum_tm7', \n",
    "        'macro_snp500_t', 'macro_dgs1mo_t']].copy()\n",
    "\n",
    "    # form btc and eth returns\n",
    "    btc_df = df[df.asset=='btc'].set_index('date')[[lhs_col]]\n",
    "    eth_df = df[df.asset=='eth'].set_index('date')[[lhs_col]]\n",
    "\n",
    "    # form cmkt return\n",
    "    df['weighted_return'] = df[lhs_col] * df['char_size_t']\n",
    "    total_market_cap = df.groupby('date')['char_size_t'].sum()\n",
    "    cmkt_df = df.groupby('date')['weighted_return'].sum() / total_market_cap\n",
    "    cmkt_df = pd.DataFrame(cmkt_df).rename(columns={0: 'return'})\n",
    "\n",
    "    # import nasdaq data and take out risk free rate\n",
    "    rf_df = df[['date', 'macro_dgs1mo_t']].drop_duplicates()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.macro_dgs1mo_t.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    nsdq_df = importYahoo('^IXIC', '2017-12-29', '2022-12-31', rf_df, 'r_nsdq_tm7')    \n",
    "\n",
    "    # calc return statistics\n",
    "    cmkt_stats = calcReturnStats(cmkt_df, 'CMKT', 'return')\n",
    "    btc_stats  = calcReturnStats(btc_df, 'Bitcoin', lhs_col)\n",
    "    eth_stats  = calcReturnStats(eth_df, 'Ethereum', lhs_col)\n",
    "    nsdq_stats = calcReturnStats(nsdq_df, 'Nasdaq', 'r_nsdq_tm7')\n",
    "    ret_df = pd.DataFrame([cmkt_stats, btc_stats, eth_stats, nsdq_stats])\n",
    "    \n",
    "    # calc extreme event statistics\n",
    "    ext_data = {'threshold': [], 'count': [], 'percent': []}\n",
    "    num_obs  = len(cmkt_df)\n",
    "    for threshold in [-.3, -.2, -.1, -.05, .05, .1, .2, .3]:\n",
    "        ext_data['threshold'].append(threshold)\n",
    "        if threshold < 0:\n",
    "            count = (cmkt_df['return'] < threshold).sum()\n",
    "            ext_data['count'].append(count)\n",
    "            ext_data['percent'].append(count / num_obs)\n",
    "        else:\n",
    "            count = (cmkt_df['return'] > threshold).sum()\n",
    "            ext_data['count'].append(count)\n",
    "            ext_data['percent'].append(count / num_obs)\n",
    "    ext_df = pd.DataFrame(ext_data)\n",
    "    \n",
    "    # calculate yearly stats of unique assets and median mcap and volume\n",
    "    df['year'] = df['date'].dt.year\n",
    "    yr_df = pd.DataFrame({\n",
    "        'num_unique_assets': df.groupby(['year'])['asset'].nunique(),\n",
    "        'median_market_cap': df.groupby(['year'])['char_size_t'].median(),\n",
    "        'median_weekly_asset_volume': df.groupby(['year'])['char_volume_sum_tm7'].median()}).reset_index()\n",
    "    all_df = pd.DataFrame({\n",
    "        'num_unique_assets': [df['asset'].nunique()],\n",
    "        'median_market_cap': [df['char_size_t'].median()],\n",
    "        'median_weekly_asset_volume': [df['char_volume_sum_tm7'].median()]})\n",
    "    all_df['year'] = 'all'\n",
    "    yr_df = pd.concat([yr_df, all_df])\n",
    "\n",
    "    # calculate the total mcap in the last week of each year\n",
    "    max_dates = df.groupby('year')['date'].max()\n",
    "    filtered_df = df[df['date'].isin(max_dates)]\n",
    "    total_mcap_by_year = filtered_df.groupby('year')[['char_size_t']].sum().reset_index()\n",
    "    yr_df = yr_df.merge(total_mcap_by_year, on='year', how='outer', validate='one_to_one')\n",
    "\n",
    "    # extract yearly returns\n",
    "    cmkt_df = cmkt_df.reset_index()\n",
    "    cmkt_df['year'] = cmkt_df.date.dt.year\n",
    "    for year in [2018, 2019, 2020, 2021, 2022]:\n",
    "        yr_df.loc[yr_df.year==year, 'cmkt_ret'] = ((cmkt_df[cmkt_df.year==year]['return']+1).cumprod()-1).values[-1]\n",
    "    yr_df.loc[yr_df.year=='all', 'cmkt_ret'] = ((cmkt_df['return']+1).cumprod()-1).values[-1]\n",
    "    \n",
    "    # save results\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        ret_df.to_excel(writer, sheet_name='raw_ret_stats')\n",
    "        ext_df.to_excel(writer, sheet_name='raw_extreme_stats')\n",
    "        yr_df.to_excel(writer, sheet_name='raw_yearly_stats')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb9e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRollingSharpe(out_fp: str) -> None:\n",
    "    \"\"\" Plot rolling four year sharpe ratio with new data for the study period. \"\"\"\n",
    "    \n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-31').reset_index()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf_tm7'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "\n",
    "    # import other asset class data\n",
    "    start_date = '2013-12-29'\n",
    "    end_date   = '2022-12-31'\n",
    "    nsdq_df = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq')\n",
    "    snp_df  = importYahoo('^GSPC', start_date, end_date, rf_df, 'SnP 500')\n",
    "    vt_df   = importYahoo('VT', start_date, end_date, rf_df, 'Global Stocks')\n",
    "    bnd_df  = importYahoo('BND', start_date, end_date, rf_df, 'US Bonds')\n",
    "    real_df = importYahoo('VNQ', start_date, end_date, rf_df, 'US Real Estate')\n",
    "    emrg_df = importYahoo('EBND', start_date, end_date, rf_df, 'Emerging Currencies')\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold')\n",
    "\n",
    "    # import the btc data and form weekly excess returns\n",
    "    btc_df = pd.read_csv('../data/raw/XBTUSD_1440.csv',\n",
    "                        header=None)\n",
    "    btc_df[0] = pd.to_datetime(btc_df[0], unit='s')\n",
    "    btc_df = btc_df[[0, 4]]\n",
    "    btc_df = btc_df.rename(columns={0: 'date', 4: 'price'})\n",
    "    btc_df.set_index('date', inplace=True)\n",
    "    btc_df = btc_df.resample('W').last()\n",
    "    btc_df['Bitcoin'] = btc_df['price'].pct_change()\n",
    "    btc_df = btc_df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'] - btc_df.r_rf_tm7\n",
    "    btc_df = btc_df.drop(['price', 'r_rf_tm7'], axis=1)\n",
    "\n",
    "    # form single dataframe\n",
    "    df = btc_df.merge(nsdq_df, on='date', how='inner', validate='one_to_one')\n",
    "    for temp_df in [snp_df, vt_df, bnd_df, real_df, emrg_df, gld_df]:\n",
    "        df = df.merge(temp_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    # form sharpe ratios\n",
    "    window_size = 208\n",
    "    df.set_index('date', inplace=True)\n",
    "    columns = list(df.columns.values)\n",
    "    for col in columns:\n",
    "        df[col] = np.sqrt(52) * df[col].rolling(window_size).mean() / df[col].rolling(window_size).std()\n",
    "\n",
    "    # subset to relevant time period\n",
    "    df = df[(df.index.year >= 2018)\n",
    "        & (df.index.year <= 2022)]\n",
    "\n",
    "    # form sharpe ratio plot\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    df.plot(cmap='viridis')\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    #plt.title('Sharpe Ratios: Bitcoin vs Major Asset Classes$^{12}$')\n",
    "    plt.legend(labels=df.columns.values, \n",
    "                loc='lower center',\n",
    "                ncol=3,\n",
    "                bbox_to_anchor=(0.5, -0.37),\n",
    "                frameon=False)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35aee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTransactionStats(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Plot rolling transaction statistics for Bitcoin. \"\"\"\n",
    "    # Extract relevant transaction data\n",
    "    tx_df = df[['date', 'macro_btc_fee_med_usd_t', 'macro_btc_tx_tfr_val_adj_usd_t']].drop_duplicates().copy()\n",
    "\n",
    "    # reformat the data\n",
    "    tx_df.set_index('date', inplace=True)\n",
    "    temp1_df = tx_df[['macro_btc_tx_tfr_val_adj_usd_t']].resample('M').sum()\n",
    "    temp2_df = tx_df[['macro_btc_fee_med_usd_t']].resample('M').median()\n",
    "    r_df = temp1_df.merge(temp2_df, how='inner', left_index=True, right_index=True, validate='one_to_one')\n",
    "    r_df = r_df.rename(columns={'macro_btc_fee_med_usd_t': 'Median Fee (USD)',\n",
    "                                'macro_btc_tx_tfr_val_adj_usd_t': 'Monthly Volume (USD)'})\n",
    "    r_df['date'] = r_df.index\n",
    "    r_df['date'] = r_df.date.dt.strftime('%Y-%m')\n",
    "    r_df = r_df.set_index('date')\n",
    "\n",
    "    # plot the data\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    r_df.plot(color=[viridis.colors[0], viridis.colors[111]])\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    plt.legend(labels=r_df.columns.values, \n",
    "            loc='lower center',\n",
    "            ncol=3,\n",
    "            bbox_to_anchor=(0.5, -0.22),\n",
    "            frameon=False)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.xticks(np.array([0,12,24,36,48]), \n",
    "            ['2018', '2019', '2020', '2021', '2022'],\n",
    "            rotation=0, ha='center')\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a09b4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHodlingStats(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    # form the hodl data\n",
    "    utxo_df = df[['date', 'macro_btc_utxo_age_med_t']].copy()\n",
    "    utxo_df = utxo_df.drop_duplicates()\n",
    "    utxo_df = utxo_df.set_index('date')\n",
    "    utxo_df = utxo_df.rename(columns={'macro_btc_utxo_age_med_t':\n",
    "                                    'UTXO Median Age (Days)'})\n",
    "\n",
    "    # Plot the hodl data\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    utxo_df.plot(color=[viridis.colors[0]],\n",
    "                legend=None)\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518f2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genForkStatistics(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Generate event studies for BTC fork dates. \"\"\"\n",
    "    # PARAMETERS\n",
    "    num_bs_samples = int(1e6)\n",
    "    num_cpus       = 22\n",
    "    window_size    = 8\n",
    "\n",
    "    # DEFINE RELEVANT FORKS\n",
    "    forks = {'bitcoin-21': datetime.datetime(2016,4,17),\n",
    "            'zcash': datetime.datetime(2016,10,28),\n",
    "            'bitcoin-cash': datetime.datetime(2017,7,31),\n",
    "            'bitcoin-gold': datetime.datetime(2017,10,24),\n",
    "            'bitcoin-diamond': datetime.datetime(2017,11,24),\n",
    "            'lightning-bitcoin': datetime.datetime(2017,12,18),\n",
    "            'bitcoinfast': datetime.datetime(2017,12,26),\n",
    "            'bitcoin2': datetime.datetime(2017,12,28),\n",
    "            'bitcoin-plus': datetime.datetime(2018,1,2),\n",
    "            'bitcoin-interest': datetime.datetime(2018,1,22),\n",
    "            'bitcoin-atom': datetime.datetime(2018,1,24),\n",
    "            'bitcoin-private': datetime.datetime(2018,2,28),\n",
    "            'microbitcoin': datetime.datetime(2018,5,29),\n",
    "            'bitcoin-bep2': datetime.datetime(2018,6,29),\n",
    "            'bitcoin-sv': datetime.datetime(2018,11,11)}\n",
    "\n",
    "    # PULL IN OLD DATA TO BUILD RELEVANT TIMESERIES FOR BTC\n",
    "    df = pd.read_csv('../data/raw/cmc_price_vol_mcap_panel.csv')\n",
    "    df = df[df.cmc_id==1]\n",
    "    df = df[['date', 'usd_per_token', 'usd_volume_24h']]\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "    df = df.drop_duplicates(subset='date')\n",
    "    df = df.reset_index(drop=True)\n",
    "    san_df = pd.read_pickle('../data/raw/santiment_panel.pkl')\n",
    "    san_df = san_df[san_df.san_slug=='bitcoin'][['date', 'active_addresses_24h', 'github_activity',\n",
    "                                                'social_volume_total']].reset_index(drop=True)\n",
    "    hash_df = pd.read_excel('../data/raw/coinmetrics_btc_hashrate.xlsx')\n",
    "    hash_df = hash_df.rename(columns={'Time': 'date',\n",
    "                                    'BTC / Mean Hash Rate': 'hash_rate'})\n",
    "    hash_df['date'] = pd.to_datetime(hash_df['date'])\n",
    "    df = df.merge(san_df,\n",
    "                on=['date'],\n",
    "                how='left',\n",
    "                validate='one_to_one')\n",
    "    df = df.merge(hash_df,\n",
    "                on='date',\n",
    "                how='left',\n",
    "                validate='one_to_one')\n",
    "    del san_df, hash_df\n",
    "\n",
    "    # CLEAN UP THE PANEL\n",
    "\n",
    "    # ensure it has all days\n",
    "    sdate   = datetime.date(2015, 1, 1) \n",
    "    edate   = datetime.date(2021, 12, 31)  \n",
    "    delta   = edate - sdate \n",
    "    days    = []\n",
    "    for i in range(delta.days + 1):\n",
    "        days.append(sdate+timedelta(days=i))\n",
    "    days_df = pd.DataFrame(data={'date':days})\n",
    "    df['date'] = df.date.dt.date\n",
    "    df      = df.merge(days_df,\n",
    "                    on='date',\n",
    "                    how='outer',\n",
    "                    validate='one_to_one')\n",
    "    df = df.sort_values('date')\n",
    "    df = df.interpolate()\n",
    "\n",
    "    # form pct change in all columns and clean up improper values\n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    for col in cols:\n",
    "        df[col] = df[col].pct_change()\n",
    "    df = df.dropna()\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # subset to time period of interest \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df[df.date.dt.year <= 2021]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # rename columns\n",
    "    df = df.rename(columns = {'usd_per_token': 'return',\n",
    "                            'usd_volume_24h': 'usd_volume',\n",
    "                            'active_addresses_24h': 'active_addresses',\n",
    "                            'github_activity': 'developer_activity',\n",
    "                            'social_volume_total': 'social_volume'})\n",
    "\n",
    "    # INITIALIZE RESULTS \n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    results_df = pd.DataFrame(data={'stat': cols,\n",
    "                                    'window': np.repeat('7 days', len(cols)),\n",
    "                                    'est': np.zeros(len(cols)),\n",
    "                                    'se': np.zeros(len(cols))})\n",
    "\n",
    "    # CALC STAT\n",
    "    for col in cols:\n",
    "        diffs = []\n",
    "        for i in range(len(forks)):\n",
    "            fork_date = list(forks.values())[i]\n",
    "            pre_date  = fork_date-pd.Timedelta(window_size, unit=\"d\")\n",
    "            post_date = fork_date+pd.Timedelta(window_size, unit=\"d\")\n",
    "            pre_mean  = np.mean(df[(df.date >= pre_date) & (df.date < fork_date)][col])\n",
    "            post_mean = np.mean(df[(df.date > fork_date) & (df.date <= post_date)][col])\n",
    "            diff      = post_mean-pre_mean\n",
    "            diffs.append(diff)\n",
    "        results_df.loc[results_df.stat==col, 'est'] = np.mean(diffs)\n",
    "\n",
    "    # CALC STANDARD ERROR (NOTE: 6 MIN RUN TIME)\n",
    "    num_forks = len(forks)\n",
    "    rel_dates = list(df.date.values)[window_size:-window_size]\n",
    "    for col in cols:\n",
    "        # calc all diffs across the panel\n",
    "        diffs = []\n",
    "        for date in rel_dates:\n",
    "            pre_date  = date-pd.Timedelta(31, unit=\"d\")\n",
    "            post_date = date+pd.Timedelta(31, unit=\"d\")\n",
    "            pre_mean  = np.mean(df[(df.date >= pre_date) & (df.date < date)][col])\n",
    "            post_mean = np.mean(df[(df.date > date) & (df.date <= post_date)][col])\n",
    "            diff      = post_mean-pre_mean\n",
    "            diffs.append(diff)\n",
    "\n",
    "        # reorder the list to randomize\n",
    "        random.shuffle(diffs)\n",
    "\n",
    "        # calc bootstrap distribution\n",
    "        def loopOverNumberBootstrapSamples(i):\n",
    "            random_diffs = []\n",
    "            for j in range(num_forks):\n",
    "                index = np.random.randint(low=0,high=len(diffs))\n",
    "                diff  = diffs[index]\n",
    "                random_diffs.append(diff)\n",
    "            return np.mean(random_diffs)\n",
    "        bs_stat = Parallel(n_jobs=num_cpus)(delayed(loopOverNumberBootstrapSamples)(i) for i in range(num_bs_samples))\n",
    "\n",
    "        # calc standard error and add it to results\n",
    "        se = np.std(bs_stat)\n",
    "        results_df.loc[results_df.stat==col, 'se'] = se\n",
    "\n",
    "    # OUTPUT\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        results_df.to_excel(writer, sheet_name='raw_forks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f07237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genCorrStatistics(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Build table of correlation statistics. \"\"\"\n",
    "    # Set dates\n",
    "    start_date = '2013-12-01'\n",
    "    end_date   = '2023-07-01'\n",
    "\n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-31').reset_index()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf_tm7'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "\n",
    "    # Import other asset class data\n",
    "    nsdq_df = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq')\n",
    "    snp_df  = importYahoo('^GSPC', start_date, end_date, rf_df, 'SnP 500')\n",
    "    rus_df  = importYahoo('^RUT', start_date, end_date, rf_df, 'Russell 2000')\n",
    "    vt_df   = importYahoo('VT', start_date, end_date, rf_df, 'Global Stocks')\n",
    "    bnd_df  = importYahoo('BND', start_date, end_date, rf_df, 'US Bonds')\n",
    "    bndx_df = importYahoo('BNDX', start_date, end_date, rf_df, 'Ex-US Global Bonds')\n",
    "    real_df = importYahoo('VNQ', start_date, end_date, rf_df, 'US Real Estate')\n",
    "    emrg_df = importYahoo('EBND', start_date, end_date, rf_df, 'Emerging Currencies')\n",
    "    dbc_df  = importYahoo('DBC', start_date, end_date, rf_df, 'Commodities')\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold')\n",
    "\n",
    "    # Form crypto time series\n",
    "    btc_df = df[df.asset=='btc'][['date', 'char_r_tm7']].reset_index(drop=True).copy()\n",
    "    btc_df = btc_df.rename(columns={'char_r_tm7': 'Bitcoin'})\n",
    "\n",
    "    eth_df = df[df.asset=='eth'][['date', 'char_r_tm7']].reset_index(drop=True).copy()\n",
    "    eth_df = eth_df.rename(columns={'char_r_tm7': 'Ethereum'})\n",
    "\n",
    "    cmkt_df = df.groupby('date')[['macro_cmkt_tm7']].mean().reset_index()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'Crypto Market'})\n",
    "\n",
    "\n",
    "    # form single dataframe\n",
    "    t_df = cmkt_df.merge(btc_df, on='date', how='inner', validate='one_to_one')\n",
    "    for temp_df in [eth_df, nsdq_df, snp_df, rus_df, vt_df, bnd_df, bndx_df, real_df, emrg_df, dbc_df, gld_df]:\n",
    "        t_df = t_df.merge(temp_df, on='date', how='inner', validate='one_to_one')\n",
    "    t_df = t_df.set_index('date')\n",
    "\n",
    "    # Export\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        t_df[(t_df.index >= '2018-01-01') \n",
    "            & (t_df.index <= '2022-12-31')].corr().to_excel(writer, sheet_name='raw_corr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef6f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRollingCorrelations(out_fp: str) -> None:\n",
    "    \"\"\" Plot figure out rolling correlations between BTC and other asset classes. \"\"\"\n",
    "    # Parameters\n",
    "    start_date = '2013-11-01'\n",
    "    end_date   = '2023-07-02'\n",
    "    rolling_window = 48\n",
    "\n",
    "    # Pull and form exp inf data\n",
    "    ei_df = pdr.DataReader('EXPINF1YR', 'fred', start_date).reset_index()\n",
    "    ei_df = ei_df.rename(columns={'DATE': 'date'})\n",
    "    ei_df = ei_df.set_index('date')\n",
    "    ei_df = ei_df.pct_change().dropna().reset_index()\n",
    "    ei_df = ei_df[(ei_df.date >= '2014-01-01') & (ei_df.date <= '2023-07-01')]\n",
    "\n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-15').reset_index()\n",
    "    rf_df['r_rf'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / 12) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "    rf_df = rf_df[(rf_df.date >= '2014-01-01') & (rf_df.date <= '2023-07-01')]\n",
    "\n",
    "    # Import other asset class data\n",
    "    nsdq_df = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq', 'M', 'r_rf')\n",
    "    bnd_df  = importYahoo('BND', start_date, end_date, rf_df, 'US Bonds', 'M', 'r_rf')\n",
    "    real_df = importYahoo('VNQ', start_date, end_date, rf_df, 'US Real Estate', 'M', 'r_rf')\n",
    "    emrg_df = importYahoo('EBND', start_date, end_date, rf_df, 'Emerging Currencies', 'M', 'r_rf')\n",
    "    dbc_df  = importYahoo('DBC', start_date, end_date, rf_df, 'Commodities', 'M', 'r_rf')\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold', 'M', 'r_rf')\n",
    "\n",
    "    # import the btc data and form monthly excess returns\n",
    "    btc_df = pd.read_csv('../data/raw/cm_btc.csv')\n",
    "    btc_df['date'] = pd.to_datetime(btc_df.date)\n",
    "    btc_df = btc_df.rename(columns={'btc': 'Bitcoin'})\n",
    "    btc_df.set_index('date', inplace=True)\n",
    "    btc_df = btc_df.resample('M').last()\n",
    "    btc_df.index = btc_df.index + pd.Timedelta(days=1)\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'].pct_change()\n",
    "    btc_df = btc_df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'] - btc_df.r_rf\n",
    "    btc_df = btc_df.drop(['r_rf'], axis=1)\n",
    "\n",
    "    # form single dataframe\n",
    "    df = btc_df.merge(nsdq_df, on='date', how='inner', validate='one_to_one')\n",
    "    for temp_df in [bnd_df, real_df, emrg_df, dbc_df, gld_df, ei_df]:\n",
    "        df = df.merge(temp_df, on='date', how='inner', validate='one_to_one')\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    # Calc rolling correlations\n",
    "    results_df = pd.DataFrame(data = {'date': df.index.values})\n",
    "    results_df = results_df.set_index('date')\n",
    "    col_list = list(df.columns.values)[1:]\n",
    "    for col in col_list:\n",
    "        temp_df = df[['Bitcoin']].rolling(rolling_window).corr(df[col])\n",
    "        temp_df = temp_df.rename(columns = {'Bitcoin': col})\n",
    "        temp_df = temp_df.dropna()\n",
    "        results_df = results_df.merge(temp_df,\n",
    "                                    how='inner',\n",
    "                                    left_index=True,\n",
    "                                    right_index=True,\n",
    "                                    validate='one_to_one')\n",
    "    results_df = results_df[results_df.index < '2023-01-01']\n",
    "\n",
    "    # Form figure\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    results_df.plot(color=[viridis.colors[0], viridis.colors[44], viridis.colors[88],\n",
    "        viridis.colors[132], viridis.colors[178], viridis.colors[222], viridis.colors[255]])\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    plt.legend(labels=results_df.columns.values, \n",
    "                loc='lower center',\n",
    "                ncol=3,\n",
    "                bbox_to_anchor=(0.5, -0.37),\n",
    "                frameon=False)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d06f6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formRiskReturnScatterPlot(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Plot all assets in return risk space. \"\"\"\n",
    "    # calc each asset's annualized return and risk\n",
    "    r_df = df[['date', 'asset', 'char_r_tm7', 'macro_dgs1mo_t']].copy()\n",
    "    r_df['char_r_tm7'] = r_df.char_r_tm7 + ((r_df.macro_dgs1mo_t.values/100)+1)**(1/52)-1\n",
    "    r_df = r_df.drop('macro_dgs1mo_t', axis=1)\n",
    "    r_df['annual_return'] = r_df.groupby('asset')['char_r_tm7'].transform(lambda x: \n",
    "                                QuantTools.calcGeomAvg(x, annualized=True, periods_in_year=52))\n",
    "    r_df['annual_risk'] = r_df.groupby('asset')['char_r_tm7'].transform(lambda x: \n",
    "                                QuantTools.calcSD(x, annualized=True, periods_in_year=52))\n",
    "    r_df = r_df[['asset', 'annual_return', 'annual_risk']].drop_duplicates()\n",
    "    r_df = r_df.sort_values(by='asset', ignore_index=True)\n",
    "\n",
    "    # add 1 month tbill to data\n",
    "    t_df = df[['date', 'macro_dgs1mo_t']].drop_duplicates().reset_index(drop=True)\n",
    "    t_weekly_returns = ((t_df.macro_dgs1mo_t.values/100)+1)**(1/52)-1\n",
    "    t_annual_return = QuantTools.calcGeomAvg(t_weekly_returns, annualized=True, periods_in_year=52)\n",
    "    r_df = pd.concat([r_df, \n",
    "                    pd.DataFrame(data={'asset': ['1moTbill'], \n",
    "                                    'annual_return': [t_annual_return],\n",
    "                                    'annual_risk': [0]})]).reset_index(drop=True)\n",
    "\n",
    "    # Calc risk free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-31').reset_index()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf_tm7'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "\n",
    "    # Import other asset class data\n",
    "    nsdq_df = importYahoo('^IXIC', '2017-11-30', '2023-01-08', rf_df, 'Nasdaq')\n",
    "    nsdq_df = nsdq_df[(nsdq_df.date >= '2018-01-01') & (nsdq_df.date <= '2022-12-31')]\n",
    "\n",
    "    # Add nsdq\n",
    "    nsdq_annual_ret = QuantTools.calcGeomAvg(nsdq_df.Nasdaq.values, annualized=True, periods_in_year=52)\n",
    "    nsdq_sd = QuantTools.calcSD(nsdq_df.Nasdaq.values, annualized=True, periods_in_year=52)\n",
    "    r_df = pd.concat([r_df, \n",
    "                    pd.DataFrame(data={'asset': ['nsdq'], \n",
    "                            'annual_return': [nsdq_annual_ret],\n",
    "                            'annual_risk': [nsdq_sd]})]).reset_index(drop=True)\n",
    "\n",
    "    # Add cmkt\n",
    "    cmkt_df = df.groupby('date')[['macro_cmkt_tm7']].mean().reset_index()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'cmkt'})\n",
    "    cmkt_df['cmkt'] = cmkt_df.cmkt+t_weekly_returns\n",
    "    cmkt_annual_ret = QuantTools.calcGeomAvg(cmkt_df.cmkt.values, annualized=True, periods_in_year=52)\n",
    "    cmkt_sd = QuantTools.calcSD(cmkt_df.cmkt.values, annualized=True, periods_in_year=52)\n",
    "    r_df = pd.concat([r_df,\n",
    "                    pd.DataFrame(data={'asset': ['cmkt'], \n",
    "                            'annual_return': [cmkt_annual_ret],\n",
    "                            'annual_risk': [cmkt_sd]})]).reset_index(drop=True)\n",
    "\n",
    "    # Form optimal risk portfolio\n",
    "    nsdq_pcts = list(np.arange(0, 1, 0.05)) + [.99]\n",
    "    max_sharpe = 0\n",
    "    nsdq_opt   = 0\n",
    "    for nsdq_pct in nsdq_pcts:\n",
    "        cmkt_pct = 1 - nsdq_pct\n",
    "        d_ret = nsdq_df.Nasdaq.values*nsdq_pct + cmkt_df.cmkt.values*cmkt_pct\n",
    "        d_annual_ret = QuantTools.calcGeomAvg(d_ret, annualized=True, periods_in_year=52)\n",
    "        d_sd = QuantTools.calcSD(d_ret, annualized=True, periods_in_year=52)\n",
    "        print(f\"for nsdq pct {np.round(nsdq_pct, 2)}, the sharpe is {np.round(d_annual_ret / d_sd, 3)} and annual return is {np.round(d_annual_ret, 4)}\")\n",
    "        if (d_annual_ret / d_sd) > max_sharpe:\n",
    "            nsdq_opt = nsdq_pct\n",
    "            opt_return = d_annual_ret\n",
    "            opt_sd     = d_sd\n",
    "            max_sharpe = opt_return / opt_sd\n",
    "        \n",
    "    print(f\"\\n for a sharpe of {np.round(max_sharpe, 4)}, allocate {np.round(nsdq_opt, 4)} to nsdq and {np.round(1-nsdq_opt, 4)} to cmkt.\")\n",
    "\n",
    "    opt_col_name = 'nsdq_'+str(np.round(nsdq_opt, 2))+'_cmkt_'+str(np.round(1-nsdq_opt, 2))\n",
    "    r_df = pd.concat([r_df,\n",
    "                    pd.DataFrame(data={'asset': [opt_col_name], \n",
    "                            'annual_return': [opt_return],\n",
    "                            'annual_risk': [opt_sd]})]).reset_index(drop=True)\n",
    "\n",
    "    # Final edits\n",
    "    r_df = r_df.set_index('asset')\n",
    "    r_df = r_df[r_df.index != 'luna']\n",
    "\n",
    "    # Form figure\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    plt.scatter(r_df['annual_risk'], r_df['annual_return'], color=viridis.colors[111])\n",
    "\n",
    "    labels = [\"1moTbill\", \"btc\", \"cmkt\", \"eth\", \"nsdq\", opt_col_name]\n",
    "    colors = ['lightgrey', viridis.colors[255], viridis.colors[33], \n",
    "        viridis.colors[177], 'grey', 'black']\n",
    "    for label, color in zip(labels, colors):\n",
    "        asset_df = r_df[r_df.index==label]\n",
    "        plt.scatter(asset_df['annual_risk'], asset_df['annual_return'], color=color)\n",
    "\n",
    "    point_rf = r_df[r_df.index == \"1moTbill\"].iloc[0]\n",
    "    point_nsdq = r_df[r_df.index == \"nsdq\"].iloc[0]\n",
    "    point_cmkt = r_df[r_df.index == \"cmkt\"].iloc[0]\n",
    "    point_opt  = r_df[r_df.index == opt_col_name].iloc[0]\n",
    "    slope_nsdq = (point_nsdq['annual_return'] - point_rf['annual_return']) / (point_nsdq['annual_risk'] - point_rf['annual_risk'])\n",
    "    slope_cmkt = (point_cmkt['annual_return'] - point_rf['annual_return']) / (point_cmkt['annual_risk'] - point_rf['annual_risk'])\n",
    "    slope_opt = (point_opt['annual_return'] - point_rf['annual_return']) / (point_opt['annual_risk'] - point_rf['annual_risk'])\n",
    "    max_x = r_df['annual_risk'].max()\n",
    "    max_y_nsdq = slope_nsdq * (max_x - point_rf['annual_risk']) + point_rf['annual_return']\n",
    "    max_y_cmkt = slope_cmkt * (max_x - point_rf['annual_risk']) + point_rf['annual_return']\n",
    "    max_y_opt  = slope_opt  * (max_x - point_rf['annual_risk']) + point_rf['annual_return']\n",
    "    plt.plot([point_rf['annual_risk'], max_x], [point_rf['annual_return'], max_y_nsdq], ':', color='grey')\n",
    "    plt.plot([point_rf['annual_risk'], max_x], [point_rf['annual_return'], max_y_cmkt], ':', color=viridis.colors[33])\n",
    "    plt.plot([point_rf['annual_risk'], max_x], [point_rf['annual_return'], max_y_opt], ':', color='black')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.xlim(0, max_x + 0.3)\n",
    "\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e824e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportICOStatistics(df: pd.DataFrame) -> None:\n",
    "    # Subset to relevant data\n",
    "    ico_df = df[['date', 'asset', 'r_ex_tp7', 'char_price_t', \n",
    "        'char_ico', 'char_ico_days_since_t', 'char_ico_price']].copy()\n",
    "\n",
    "    # Determine which assets are ico assets\n",
    "    ico_assets = np.unique(ico_df[ico_df.char_ico==1].asset.values)\n",
    "\n",
    "    # Subset to relevant assets\n",
    "    ico_df = ico_df[ico_df.asset.isin(ico_assets)]\n",
    "    ico_df = ico_df.drop(columns='char_ico', axis=1)\n",
    "\n",
    "    # Form dataframe of the last tradable data for all assets\n",
    "    last_df = ico_df.groupby('asset')[['date']].max().reset_index()\n",
    "    last_df = last_df.merge(ico_df[['date', 'asset', 'char_price_t', 'char_ico_price']], \n",
    "        on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Calc and report return since ICO\n",
    "    last_df['r_from_ico'] = last_df.char_price_t / last_df.char_ico_price - 1\n",
    "    return_from_ico_dates = last_df.r_from_ico.mean()\n",
    "    print(f\"Return on ICO assets from ICO date to end of panel is: {int(return_from_ico_dates)}x.\")\n",
    "\n",
    "    # Calc return of ico assets from first tradable data to end of panel\n",
    "    min_df = ico_df[(ico_df.char_ico_days_since_t>0)].groupby('asset')[['date']].min().reset_index()\n",
    "    min_df = min_df.merge(ico_df[['date', 'asset', 'char_price_t']],\n",
    "        on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    min_df = min_df.rename(columns={'char_price_t': 'first_trade_price'})\n",
    "    min_df = min_df.drop('date', axis=1)\n",
    "\n",
    "    max_df = ico_df.groupby('asset')[['date']].max().reset_index()\n",
    "    max_df = max_df.merge(ico_df[['date', 'asset', 'char_price_t']], \n",
    "        on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    max_df = max_df.rename(columns={'char_price_t': 'last_trade_price'})\n",
    "    max_df = max_df.drop('date', axis=1)\n",
    "    trade_df = max_df.merge(min_df, on='asset', how='inner', validate='one_to_one')\n",
    "    trade_df['r'] = trade_df.last_trade_price / trade_df.first_trade_price - 1\n",
    "\n",
    "    return_from_first_tradable = trade_df.r.mean()\n",
    "    print(f\"Return on ICO assets from first tradable date to end of panel is: {np.round(100*return_from_first_tradable, 1)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6dd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflationStatistics(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    # Copy the raw data for use later as this func is terribly written\n",
    "    temp_df = df.copy()\n",
    "\n",
    "    # Params\n",
    "    start_date = '2013-11-01'\n",
    "    end_date   = '2023-07-02'\n",
    "\n",
    "    # Obtain monthly excess returns for cmkt\n",
    "    cmkt_df = df[['date', 'macro_cmkt_tm7']].drop_duplicates().copy()\n",
    "    cmkt_df.set_index('date', inplace=True)\n",
    "    cmkt_df['cmkt'] = cmkt_df.macro_cmkt_tm7+1\n",
    "    cmkt_df = cmkt_df.drop('macro_cmkt_tm7', axis=1)\n",
    "    cmkt_df = cmkt_df.resample('M')[['cmkt']].prod()-1\n",
    "    cmkt_df.index = cmkt_df.index + pd.DateOffset(days=1) - pd.DateOffset(months=1)\n",
    "\n",
    "    # Pull and form exp inf data\n",
    "    inf_col = 'EXPINF10YR'\n",
    "    ei_df = pdr.DataReader(inf_col, 'fred', start_date).reset_index()\n",
    "    ei_df = ei_df.rename(columns={'DATE': 'date'})\n",
    "    ei_df = ei_df.set_index('date')\n",
    "    ei_df[inf_col] = (1+ei_df[inf_col])**(1/12)-1 # annual inflation rate to monthly rate\n",
    "    ei_df = ei_df.pct_change().dropna().reset_index()\n",
    "    ei_df = ei_df[(ei_df.date >= '2014-01-01') & (ei_df.date <= '2023-07-01')]\n",
    "\n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-15').reset_index()\n",
    "    rf_df['r_rf'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / 12) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "    rf_df = rf_df[(rf_df.date >= '2014-01-01') & (rf_df.date <= '2023-07-01')]\n",
    "\n",
    "    # Import other asset class data\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold', 'M', 'r_rf')\n",
    "    nsdq_df  = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq', 'M', 'r_rf')\n",
    "\n",
    "    # import the btc data and form monthly excess returns\n",
    "    btc_df = pd.read_csv('../data/raw/cm_btc.csv')\n",
    "    btc_df['date'] = pd.to_datetime(btc_df.date)\n",
    "    btc_df = btc_df.rename(columns={'btc': 'Bitcoin'})\n",
    "    btc_df.set_index('date', inplace=True)\n",
    "    btc_df = btc_df.resample('M').last()\n",
    "    btc_df.index = btc_df.index + pd.Timedelta(days=1)\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'].pct_change()\n",
    "    btc_df = btc_df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'] - btc_df.r_rf\n",
    "    btc_df = btc_df.drop(['r_rf'], axis=1)\n",
    "\n",
    "    # form single dataframe\n",
    "    df = cmkt_df.merge(btc_df, on='date', how='right', validate='one_to_one')\n",
    "    df = df.merge(ei_df, on='date', how='outer', validate='one_to_one')\n",
    "    df = df.merge(gld_df, on='date', how='outer', validate='one_to_one')\n",
    "    df = df.merge(nsdq_df, on='date', how='outer', validate='one_to_one')\n",
    "    df = df.set_index('date')\n",
    "    df = df[df.index <= '2023-01-01']\n",
    "\n",
    "    # OVERALL CORR OF MONTHLY RETURNS AND INF INNOV\n",
    "    print(df.index.min())\n",
    "    print(df.index.max())\n",
    "    print(df.drop('cmkt', axis=1).corr())\n",
    "\n",
    "    # OVERALL CORR OF MONTHLY RETURNS AND INF INNOV JUST 2018-2022\n",
    "    print(df[df.cmkt.notnull()].corr())\n",
    "\n",
    "    # CORR BETWEEN BTC AND EXPINF1YR AS WELL AS BTWN GOLD AND EXPINF1YR \n",
    "    # ON TOP 12 MONTHS IN 2014 Jan to Dec 2022 and Jan 2018 to Dec 2022 for cmkt\n",
    "    df['inf_abs'] = np.abs(df[inf_col])\n",
    "    print(df.drop('cmkt', axis=1).sort_values(by='inf_abs', ascending=False)[:12].corr())\n",
    "    print(df[df.cmkt.notnull()].sort_values(by='inf_abs', ascending=False)[:12].corr())\n",
    "    df = df.drop('inf_abs', axis=1)\n",
    "\n",
    "    # Regress BTC returns on CMKT and inf\n",
    "    df = df[df.cmkt.notnull()]\n",
    "    y = df.Bitcoin\n",
    "    X = df[['cmkt', inf_col]]\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    results_summary = results.summary2().tables\n",
    "    summary_df = pd.DataFrame(results_summary[1])\n",
    "    summary_df['N'] = len(y)\n",
    "    summary_df['R2'] = results.rsquared\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        summary_df.to_excel(writer, sheet_name='raw_inf_reg')\n",
    "\n",
    "    # Fama Macbeth\n",
    "    temp_df = temp_df[['date', 'asset', 'char_r_tm7']].copy()\n",
    "    asset_df = pd.DataFrame()\n",
    "    assets = list(np.unique(temp_df.asset.values))\n",
    "    for asset in assets:\n",
    "        t_df = temp_df[temp_df.asset==asset][['date', 'char_r_tm7']]\n",
    "        t_df.set_index('date', inplace=True)\n",
    "        t_df['r'] = t_df['char_r_tm7']+1\n",
    "        t_df = t_df.drop('char_r_tm7', axis=1)\n",
    "        t_df = t_df.resample('M')[['r']].prod()-1\n",
    "        t_df.index = t_df.index + pd.DateOffset(days=1) - pd.DateOffset(months=1)\n",
    "        t_df = t_df.reset_index()\n",
    "        t_df['asset'] = asset\n",
    "        asset_df = pd.concat([asset_df, t_df])\n",
    "\n",
    "    beta_hats = []\n",
    "    assets = list(np.unique(asset_df.asset.values))\n",
    "    assets.remove('inv')\n",
    "    for asset in assets:\n",
    "        a_df = asset_df[asset_df.asset==asset]\n",
    "        a_df = a_df.merge(df[[inf_col]].reset_index(), on='date', how='inner', validate='one_to_one')\n",
    "        y = a_df.r\n",
    "        X = a_df[[inf_col]]\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X)\n",
    "        results = model.fit()\n",
    "        beta_hats.append(results.params[1])\n",
    "\n",
    "    y = asset_df[asset_df.asset!='inv'].groupby('asset')['r'].mean().values\n",
    "    x = np.array(beta_hats)\n",
    "    X = sm.add_constant(x)\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    results_summary = results.summary2().tables\n",
    "    summary_df = pd.DataFrame(results_summary[1])\n",
    "    summary_df['N'] = len(y)\n",
    "    summary_df['R2'] = results.rsquared\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        summary_df.to_excel(writer, sheet_name='raw_fama_macbeth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "093d7671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for nsdq pct 0.0, the sharpe is 0.327 and annual return is 0.2643\n",
      "for nsdq pct 0.05, the sharpe is 0.355 and annual return is 0.2733\n",
      "for nsdq pct 0.1, the sharpe is 0.382 and annual return is 0.2801\n",
      "for nsdq pct 0.15, the sharpe is 0.409 and annual return is 0.2849\n",
      "for nsdq pct 0.2, the sharpe is 0.436 and annual return is 0.2876\n",
      "for nsdq pct 0.25, the sharpe is 0.462 and annual return is 0.2882\n",
      "for nsdq pct 0.3, the sharpe is 0.488 and annual return is 0.2867\n",
      "for nsdq pct 0.35, the sharpe is 0.514 and annual return is 0.2832\n",
      "for nsdq pct 0.4, the sharpe is 0.538 and annual return is 0.2777\n",
      "for nsdq pct 0.45, the sharpe is 0.561 and annual return is 0.2702\n",
      "for nsdq pct 0.5, the sharpe is 0.582 and annual return is 0.2608\n",
      "for nsdq pct 0.55, the sharpe is 0.601 and annual return is 0.2495\n",
      "for nsdq pct 0.6, the sharpe is 0.617 and annual return is 0.2363\n",
      "for nsdq pct 0.65, the sharpe is 0.628 and annual return is 0.2214\n",
      "for nsdq pct 0.7, the sharpe is 0.632 and annual return is 0.2049\n",
      "for nsdq pct 0.75, the sharpe is 0.627 and annual return is 0.1867\n",
      "for nsdq pct 0.8, the sharpe is 0.608 and annual return is 0.167\n",
      "for nsdq pct 0.85, the sharpe is 0.571 and annual return is 0.1458\n",
      "for nsdq pct 0.9, the sharpe is 0.512 and annual return is 0.1233\n",
      "for nsdq pct 0.95, the sharpe is 0.429 and annual return is 0.0996\n",
      "for nsdq pct 0.99, the sharpe is 0.347 and annual return is 0.0797\n",
      "\n",
      " for a sharpe of 0.6322, allocate 0.7 to nsdq and 0.3 to cmkt.\n",
      "Return on ICO assets from ICO date to end of panel is: 125x.\n",
      "Return on ICO assets from first tradable date to end of panel is: -1.4%.\n",
      "2014-01-01 00:00:00\n",
      "2023-01-01 00:00:00\n",
      "             Bitcoin  EXPINF10YR      Gold    Nasdaq\n",
      "Bitcoin     1.000000    0.049027  0.055291  0.293570\n",
      "EXPINF10YR  0.049027    1.000000 -0.261308  0.233625\n",
      "Gold        0.055291   -0.261308  1.000000  0.052724\n",
      "Nasdaq      0.293570    0.233625  0.052724  1.000000\n",
      "                cmkt   Bitcoin  EXPINF10YR      Gold    Nasdaq\n",
      "cmkt        1.000000  0.338887    0.006599 -0.047524  0.077200\n",
      "Bitcoin     0.338887  1.000000    0.053661  0.112770  0.342173\n",
      "EXPINF10YR  0.006599  0.053661    1.000000 -0.160458  0.220225\n",
      "Gold       -0.047524  0.112770   -0.160458  1.000000  0.183203\n",
      "Nasdaq      0.077200  0.342173    0.220225  0.183203  1.000000\n",
      "             Bitcoin  EXPINF10YR      Gold    Nasdaq   inf_abs\n",
      "Bitcoin     1.000000    0.180507 -0.258121  0.181457 -0.146656\n",
      "EXPINF10YR  0.180507    1.000000 -0.391412  0.550316 -0.486006\n",
      "Gold       -0.258121   -0.391412  1.000000 -0.033053 -0.218750\n",
      "Nasdaq      0.181457    0.550316 -0.033053  1.000000 -0.250773\n",
      "inf_abs    -0.146656   -0.486006 -0.218750 -0.250773  1.000000\n",
      "                cmkt   Bitcoin  EXPINF10YR      Gold    Nasdaq   inf_abs\n",
      "cmkt        1.000000  0.264719    0.059675  0.152227 -0.212096 -0.738597\n",
      "Bitcoin     0.264719  1.000000    0.025527  0.271476  0.342803 -0.051505\n",
      "EXPINF10YR  0.059675  0.025527    1.000000 -0.096900  0.332339 -0.449820\n",
      "Gold        0.152227  0.271476   -0.096900  1.000000  0.573296 -0.244409\n",
      "Nasdaq     -0.212096  0.342803    0.332339  0.573296  1.000000 -0.025031\n",
      "inf_abs    -0.738597 -0.051505   -0.449820 -0.244409 -0.025031  1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 644x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 644x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 644x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 644x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    PANEL_IN_FP      = '../data/clean/panel_weekly.pkl' \n",
    "    ASSET_IN_FP      = '../data/clean/asset_universe_dict.pickle'\n",
    "    HIST_OUT_FP      = '../output/desc_stats/histograms.png'\n",
    "    CUM_RET_OUT_FP   = '../output/desc_stats/cumulative_returns.png'\n",
    "    SHARPE_OUT_FP    = '../output/desc_stats/sharpe.png'\n",
    "    TX_OUT_FP        = '../output/desc_stats/btc_tx.png'\n",
    "    HODL_OUT_FP      = '../output/desc_stats/hodl.png'\n",
    "    CORR_OUT_FP      = '../output/desc_stats/corr.png'\n",
    "    RISK_RTRN_OUT_FP = '../output/desc_stats/risk_return.png'\n",
    "    OUT_FP           = '../output/desc_stats/descriptive_statistics.xlsx'\n",
    "    PERIODS_IN_YEAR  = 52\n",
    "    TS_AVG_METHOD    = 'arithmetic'\n",
    "    LHS_COL          = 'r_ex_tp7'\n",
    "    ANNUALIZED       = False\n",
    "\n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # drop rows that are not in the asset universe\n",
    "    df = subsetToAssetUniverse(df, asset_universe_dict)\n",
    "\n",
    "    # generate plots\n",
    "    plotReturnHistograms(df, HIST_OUT_FP)\n",
    "    plotCumulativeReturns(df, CUM_RET_OUT_FP)\n",
    "    plotRollingSharpe(SHARPE_OUT_FP)\n",
    "    plotTransactionStats(df, TX_OUT_FP)\n",
    "    plotHodlingStats(df, HODL_OUT_FP)\n",
    "    plotRollingCorrelations(CORR_OUT_FP)\n",
    "    formRiskReturnScatterPlot(df, RISK_RTRN_OUT_FP)\n",
    "\n",
    "    # generate tables\n",
    "    genSummaryStatistics(df, LHS_COL, OUT_FP)\n",
    "    # genForkStatistics(df, OUT_FP)\n",
    "    genCorrStatistics(df, OUT_FP)\n",
    "\n",
    "    # report statistics\n",
    "    reportICOStatistics(df)\n",
    "    inflationStatistics(df, OUT_FP)\n",
    "    \n",
    "    # # TODO SCOPE IF RESULTS FOR ALL CHANGE MUCH AFTER A WINSOR\n",
    "    # p1 = df[LHS_COL].quantile(0.01)\n",
    "    # p99 = df[LHS_COL].quantile(0.99)\n",
    "    # df.loc[df[LHS_COL] < p1, LHS_COL] = p1 \n",
    "    # df.loc[df[LHS_COL] > p99, LHS_COL] = p1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
