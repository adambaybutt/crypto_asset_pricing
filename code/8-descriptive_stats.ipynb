{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a21a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# Import packages\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "from tools import QuantTools\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# set color map\n",
    "viridis = matplotlib.colormaps['viridis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b9f2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importYahoo(ticker: str, start_date: str, end_date: str, rf_df: pd.DataFrame, new_ret_col: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Import Yahoo Finance data for given ticker, time period, taking our risk free rate in rf_df. \n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): The ticker symbol to get data for.\n",
    "    start_date (str): The start date of the data retrieval period.\n",
    "    end_date (str): The end date of the data retrieval period.\n",
    "    rf_df (DataFrame): A DataFrame containing risk-free rates.\n",
    "    new_ret_col (str): The new column name for returns.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the date and return data.\n",
    "\n",
    "    Note: this is done at weekly frequency; would need adjustment for different.\n",
    "    \"\"\"\n",
    "    # import the data\n",
    "    df = yf.Ticker(ticker).history(period='1d', start=start_date, end=end_date).reset_index()\n",
    "    \n",
    "    # reformat\n",
    "    df['Date'] = pd.to_datetime(df.Date).to_numpy(dtype='datetime64[D]')\n",
    "    df = df[['Date', 'Close']].rename(columns={'Date': 'date', 'Close': new_ret_col}).set_index('date')\n",
    "    df = df.resample('W').last().pct_change().dropna()\n",
    "\n",
    "    # take out rf rate\n",
    "    df = df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    df[new_ret_col] = df[new_ret_col] - df.r_rf_tm7\n",
    "\n",
    "    # check for NaN values in new_ret_col\n",
    "    if df[new_ret_col].isnull().values.any():\n",
    "        print(\"Warning: NaN values found in return data\")\n",
    "    \n",
    "    return df[['date', new_ret_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8347f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniverse(df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset a DataFrame based on a dictionary of asset universes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame. Must contain columns \"date\" and \"asset\".\n",
    "    asset_universe_dict : Dict[str, List[str]]\n",
    "        A dictionary where keys are dates in 'YYYY-MM-DD' format and values are lists of asset names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The subsetted DataFrame.\n",
    "    \"\"\"\n",
    "    # Check that the required columns are present in the DataFrame\n",
    "    if not set(['date', 'asset']).issubset(df.columns):\n",
    "        raise ValueError('Input DataFrame must contain \"date\" and \"asset\" columns.')\n",
    "\n",
    "    # Ensure that the 'date' column is of datetime type\n",
    "    if df['date'].dtype != 'datetime64[ns]':\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Loop over all months with their relevant assets\n",
    "    for key, values in asset_universe_dict.items():\n",
    "        # Extract the year and month from the key\n",
    "        year, month = key.split('-')[:2]\n",
    "\n",
    "        # Drop rows from the dataframe which match the year and month but not the assets\n",
    "        df = df[~((df.date.dt.year == int(year)) \n",
    "                    & (df.date.dt.month == int(month)) \n",
    "                    & (~df.asset.isin(values)))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9bd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotReturnHistograms(df: pd.DataFrame, out_fp: str):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame containing time series data for returns of\n",
    "    btc, eth, and the cmkt and saves a histogram plot for all three to given fp.\n",
    "    Each histogram also includes a normal distribution fit.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): A DataFrame containing columns 'date', 'asset', 'char_r_tm7', 'macro_cmkt_tm7'.\n",
    "    out_fp (str): A string specifying the filepath where the plot should be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # extract relevant returns\n",
    "    btc_df  = df[df.asset=='btc'][['date', 'char_r_tm7']]\n",
    "    btc_df  = btc_df.rename(columns={'char_r_tm7': 'btc'})\n",
    "    eth_df  = df[df.asset=='eth'][['date', 'char_r_tm7']]\n",
    "    eth_df  = eth_df.rename(columns={'char_r_tm7': 'eth'})\n",
    "    cmkt_df = df.groupby('date')[['macro_cmkt_tm7']].mean().reset_index()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'cmkt'})\n",
    "\n",
    "    # form single dataframe\n",
    "    hist_df = cmkt_df.merge(btc_df, on='date', how='inner', validate='one_to_one')\n",
    "    hist_df = hist_df.merge(eth_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    # initiate the plot with given colors and columns\n",
    "    fig, axs = plt.subplots(3, sharex=True, sharey=True, figsize=(6.4,4), facecolor='none')\n",
    "    colors = plt.get_cmap('viridis')(np.linspace(0, 10))\n",
    "    data_columns = ['cmkt', 'btc', 'eth']\n",
    "\n",
    "    # plot the data with the normal dist fit\n",
    "    for idx, ax in enumerate(axs):\n",
    "        data = hist_df[data_columns[idx]]\n",
    "        n, bins, patches = ax.hist(data, bins=30, color=colors[idx], alpha=1)\n",
    "        #  density=True, \n",
    "\n",
    "        # Fit a normal distribution\n",
    "        mu, std = norm.fit(data)\n",
    "\n",
    "        # Scale normal distribution to histogram\n",
    "        scale = n.max() / norm.pdf(mu, mu, std).max()\n",
    "        \n",
    "        # Plot the PDF\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        x = np.linspace(xmin, xmax, 100)\n",
    "        p = norm.pdf(x, mu, std) * scale\n",
    "        ax.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    # tighen up the plot\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # adjust x axis labels\n",
    "    plt.xticks(np.arange(-.5, 0.7, 0.1))\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "\n",
    "    # close the figure\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15c837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCumulativeReturns(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot the time series of cumulative returns to the given output filepath.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the time series data\n",
    "        out_fp (str): a relative filepath to save the figure to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # initialize df with timeserieses to plot\n",
    "    plot_df = pd.DataFrame(data={'date': []})\n",
    "\n",
    "    # find all assets present in the panel\n",
    "    assets = list(np.unique(df.asset.values))\n",
    "\n",
    "    # form each asset's cumulative return\n",
    "    for asset in assets:\n",
    "        # extract asset's returns\n",
    "        temp_df = df[df.asset==asset][['date', 'char_r_tm7']]\n",
    "\n",
    "        # ensure it is sorted\n",
    "        temp_df = temp_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "        # form cumulative return\n",
    "        temp_df[asset] = (1 + temp_df['char_r_tm7']).cumprod()\n",
    "\n",
    "        # merge on results\n",
    "        plot_df = plot_df.merge(temp_df[['date', asset]], on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # form the cmkt return\n",
    "    temp_df = df[df.asset=='btc'][['date', 'macro_cmkt_tm7']]\n",
    "    temp_df = temp_df.sort_values(by='date', ignore_index=True)\n",
    "    temp_df['cmkt'] = (1 + temp_df['macro_cmkt_tm7']).cumprod()\n",
    "    plot_df = plot_df.merge(temp_df[['date', 'cmkt']], on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # resort\n",
    "    plot_df = plot_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # set index\n",
    "    plot_df.set_index('date', inplace=True)\n",
    "\n",
    "    # Plotting the time series\n",
    "    plt.figure(figsize=(4*1.61, 4), facecolor='none')\n",
    "\n",
    "    # Form column list\n",
    "    columns = list(plot_df.columns)\n",
    "    columns.remove('btc')\n",
    "    columns.remove('eth')\n",
    "    columns.remove('cmkt')\n",
    "    columns = columns + ['eth', 'btc', 'cmkt']\n",
    "\n",
    "    # Iterate over the columns and plot each time series\n",
    "    for column in columns:\n",
    "        if column == 'btc':\n",
    "            color = '#FDE725FF'\n",
    "            linewidth = 2\n",
    "        elif column == 'eth':\n",
    "            color = '#2D708EFF'\n",
    "            linewidth = 2\n",
    "        elif column == 'cmkt':\n",
    "            color = '#482677FF'\n",
    "            linewidth = 2\n",
    "        else:\n",
    "            color = 'gray'\n",
    "            linewidth = 0.5\n",
    "        plt.plot(plot_df.index, plot_df[column], color=color, linewidth=linewidth)\n",
    "\n",
    "    # Set y-axis to logarithmic scale\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Remove y-axis minor ticks\n",
    "    plt.gca().yaxis.set_minor_locator(plt.NullLocator())\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.grid(visible=True, which='major', axis='y', linewidth=0.5)\n",
    "    plt.box(False)\n",
    "\n",
    "    # Add custom labels for important time series\n",
    "    plt.text(plot_df.index[-45], plot_df['cmkt'].iloc[-52]+5, 'cmkt', color='#482677FF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "    plt.text(plot_df.index[-1], plot_df['btc'].iloc[-1]-0.1, 'btc', color='#FDE725FF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "    plt.text(plot_df.index[-1], plot_df['eth'].iloc[-1]+0.3, 'eth', color='#2D708EFF', fontweight='bold', verticalalignment='center', bbox=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    # output\n",
    "    plt.savefig(out_fp)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac7195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSummaryStatistics(df: pd.DataFrame, lhs_col: str, out_fp: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates summary statistics for the panel and saves them to an Excel file.\n",
    "\n",
    "    :param df: DataFrame containing asset return data.\n",
    "    :param lhs_col: Column in df that contains the return data.\n",
    "    :param out_fp: Output file path for the Excel file.\n",
    "    \"\"\"\n",
    "    # define function for calculating return statistics\n",
    "    def calcReturnStats(temp_df: pd.DataFrame, asset: str, return_col: str) -> dict:\n",
    "        mean_return = QuantTools.calcTSAvgReturn(temp_df[return_col].values, annualized=True, periods_in_year=52)\n",
    "        std_dev = QuantTools.calcSD(temp_df[return_col].values, annualized=True, periods_in_year=52)\n",
    "        sharpe_ratio = QuantTools.calcSharpe(temp_df[return_col].values, periods_in_year=52)\n",
    "        skewness = stats.skew(temp_df[return_col].values) / np.sqrt(52)\n",
    "        kurtosis = stats.kurtosis(temp_df[return_col].values) / 52\n",
    "        perc_return_above_zero = np.sum(temp_df[return_col]>0) / len(temp_df)\n",
    "        \n",
    "        return {'asset': asset,\n",
    "            'Mean': mean_return,\n",
    "            'SD': std_dev,\n",
    "            'Sharpe': sharpe_ratio,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'Pct pos': perc_return_above_zero}\n",
    "\n",
    "    # drop to only necessary columns\n",
    "    df = df[['date', 'asset', lhs_col, 'char_size_t', 'char_volume_sum_tm7', \n",
    "        'macro_snp500_t', 'macro_dgs1mo_t']].copy()\n",
    "\n",
    "    # form btc and eth returns\n",
    "    btc_df = df[df.asset=='btc'].set_index('date')[[lhs_col]]\n",
    "    eth_df = df[df.asset=='eth'].set_index('date')[[lhs_col]]\n",
    "\n",
    "    # form cmkt return\n",
    "    df['weighted_return'] = df[lhs_col] * df['char_size_t']\n",
    "    total_market_cap = df.groupby('date')['char_size_t'].sum()\n",
    "    cmkt_df = df.groupby('date')['weighted_return'].sum() / total_market_cap\n",
    "    cmkt_df = pd.DataFrame(cmkt_df).rename(columns={0: 'return'})\n",
    "\n",
    "    # import nasdaq data and take out risk free rate\n",
    "    rf_df = df[['date', 'macro_dgs1mo_t']].drop_duplicates()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.macro_dgs1mo_t.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    nsdq_df = importYahoo('^IXIC', '2017-12-29', '2022-12-31', rf_df, 'r_nsdq_tm7')    \n",
    "\n",
    "    # calc return statistics\n",
    "    cmkt_stats = calcReturnStats(cmkt_df, 'CMKT', 'return')\n",
    "    btc_stats  = calcReturnStats(btc_df, 'Bitcoin', lhs_col)\n",
    "    eth_stats  = calcReturnStats(eth_df, 'Ethereum', lhs_col)\n",
    "    nsdq_stats = calcReturnStats(nsdq_df, 'Nasdaq', 'r_nsdq_tm7')\n",
    "    ret_df = pd.DataFrame([cmkt_stats, btc_stats, eth_stats, nsdq_stats])\n",
    "    \n",
    "    # calc extreme event statistics\n",
    "    ext_data = {'threshold': [], 'count': [], 'percent': []}\n",
    "    num_obs  = len(cmkt_df)\n",
    "    for threshold in [-.3, -.2, -.1, -.05, .05, .1, .2, .3]:\n",
    "        ext_data['threshold'].append(threshold)\n",
    "        if threshold < 0:\n",
    "            count = (cmkt_df['return'] < threshold).sum()\n",
    "            ext_data['count'].append(count)\n",
    "            ext_data['percent'].append(count / num_obs)\n",
    "        else:\n",
    "            count = (cmkt_df['return'] > threshold).sum()\n",
    "            ext_data['count'].append(count)\n",
    "            ext_data['percent'].append(count / num_obs)\n",
    "    ext_df = pd.DataFrame(ext_data)\n",
    "    \n",
    "    # calculate yearly stats of unique assets and median mcap and volume\n",
    "    df['year'] = df['date'].dt.year\n",
    "    yr_df = pd.DataFrame({\n",
    "        'num_unique_assets': df.groupby(['year'])['asset'].nunique(),\n",
    "        'median_market_cap': df.groupby(['year'])['char_size_t'].median(),\n",
    "        'median_weekly_asset_volume': df.groupby(['year'])['char_volume_sum_tm7'].median()}).reset_index()\n",
    "    all_df = pd.DataFrame({\n",
    "        'num_unique_assets': [df['asset'].nunique()],\n",
    "        'median_market_cap': [df['char_size_t'].median()],\n",
    "        'median_weekly_asset_volume': [df['char_volume_sum_tm7'].median()]})\n",
    "    all_df['year'] = 'all'\n",
    "    yr_df = pd.concat([yr_df, all_df])\n",
    "\n",
    "    # calculate the total mcap in the last week of each year\n",
    "    max_dates = df.groupby('year')['date'].max()\n",
    "    filtered_df = df[df['date'].isin(max_dates)]\n",
    "    total_mcap_by_year = filtered_df.groupby('year')[['char_size_t']].sum().reset_index()\n",
    "    yr_df = yr_df.merge(total_mcap_by_year, on='year', how='outer', validate='one_to_one')\n",
    "\n",
    "    # extract yearly returns\n",
    "    cmkt_df = cmkt_df.reset_index()\n",
    "    cmkt_df['year'] = cmkt_df.date.dt.year\n",
    "    for year in [2018, 2019, 2020, 2021, 2022]:\n",
    "        yr_df.loc[yr_df.year==year, 'cmkt_ret'] = ((cmkt_df[cmkt_df.year==year]['return']+1).cumprod()-1).values[-1]\n",
    "    yr_df.loc[yr_df.year=='all', 'cmkt_ret'] = ((cmkt_df['return']+1).cumprod()-1).values[-1]\n",
    "    \n",
    "    # save results\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        ret_df.to_excel(writer, sheet_name='raw_ret_stats')\n",
    "        ext_df.to_excel(writer, sheet_name='raw_extreme_stats')\n",
    "        yr_df.to_excel(writer, sheet_name='raw_yearly_stats')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb9e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRollingSharpe(out_fp: str) -> None:\n",
    "    \"\"\" Plot rolling four year sharpe ratio with new data for the study period. \"\"\"\n",
    "    \n",
    "    # Obtain risk-free rate\n",
    "    rf_df = pdr.DataReader('DGS1MO', 'fred', '2013-12-31').reset_index()\n",
    "    rf_df['r_rf_tm7'] = (1 + rf_df.DGS1MO.values / 100) ** (1 / (365 / 7)) - 1\n",
    "    rf_df['date'] = pd.to_datetime(rf_df.DATE)\n",
    "    rf_df = rf_df[['date', 'r_rf_tm7']]\n",
    "    rf_df.set_index('date', inplace=True)\n",
    "    date_range = pd.date_range(start=rf_df.index.min(), end=rf_df.index.max(), freq='D')\n",
    "    rf_df = rf_df.reindex(date_range)\n",
    "    rf_df['r_rf_tm7'].fillna(method='ffill', inplace=True)\n",
    "    rf_df.reset_index(inplace=True)\n",
    "    rf_df = rf_df.rename(columns={'index': 'date'})\n",
    "\n",
    "    # import other asset class data\n",
    "    start_date = '2013-12-29'\n",
    "    end_date   = '2022-12-31'\n",
    "    nsdq_df = importYahoo('^IXIC', start_date, end_date, rf_df, 'Nasdaq')\n",
    "    snp_df  = importYahoo('^GSPC', start_date, end_date, rf_df, 'SnP 500')\n",
    "    vt_df   = importYahoo('VT', start_date, end_date, rf_df, 'Global Stocks')\n",
    "    bnd_df  = importYahoo('BND', start_date, end_date, rf_df, 'US Bonds')\n",
    "    real_df = importYahoo('VNQ', start_date, end_date, rf_df, 'US Real Estate')\n",
    "    emrg_df = importYahoo('EBND', start_date, end_date, rf_df, 'Emerging Currencies')\n",
    "    gld_df  = importYahoo('GLD', start_date, end_date, rf_df, 'Gold')\n",
    "\n",
    "    # import the btc data and form weekly excess returns\n",
    "    btc_df = pd.read_csv('../data/raw/XBTUSD_1440.csv',\n",
    "                        header=None)\n",
    "    btc_df[0] = pd.to_datetime(btc_df[0], unit='s')\n",
    "    btc_df = btc_df[[0, 4]]\n",
    "    btc_df = btc_df.rename(columns={0: 'date', 4: 'price'})\n",
    "    btc_df.set_index('date', inplace=True)\n",
    "    btc_df = btc_df.resample('W').last()\n",
    "    btc_df['Bitcoin'] = btc_df['price'].pct_change()\n",
    "    btc_df = btc_df.merge(rf_df, on='date', how='inner', validate='one_to_one')\n",
    "    btc_df['Bitcoin'] = btc_df['Bitcoin'] - btc_df.r_rf_tm7\n",
    "    btc_df = btc_df.drop(['price', 'r_rf_tm7'], axis=1)\n",
    "\n",
    "    # form single dataframe\n",
    "    df = btc_df.merge(nsdq_df, on='date', how='inner', validate='one_to_one')\n",
    "    for temp_df in [snp_df, vt_df, bnd_df, real_df, emrg_df, gld_df]:\n",
    "        df = df.merge(temp_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    # form sharpe ratios\n",
    "    window_size = 208\n",
    "    df.set_index('date', inplace=True)\n",
    "    columns = list(df.columns.values)\n",
    "    for col in columns:\n",
    "        df[col] = np.sqrt(52) * df[col].rolling(window_size).mean() / df[col].rolling(window_size).std()\n",
    "\n",
    "    # subset to relevant time period\n",
    "    df = df[(df.index.year >= 2018)\n",
    "        & (df.index.year <= 2022)]\n",
    "\n",
    "    # form sharpe ratio plot\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    df.plot(cmap='viridis')\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    #plt.title('Sharpe Ratios: Bitcoin vs Major Asset Classes$^{12}$')\n",
    "    plt.legend(labels=df.columns.values, \n",
    "                loc='lower center',\n",
    "                ncol=3,\n",
    "                bbox_to_anchor=(0.5, -0.37),\n",
    "                frameon=False)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.xticks(rotation=0, ha='center')\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35aee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTransactionStats(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Plot rolling transaction statistics for Bitcoin. \"\"\"\n",
    "    # Extract relevant transaction data\n",
    "    tx_df = df[['date', 'macro_btc_fee_med_usd_t', 'macro_btc_tx_tfr_val_adj_usd_t']].drop_duplicates().copy()\n",
    "\n",
    "    # reformat the data\n",
    "    tx_df.set_index('date', inplace=True)\n",
    "    temp1_df = tx_df[['macro_btc_tx_tfr_val_adj_usd_t']].resample('M').sum()\n",
    "    temp2_df = tx_df[['macro_btc_fee_med_usd_t']].resample('M').median()\n",
    "    r_df = temp1_df.merge(temp2_df, how='inner', left_index=True, right_index=True, validate='one_to_one')\n",
    "    r_df = r_df.rename(columns={'macro_btc_fee_med_usd_t': 'Median Fee (USD)',\n",
    "                                'macro_btc_tx_tfr_val_adj_usd_t': 'Monthly Volume (USD)'})\n",
    "    r_df['date'] = r_df.index\n",
    "    r_df['date'] = r_df.date.dt.strftime('%Y-%m')\n",
    "    r_df = r_df.set_index('date')\n",
    "\n",
    "    # plot the data\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    r_df.plot(color=[viridis.colors[0], viridis.colors[111]])\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    plt.legend(labels=r_df.columns.values, \n",
    "            loc='lower center',\n",
    "            ncol=3,\n",
    "            bbox_to_anchor=(0.5, -0.22),\n",
    "            frameon=False)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.xticks(np.array([0,12,24,36,48]), \n",
    "            ['2018', '2019', '2020', '2021', '2022'],\n",
    "            rotation=0, ha='center')\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09b4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHodlingStats(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    # form the hodl data\n",
    "    utxo_df = df[['date', 'macro_btc_utxo_age_med_t']].copy()\n",
    "    utxo_df = utxo_df.drop_duplicates()\n",
    "    utxo_df = utxo_df.set_index('date')\n",
    "    utxo_df = utxo_df.rename(columns={'macro_btc_utxo_age_med_t':\n",
    "                                    'UTXO Median Age (Days)'})\n",
    "\n",
    "    # Plot the hodl data\n",
    "    plt.figure(figsize=(4*1.61, 4))\n",
    "    utxo_df.plot(color=[viridis.colors[0]],\n",
    "                legend=None)\n",
    "    plt.box(False)\n",
    "    plt.grid(visible=True, which='major', axis='x', linewidth=0.5)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tick_params(axis='both', which='both', bottom=False)\n",
    "    plt.savefig(out_fp, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518f2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genForkStatistics(df: pd.DataFrame, out_fp: str) -> None:\n",
    "    \"\"\" Generate event studies for BTC fork dates. \"\"\"\n",
    "    # PARAMETERS\n",
    "    num_bs_samples = int(1e6)\n",
    "    num_cpus       = 22\n",
    "    window_size    = 8\n",
    "\n",
    "    # DEFINE RELEVANT FORKS\n",
    "    forks = {'bitcoin-21': datetime.datetime(2016,4,17),\n",
    "            'zcash': datetime.datetime(2016,10,28),\n",
    "            'bitcoin-cash': datetime.datetime(2017,7,31),\n",
    "            'bitcoin-gold': datetime.datetime(2017,10,24),\n",
    "            'bitcoin-diamond': datetime.datetime(2017,11,24),\n",
    "            'lightning-bitcoin': datetime.datetime(2017,12,18),\n",
    "            'bitcoinfast': datetime.datetime(2017,12,26),\n",
    "            'bitcoin2': datetime.datetime(2017,12,28),\n",
    "            'bitcoin-plus': datetime.datetime(2018,1,2),\n",
    "            'bitcoin-interest': datetime.datetime(2018,1,22),\n",
    "            'bitcoin-atom': datetime.datetime(2018,1,24),\n",
    "            'bitcoin-private': datetime.datetime(2018,2,28),\n",
    "            'microbitcoin': datetime.datetime(2018,5,29),\n",
    "            'bitcoin-bep2': datetime.datetime(2018,6,29),\n",
    "            'bitcoin-sv': datetime.datetime(2018,11,11)}\n",
    "\n",
    "    # PULL IN OLD DATA TO BUILD RELEVANT TIMESERIES FOR BTC\n",
    "    df = pd.read_csv('../data/raw/cmc_price_vol_mcap_panel.csv')\n",
    "    df = df[df.cmc_id==1]\n",
    "    df = df[['date', 'usd_per_token', 'usd_volume_24h']]\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "    df = df.drop_duplicates(subset='date')\n",
    "    df = df.reset_index(drop=True)\n",
    "    san_df = pd.read_pickle('../data/raw/santiment_panel.pkl')\n",
    "    san_df = san_df[san_df.san_slug=='bitcoin'][['date', 'active_addresses_24h', 'github_activity',\n",
    "                                                'social_volume_total']].reset_index(drop=True)\n",
    "    hash_df = pd.read_excel('../data/raw/coinmetrics_btc_hashrate.xlsx')\n",
    "    hash_df = hash_df.rename(columns={'Time': 'date',\n",
    "                                    'BTC / Mean Hash Rate': 'hash_rate'})\n",
    "    hash_df['date'] = pd.to_datetime(hash_df['date'])\n",
    "    df = df.merge(san_df,\n",
    "                on=['date'],\n",
    "                how='left',\n",
    "                validate='one_to_one')\n",
    "    df = df.merge(hash_df,\n",
    "                on='date',\n",
    "                how='left',\n",
    "                validate='one_to_one')\n",
    "    del san_df, hash_df\n",
    "\n",
    "    # CLEAN UP THE PANEL\n",
    "\n",
    "    # ensure it has all days\n",
    "    sdate   = datetime.date(2015, 1, 1) \n",
    "    edate   = datetime.date(2021, 12, 31)  \n",
    "    delta   = edate - sdate \n",
    "    days    = []\n",
    "    for i in range(delta.days + 1):\n",
    "        days.append(sdate+timedelta(days=i))\n",
    "    days_df = pd.DataFrame(data={'date':days})\n",
    "    df['date'] = df.date.dt.date\n",
    "    df      = df.merge(days_df,\n",
    "                    on='date',\n",
    "                    how='outer',\n",
    "                    validate='one_to_one')\n",
    "    df = df.sort_values('date')\n",
    "    df = df.interpolate()\n",
    "\n",
    "    # form pct change in all columns and clean up improper values\n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    for col in cols:\n",
    "        df[col] = df[col].pct_change()\n",
    "    df = df.dropna()\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # subset to time period of interest \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df[df.date.dt.year <= 2021]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # rename columns\n",
    "    df = df.rename(columns = {'usd_per_token': 'return',\n",
    "                            'usd_volume_24h': 'usd_volume',\n",
    "                            'active_addresses_24h': 'active_addresses',\n",
    "                            'github_activity': 'developer_activity',\n",
    "                            'social_volume_total': 'social_volume'})\n",
    "\n",
    "    # INITIALIZE RESULTS \n",
    "    cols = list(df.columns.values)\n",
    "    cols.remove('date')\n",
    "    results_df = pd.DataFrame(data={'stat': cols,\n",
    "                                    'window': np.repeat('7 days', len(cols)),\n",
    "                                    'est': np.zeros(len(cols)),\n",
    "                                    'se': np.zeros(len(cols))})\n",
    "\n",
    "    # CALC STAT\n",
    "    for col in cols:\n",
    "        diffs = []\n",
    "        for i in range(len(forks)):\n",
    "            fork_date = list(forks.values())[i]\n",
    "            pre_date  = fork_date-pd.Timedelta(window_size, unit=\"d\")\n",
    "            post_date = fork_date+pd.Timedelta(window_size, unit=\"d\")\n",
    "            pre_mean  = np.mean(df[(df.date >= pre_date) & (df.date < fork_date)][col])\n",
    "            post_mean = np.mean(df[(df.date > fork_date) & (df.date <= post_date)][col])\n",
    "            diff      = post_mean-pre_mean\n",
    "            diffs.append(diff)\n",
    "        results_df.loc[results_df.stat==col, 'est'] = np.mean(diffs)\n",
    "\n",
    "    # CALC STANDARD ERROR (NOTE: 6 MIN RUN TIME)\n",
    "    num_forks = len(forks)\n",
    "    rel_dates = list(df.date.values)[window_size:-window_size]\n",
    "    for col in cols:\n",
    "        # calc all diffs across the panel\n",
    "        diffs = []\n",
    "        for date in rel_dates:\n",
    "            pre_date  = date-pd.Timedelta(31, unit=\"d\")\n",
    "            post_date = date+pd.Timedelta(31, unit=\"d\")\n",
    "            pre_mean  = np.mean(df[(df.date >= pre_date) & (df.date < date)][col])\n",
    "            post_mean = np.mean(df[(df.date > date) & (df.date <= post_date)][col])\n",
    "            diff      = post_mean-pre_mean\n",
    "            diffs.append(diff)\n",
    "\n",
    "        # reorder the list to randomize\n",
    "        random.shuffle(diffs)\n",
    "\n",
    "        # calc bootstrap distribution\n",
    "        def loopOverNumberBootstrapSamples(i):\n",
    "            random_diffs = []\n",
    "            for j in range(num_forks):\n",
    "                index = np.random.randint(low=0,high=len(diffs))\n",
    "                diff  = diffs[index]\n",
    "                random_diffs.append(diff)\n",
    "            return np.mean(random_diffs)\n",
    "        bs_stat = Parallel(n_jobs=num_cpus)(delayed(loopOverNumberBootstrapSamples)(i) for i in range(num_bs_samples))\n",
    "\n",
    "        # calc standard error and add it to results\n",
    "        se = np.std(bs_stat)\n",
    "        results_df.loc[results_df.stat==col, 'se'] = se\n",
    "\n",
    "    # OUTPUT\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        results_df.to_excel(writer, sheet_name='raw_forks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093d7671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 644x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 644x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 644x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    PANEL_IN_FP     = '../data/clean/panel_weekly.pkl' \n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    HIST_OUT_FP     = '../output/desc_stats/histograms.png'\n",
    "    CUM_RET_OUT_FP  = '../output/desc_stats/cumulative_returns.png'\n",
    "    SHARPE_OUT_FP   = '../output/desc_stats/sharpe.png'\n",
    "    TX_OUT_FP       = '../output/desc_stats/btc_tx.png'\n",
    "    HODL_OUT_FP     = '../output/desc_stats/hodl.png'\n",
    "    OUT_FP          = '../output/desc_stats/descriptive_statistics.xlsx'\n",
    "    PERIODS_IN_YEAR = 52\n",
    "    TS_AVG_METHOD   = 'arithmetic'\n",
    "    LHS_COL         = 'r_ex_tp7'\n",
    "    ANNUALIZED      = False\n",
    "\n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # drop rows that are not in the asset universe\n",
    "    df = subsetToAssetUniverse(df, asset_universe_dict)\n",
    "\n",
    "    # generate plots\n",
    "    plotReturnHistograms(df, HIST_OUT_FP)\n",
    "    plotCumulativeReturns(df, CUM_RET_OUT_FP)\n",
    "    plotRollingSharpe(SHARPE_OUT_FP)\n",
    "    plotTransactionStats(df, TX_OUT_FP)\n",
    "    plotHodlingStats(df, HODL_OUT_FP)\n",
    "\n",
    "    # generate tables\n",
    "    genSummaryStatistics(df, LHS_COL, OUT_FP)\n",
    "    genForkStatistics(df, OUT_FP)\n",
    "\n",
    "    # # TODO SCOPE IF RESULTS FOR ALL CHANGE MUCH AFTER A WINSOR\n",
    "    # p1 = df[LHS_COL].quantile(0.01)\n",
    "    # p99 = df[LHS_COL].quantile(0.99)\n",
    "    # df.loc[df[LHS_COL] < p1, LHS_COL] = p1 \n",
    "    # df.loc[df[LHS_COL] > p99, LHS_COL] = p1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
