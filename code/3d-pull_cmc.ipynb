{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6359323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from helper_functions import Helper\n",
    "from typing import Dict, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e335d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formCmcAssetUniverse(base_url: str, base_headers: Dict[str, str], asset_universe: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Form universe of CMC assets mapped to coinmetrics asset ids.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): Base URL for the API.\n",
    "        base_headers (Dict[str, str]): Base headers for the API.  \n",
    "        asset_universe (List[str]): list of strings of coinmetrics asset IDs.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): crosswalk betwen asset_cmc IDs and asset_cm IDs.\n",
    "    \"\"\"\n",
    "    # Set slugs to drop that are duplicates\n",
    "    slugs_to_drop = ['ethercoin', 'unitecoin', 'universe', 'uni-coin', 'unicorn-token', 'bantam',\n",
    "        'stealthcash', 'stox', 'staykx', 'icon-futures', 'global-tour-coin', 'game', 'gastrocoin',\n",
    "        'midnight', 'meta-dance', 'polybit', 'farmpoly', 'poly-maximus', 'blazecoin', 'cartercoin',\n",
    "        'credit-tag-chain', 'culture-ticket-chain', 'cybertronchain', 'cryptocoin', 'calltocombat',\n",
    "        'solcoin', 'sola-token', 'sol-rune---rune-game', 'wrapped-solana', 'plair', 'playchip', \n",
    "        'planet', 'atomic-coin', 'burstocean', 'farmatrust', 'freetip', 'cronos-coin', 'arcoin',\n",
    "        'hinto', 'compound-coin', 'global-rental-token', 'golden-ratio-token', 'aca-token',\n",
    "        'retawars-goldrose-token', 'flux', 'flux-protocol', 'crowdvilla-ownership', 'acash-coin', \n",
    "        'six-dragons-mft', 'metaface', 'my-farm', 'anoncoin', 'aragon-china-token', 'synereo', \n",
    "        'ach', 'supercoin', 'superswap', 'superciety', 'bond', 'bonded-finance', 'bondly-', \n",
    "        'truebit', 'mir-coin', 'mir-token', 'oxy-fi', 'oxycoin', 'rare', 'unique-one', \n",
    "        'rare-finance', 'thorchain-erc20', 'rune', 'rune-farm', 'clevercoin', 'nftx-hashmasks-index',\n",
    "        'investoland', 'apecoin', 'apestrong-finance', 'ape-finance', 'wall-street-apes', 'apelab', \n",
    "        'just-ape', 'miss-ape-yacht-club', 'apemove', 'mercury-protocol', 'gambit-finance', \n",
    "        'gomining-token', 'greekmythology', 'qiswap', 'qidao', 'impact', 'impermax', 'fitmin-finance',\n",
    "        'bat-finance', 'orca-alliance', 'orcadao', 'operand', 'onplanet', 'galatasaray-fan-token', \n",
    "        'gallant', 'polyalpha-finance', 'alpha', 'aavegotchi-alpha', 'subgame', 'rinnegan', 'robinos',\n",
    "        'covicoin', 'covid-cutter', 'coinviewcap', 'genesis-mana', 'uniswap-finance', 'cardanomics',\n",
    "        'kart-racing-league', 'pyroblock', 'atlantis', 'atlas-cloud', 'the-atlas-coin', \n",
    "        'atlas-fc-fan-token', 'snt', 'shib-ninja-token', 'share-nft-token', 'flower-solana', 'cake',\n",
    "        'agrofarm', 'anontoken', 'scarpacoin', 'silver-coin', 'shibchain', 'gas-dao', 'rose',\n",
    "        'metaplanet', 'meta-plane', 'rari-games', 'icecream-finance', 'bobatama', \n",
    "        'virtual-reality-asset', 'playground-waves-floor-index', 'ecowatt', 'stargod', 'jumpn', \n",
    "        'quickswap-new', 'kaisen-inu', 'rising-sun', 'smartlands-network-new', 'omega-finance',\n",
    "        'mechaverse', 'musicfi', 'listenify', 'onlymemes', 'avatly', 'avalon', 't', 'twitfi',\n",
    "        'synergy-diamonds']\n",
    "\n",
    "    # initialize a df for the crosswalk\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # obtain cmc asset ids\n",
    "    endpoint = '/v1/cryptocurrency/map'\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "    for start in [1, 5001, 10001, 15001, 20001]:\n",
    "        params = {'listing_status': 'active,inactive,untracked', 'start': start, 'limit': 5000}\n",
    "        response_json = Helper.makeApiCall(url, headers=base_headers, params=params)\n",
    "        df = pd.concat([df, pd.DataFrame(response_json['data'])])\n",
    "\n",
    "    # subset down to matched assets\n",
    "    df['symbol_lower'] = df.symbol.str.lower()\n",
    "    df = df[df.symbol_lower.isin(asset_universe)]\n",
    "\n",
    "    # remove duplicated assets\n",
    "    df = df[~df.slug.isin(slugs_to_drop)]\n",
    "\n",
    "    # manually add one missing asset\n",
    "    df = pd.concat([df, pd.DataFrame(data={'id': 1567, 'slug': ['nano'], 'symbol_lower': ['nano']})])\n",
    "\n",
    "    # rename\n",
    "    df = df.rename(columns={'slug': 'asset_cmc', 'symbol_lower': 'asset_cm'})\n",
    "\n",
    "    # confirm full one to one mapping\n",
    "    assert len(asset_universe) == np.sum(np.unique(df.asset_cm.values) == np.unique(asset_universe))\n",
    "\n",
    "    # return\n",
    "    return df[['id', 'asset_cmc', 'asset_cm']].sort_values(by='asset_cm', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe04b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullAssetHistoricalMetadata(base_url: str, base_headers: Dict[str, str], \n",
    "    cmc_slug_universe: List[str], study_start: str, study_end: str) -> pd.DataFrame:\n",
    "    \"\"\" Pull metadata on cmc asset universe.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL for the CMC API.\n",
    "        base_headers (Dict[str, str]): A dictionary containing the basic headers for the CMC API call.\n",
    "        cmc_slug_universe (List[str]): A list of cmc slugs that are in our asset universe.        \n",
    "        study_start (str): string time for the start of the study window in format 'YYYY-MM-DD'.\n",
    "        study_end (str): string time for the end of the study window in format 'YYYY-MM-DD'.                               \n",
    "            \n",
    "    Returns:\n",
    "        asset_covars_df (pd.DataFrame): panel data with asset covariates.    \n",
    "    \"\"\"\n",
    "    # extract all dates and add one on the front as we will lag dates by one day\n",
    "    all_dates  = Helper.generateDailyDateList(study_start, study_end)\n",
    "    all_dates = ['2016-06-30'] + all_dates\n",
    "\n",
    "    # initialize metadata for the results\n",
    "    results_dict = {'date': [],\n",
    "                    'slug': [],\n",
    "                    'rank_cmc': [],\n",
    "                    'num_market_pairs_cmc': [],\n",
    "                    'circulating_supply': [],\n",
    "                    'total_supply': [],\n",
    "                    'max_supply': [],\n",
    "                    'tags': [],\n",
    "                    'platform': [],\n",
    "                    'tvl_ratio': []}\n",
    "\n",
    "    # form url\n",
    "    endpoint = '/v1/cryptocurrency/listings/historical'\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    # form params\n",
    "    params = {'convert': 'USD',\n",
    "              'limit': 5000,\n",
    "              'aux': 'platform,tags,circulating_supply,total_supply,max_supply,cmc_rank,num_market_pairs'}\n",
    "\n",
    "    # loop over all dates except the last date we are lagging days by one\n",
    "    for i in range(len(all_dates[:-1])):\n",
    "        # update date to pull\n",
    "        current_date = all_dates[i]\n",
    "        params['date'] = current_date\n",
    "\n",
    "        # monitor progress\n",
    "        print(f\"Processing date number #{i+1} ({(i+1)/len(all_dates)*100:.2f}%): {current_date}\")\n",
    "            \n",
    "        # make the call for all assets and append\n",
    "        for start in [1, 5001, 10001, 15001, 20001]:\n",
    "            params['start'] = start\n",
    "            response_json = Helper.makeApiCall(url, headers=base_headers, params=params)\n",
    "            try:\n",
    "                for result in response_json['data']:\n",
    "                    if result['slug'] in cmc_slug_universe:\n",
    "                        results_dict['date'].append(all_dates[i+1]) # note: this info is updated at end of this utc so midnight of next\n",
    "                        results_dict['slug'].append(result['slug'])\n",
    "                        results_dict['rank_cmc'].append(result['cmc_rank'])\n",
    "                        results_dict['num_market_pairs_cmc'].append(result['num_market_pairs'])\n",
    "                        results_dict['circulating_supply'].append(result['circulating_supply'])\n",
    "                        results_dict['total_supply'].append(result['total_supply'])\n",
    "                        results_dict['max_supply'].append(result['max_supply'])\n",
    "                        results_dict['tags'].append(result['tags'])\n",
    "                        results_dict['platform'].append(result['platform'])\n",
    "                        results_dict['tvl_ratio'].append(result['tvl_ratio'])\n",
    "            except:\n",
    "                print(f\"No data for starting at {start} for date {current_date}\")\n",
    "\n",
    "            # space out calls\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    # build dataframe to return\n",
    "    return pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff44d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullAssetPriceVolumeMcap(base_url: str, base_headers: Dict[str, str], \n",
    "                             study_start: str, study_end: str, \n",
    "                             cmc_df: pd.DataFrame, daily_panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Pull price, volume, and mcap data at hourly freq for cmc asset universe in cmc_df.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL for the CMC API.\n",
    "        base_headers (Dict[str, str]): A dictionary containing the basic headers for the CMC API call.  \n",
    "        study_start (str): string time for the start of the study window in format 'YYYY-MM-DD'.\n",
    "        study_end (str): string time for the end of the study window in format 'YYYY-MM-DD'.     \n",
    "        cmc_df (pd.DataFrame): crosswalk between cmc ids and CoinMetrics asset ids.\n",
    "        daily_panel_df (pd.DataFrame): panel data with CoinMetrics asset ids to use for \n",
    "                                       start and end date of each asset.                          \n",
    "            \n",
    "    Returns:\n",
    "        df (pd.DataFrame): panel data with asset price, 24h volume, and mcap at hourly frequency.\n",
    "    \"\"\"\n",
    "    # convert strings to datetimes\n",
    "    study_start_dt = np.datetime64(study_start)\n",
    "    study_end_dt = np.datetime64(study_end)\n",
    "\n",
    "    # initialze asset list to pull\n",
    "    cmc_asset_ids = list(cmc_df.id.values)\n",
    "\n",
    "    # initialize dict for the results\n",
    "    results_dict = {'date': [],\n",
    "                    'cmc_id': [],\n",
    "                    'usd_per_token': [],\n",
    "                    'usd_volume_24h': [],\n",
    "                    'usd_mcap': []}\n",
    "\n",
    "    # form url\n",
    "    endpoint = '/v3/cryptocurrency/quotes/historical'\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    # form parameters dictionary\n",
    "    params = {'count': 10000,\n",
    "              'interval': '1h',\n",
    "              'aux': 'price,volume,market_cap,quote_timestamp'}\n",
    "\n",
    "    # loop over the assets\n",
    "    num_assets = len(cmc_asset_ids)\n",
    "    for i in range(num_assets):\n",
    "        # update asset\n",
    "        cmc_asset_id = cmc_asset_ids[i]\n",
    "        params['id'] = str(cmc_asset_id)\n",
    "\n",
    "        # monior progress\n",
    "        print(f\"Processing asset number #{i+1} ({(i+1)/num_assets*100:.2f}%).\")\n",
    "\n",
    "        # form list of all dates for this asset\n",
    "        asset_cm = cmc_df[cmc_df.id==cmc_asset_id].asset_cm.values[0]\n",
    "        asset_dates = daily_panel_df[daily_panel_df.asset==asset_cm].date.values\n",
    "        asset_min_date = np.min(asset_dates)\n",
    "        asset_max_date = np.max(asset_dates)\n",
    "        if study_start_dt >= asset_min_date:\n",
    "            start_date = np.datetime_as_string(study_start_dt, 'D')\n",
    "        else:\n",
    "            start_date = np.datetime_as_string(asset_min_date, 'D')\n",
    "        if study_end_dt <= asset_max_date:\n",
    "            end_date = np.datetime_as_string(study_end_dt, 'D') \n",
    "        else:\n",
    "            end_date = np.datetime_as_string(asset_max_date, 'D') \n",
    "\n",
    "        # extract dates for this asset\n",
    "        date_list  = Helper.generateYearlyCalendarYearDateList(start_date, end_date)\n",
    "\n",
    "        # loop over the dates for this asset\n",
    "        for j in range(len(date_list)-1):\n",
    "            # update params for these dates\n",
    "            params['time_start'] = date_list[j]\n",
    "            params['time_end']   = date_list[j+1]\n",
    "\n",
    "            # make the call\n",
    "            response_json = Helper.makeApiCall(url, headers=base_headers, params=params)\n",
    "            \n",
    "            # process the data\n",
    "            if isinstance(response_json, dict):\n",
    "                if  response_json['data'] is not None:\n",
    "                    for quote in response_json['data'][str(cmc_asset_id)]['quotes']:\n",
    "                        results_dict['date'].append(quote['quote']['USD']['timestamp'])\n",
    "                        results_dict['cmc_id'].append(cmc_asset_id) \n",
    "                        results_dict['usd_per_token'].append(quote['quote']['USD']['price'])\n",
    "                        results_dict['usd_volume_24h'].append(quote['quote']['USD']['volume_24h'])\n",
    "                        results_dict['usd_mcap'].append(quote['quote']['USD']['market_cap'])\n",
    "            else:\n",
    "                print(f'No data for {cmc_asset_id} for date {date_list[j]}')\n",
    "                continue\n",
    "\n",
    "            # space out the calls\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    # build dataframe to return\n",
    "    df = pd.DataFrame(results_dict)  \n",
    "    df = df.drop_duplicates(subset=['date', 'cmc_id'])\n",
    "    df = df.merge(cmc_df[['id', 'asset_cmc']], left_on='cmc_id', right_on='id', how='left', validate='many_to_one')\n",
    "    df = df[['date', 'asset_cmc', 'usd_per_token', 'usd_volume_24h', 'usd_mcap']]\n",
    "    df = df.sort_values(by=['date', 'asset_cmc'], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fcdb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullMacro(base_url: str, base_headers: Dict[str, str], \n",
    "              study_start: str, study_end: str) -> pd.DataFrame:\n",
    "    \"\"\" Pull macro data for the study period.\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): The base URL for the CMC API.\n",
    "        base_headers (Dict[str, str]): A dictionary containing the basic headers for the CMC API call.  \n",
    "        study_start (str): string time for the start of the study window in format 'YYYY-MM-DD'.\n",
    "        study_end (str): string time for the end of the study window in format 'YYYY-MM-DD'.    \n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): timeseries data for the available macro covariates from CMC.\n",
    "    \"\"\"\n",
    "    # initialize dict for the results\n",
    "    results_dict = {'date': [],\n",
    "                    'total_usd_mcap': [],\n",
    "                    'altcoin_usd_mcap': [],\n",
    "                    'total_usd_volume_24h': [],\n",
    "                    'altcoin_usd_volume_24h': [],\n",
    "                    'active_cryptos': [],\n",
    "                    'active_exchanges': [],\n",
    "                    'active_market_pairs': [],\n",
    "                    'btc_dominance': []}\n",
    "\n",
    "    # form url\n",
    "    endpoint = '/v1/global-metrics/quotes/historical'\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    # form parameters dictionary\n",
    "    params = {'count': 10000,\n",
    "              'interval': '1h',\n",
    "              'aux': \"btc_dominance,active_cryptocurrencies,active_exchanges,active_market_pairs,total_volume_24h,altcoin_market_cap,altcoin_volume_24h\"}\n",
    "\n",
    "    # form the yearly dates thing\n",
    "    date_list  = Helper.generateYearlyCalendarYearDateList(study_start, study_end)\n",
    "\n",
    "    # loop over dates to pull\n",
    "    for i in range(len(date_list)-1):\n",
    "        # set dates\n",
    "        params['time_start'] = date_list[i]\n",
    "        params['time_end'] = date_list[i+1]\n",
    "\n",
    "        # make the call\n",
    "        response_json = Helper.makeApiCall(url, headers=base_headers, params=params)\n",
    "        assert(type(response_json)==dict)\n",
    "        assert('data' in response_json.keys())\n",
    "        for quote in response_json['data']['quotes']:\n",
    "            results_dict['date'].append(quote['timestamp'])\n",
    "            results_dict['total_usd_mcap'].append(quote['quote']['USD']['total_market_cap'])\n",
    "            results_dict['altcoin_usd_mcap'].append(quote['quote']['USD']['altcoin_volume_24h'])\n",
    "            results_dict['total_usd_volume_24h'].append(quote['quote']['USD']['total_volume_24h'])\n",
    "            results_dict['altcoin_usd_volume_24h'].append(quote['quote']['USD']['altcoin_volume_24h'])\n",
    "            results_dict['active_cryptos'].append(quote['active_cryptocurrencies'])\n",
    "            results_dict['active_exchanges'].append(quote['active_exchanges'])\n",
    "            results_dict['active_market_pairs'].append(quote['active_market_pairs'])\n",
    "            results_dict['btc_dominance'].append(quote['btc_dominance'])\n",
    "\n",
    "    # build the dataframe to return\n",
    "    df = pd.DataFrame(results_dict)\n",
    "    df = df.drop_duplicates(subset=['date'])\n",
    "    df = df.sort_values(by=['date'], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c791c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullCMCExchangeHistoricalData(base_url: str, base_headers: Dict[str, str], \n",
    "                                  study_start: str, study_end: str) -> pd.DataFrame:\n",
    "    \"\"\" Pull exchange data.\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): The base URL for the CMC API.\n",
    "        base_headers (Dict[str, str]): A dictionary containing the basic headers for the CMC API call.  \n",
    "        study_start (str): string time for the start of the study window in format 'YYYY-MM-DD'.\n",
    "        study_end (str): string time for the end of the study window in format 'YYYY-MM-DD'.    \n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): panel data frame of exchange covariates.\n",
    "    \"\"\"\n",
    "    # convert strings to datetimes\n",
    "    study_start_dt = np.datetime64(study_start)\n",
    "    study_end_dt = np.datetime64(study_end)\n",
    "\n",
    "    # specify legit exchanges that we will consider\n",
    "    legit_exchanges = ['aave', 'balancer-v2', 'bancor-network', 'binance', 'binance-us',\n",
    "                       'bitfinex', 'bitmex', 'bitstamp', 'coinbase-exchange',\n",
    "                       'compound', 'crypto-com-exchange', 'curve-finance', 'deribit',\n",
    "                       'dydx', 'ftx', 'ftx-us', 'gemini', 'huobi', 'kraken', 'kucoin',\n",
    "                       'okcoin', 'pancakeswap-v2', 'poloniex', 'sushiswap',\n",
    "                       'uniswap-v2', 'uniswap-v3']\n",
    "\n",
    "    # obtain exchange mapping ids\n",
    "    endpoint = '/v1/exchange/map'\n",
    "    url      = f\"{base_url}{endpoint}\"\n",
    "    params   = {'listing_status': 'active,inactive'}\n",
    "    response_json = Helper.makeApiCall(url, headers=base_headers, params=params)\n",
    "    exchanges_dict = {'slug': [],\n",
    "                      'id': [],\n",
    "                      'first_date': [],\n",
    "                      'last_date': []}\n",
    "    for ex in response_json['data']:\n",
    "        if ex['slug'] in legit_exchanges:\n",
    "            exchanges_dict['slug'].append(ex['slug'])\n",
    "            exchanges_dict['id'].append(ex['id'])\n",
    "            exchanges_dict['first_date'].append(ex['first_historical_data'])\n",
    "            exchanges_dict['last_date'].append(ex['last_historical_data'])\n",
    "    exchanges_df = pd.DataFrame(exchanges_dict)\n",
    "    \n",
    "    # build url and params for exchange historical data\n",
    "    endpoint = '/v1/exchange/quotes/historical'\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "    params = {'interval': '1h',\n",
    "              'count': 10000}\n",
    "\n",
    "    # initialize dict for ex data\n",
    "    ex_data_dict = {'date': [],\n",
    "                    'ex_slug': [],\n",
    "                    'ex_usd_volume_24h': [],\n",
    "                    'ex_num_market_pairs': []}\n",
    "\n",
    "    # loop over all exchanges\n",
    "    ex_ids = list(exchanges_df.id.values)\n",
    "    for ex_id in ex_ids:\n",
    "        # Monitor progress\n",
    "        ex_slug = exchanges_df[exchanges_df.id==ex_id].slug.values[0]\n",
    "        print(f\"Working on {ex_slug}.\")\n",
    "\n",
    "        # Add ex id to params\n",
    "        params['id'] = ex_id\n",
    "\n",
    "        # Determine start and end date and break into years\n",
    "        ex_min_date = exchanges_df[exchanges_df.id==ex_id].first_date.values[0]\n",
    "        ex_max_date = exchanges_df[exchanges_df.id==ex_id].last_date.values[0]\n",
    "        ex_min_date_dt = np.datetime64(ex_min_date[:10])\n",
    "        ex_max_date_dt = np.datetime64(ex_max_date[:10])\n",
    "        if study_start_dt >= ex_min_date_dt:\n",
    "            start_date = np.datetime_as_string(study_start_dt, 'D')\n",
    "        else:\n",
    "            start_date = np.datetime_as_string(ex_min_date_dt, 'D')\n",
    "        if study_end_dt <= ex_max_date_dt:\n",
    "            end_date = np.datetime_as_string(study_end_dt, 'D') \n",
    "        else:\n",
    "            end_date = np.datetime_as_string(ex_max_date_dt, 'D') \n",
    "\n",
    "        # extract dates for this exchange\n",
    "        date_list  = Helper.generateYearlyCalendarYearDateList(start_date, end_date)\n",
    "\n",
    "        # loop over date list\n",
    "        for j in range(len(date_list)-1):\n",
    "            # update params\n",
    "            params['time_start'] = date_list[j]\n",
    "            params['time_end'] = date_list[j+1]\n",
    "\n",
    "            # make the call\n",
    "            response_json = Helper.makeApiCall(url, headers=base_headers, params=params)\n",
    "\n",
    "            # extract the data to a dict\n",
    "            for ex in response_json['data']['quotes']:\n",
    "                ex_data_dict['date'].append(ex['quote']['USD']['timestamp'])\n",
    "                ex_data_dict['ex_slug'].append(ex_slug)\n",
    "                ex_data_dict['ex_usd_volume_24h'].append(ex['quote']['USD']['volume_24h'])\n",
    "                ex_data_dict['ex_num_market_pairs'].append(ex['num_market_pairs'])\n",
    "            \n",
    "            # space out the calls\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    # build the dataframe to return\n",
    "    df = pd.DataFrame(ex_data_dict)\n",
    "    df = df.drop_duplicates(subset=['date', 'ex_slug'])\n",
    "    df = df.sort_values(by=['date', 'ex_slug'], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "05d01a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formFinalPanel(panel_df: pd.DataFrame, assets_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" add the asset covars to the main panel of price, volume, and mcap.\n",
    "    \n",
    "    Args:\n",
    "        panel_df (pd.DataFrame): panel with asset price, trading volume, and mcap.\n",
    "        assets_df (pd.DataFrame): panel with asset metadata.\n",
    "    \n",
    "    Returns:\n",
    "        panel_df (pd.DataFrame): panel with all asset covariates.\n",
    "    \"\"\"\n",
    "    # prep each df for the merge\n",
    "    assets_df = assets_df.drop_duplicates(subset=['date', 'slug'])\n",
    "    assets_df = assets_df.rename(columns={'slug': 'asset_cmc'})\n",
    "    assets_df['date_day'] = pd.to_datetime(assets_df['date'], utc=True).dt.tz_localize(None)\n",
    "    assets_df = assets_df.drop('date', axis=1)\n",
    "    panel_df['date'] = pd.to_datetime(panel_df['date'], utc=True).dt.tz_localize(None)\n",
    "    panel_df['date_day'] = panel_df.date.dt.floor(\"D\") - pd.Timedelta(hours=24)\n",
    "\n",
    "    # merge\n",
    "    panel_df = panel_df.merge(assets_df,\n",
    "                            on=['date_day', 'asset_cmc'],\n",
    "                            validate='many_to_one',\n",
    "                            how='left')\n",
    "    \n",
    "    # clean up\n",
    "    panel_df = panel_df.drop('date_day', axis=1)\n",
    "    panel_df = panel_df.sort_values(by=['date', 'asset_cmc'], ignore_index=True)\n",
    "    \n",
    "    return panel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445d559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set args\n",
    "    CW_IN_FP = '../data/derived/cm_to_coinapi_cw.pkl'\n",
    "    ASSET_IN_FP = '../data/clean/asset_universe_dict.pickle'\n",
    "    PANEL_DAILY_IN_FP = '../data/derived/basic_panel.pkl'\n",
    "    API_FP = '../../admin/cmc.txt'\n",
    "    STUDY_START = '2016-07-01'\n",
    "    STUDY_END = '2023-01-02'\n",
    "    BASE_URL = \"https://pro-api.coinmarketcap.com\"\n",
    "    PANEL_OUT_FP = \"../data/raw/cmc_panel.pkl\"\n",
    "    CW_OUT_FP = '../data/raw/cmc_coinmetrics_cw.pkl'\n",
    "    MACRO_OUT_FP = '../data/raw/cmc_macro.pkl'\n",
    "    EX_OUT_FP = '../data/raw/cmc_exchange_panel.pkl'\n",
    "\n",
    "    # Import asset universe and cw\n",
    "    cw_df = pd.read_pickle(CW_IN_FP)\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    asset_universe = Helper.findUniqueAssets(asset_universe_dict)\n",
    "    daily_panel_df = pd.read_pickle(PANEL_DAILY_IN_FP)\n",
    "\n",
    "    # import api key and set base parameters\n",
    "    with open(API_FP) as f:\n",
    "        API_KEY = f.readlines()\n",
    "        API_KEY = API_KEY[0].strip()\n",
    "    BASE_HEADERS = {'Accepts': 'application/json', 'X-CMC_PRO_API_KEY': API_KEY}\n",
    "\n",
    "    # Form crosswalk\n",
    "    cmc_df = formCmcAssetUniverse(BASE_URL, BASE_HEADERS, asset_universe)\n",
    "    cmc_df[['asset_cmc', 'asset_cm']].to_pickle(CW_OUT_FP)\n",
    "    cmc_slug_universe = list(cmc_df.asset_cmc.values)\n",
    "\n",
    "    # Pull historical asset metadata\n",
    "    assets_df = pullAssetHistoricalMetadata(BASE_URL, BASE_HEADERS, \n",
    "                                            cmc_slug_universe, STUDY_START, STUDY_END)\n",
    "\n",
    "    # Pull asset price volume and mcap data\n",
    "    panel_df = pullAssetPriceVolumeMcap(BASE_URL, BASE_HEADERS, \n",
    "                                        STUDY_START, STUDY_END,\n",
    "                                        cmc_df, daily_panel_df)\n",
    "    \n",
    "    # Form final panel and save\n",
    "    panel_df = formFinalPanel(panel_df, assets_df)\n",
    "    panel_df.to_pickle(PANEL_OUT_FP)\n",
    "    \n",
    "    # Pull macro data\n",
    "    macro_df = pullMacro(BASE_URL, BASE_HEADERS, STUDY_START, STUDY_END)\n",
    "    macro_df.to_pickle(MACRO_OUT_FP)\n",
    "    ex_df = pullCMCExchangeHistoricalData(BASE_URL, BASE_HEADERS, STUDY_START, STUDY_END)\n",
    "    ex_df.to_pickle(EX_OUT_FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
