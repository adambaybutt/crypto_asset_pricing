{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aabc20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# IMPORT PACKAGES\n",
    "from tools import QuantTools\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7547b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniverse(df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset a DataFrame based on a dictionary of asset universes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame. Must contain columns \"date\" and \"asset\".\n",
    "    asset_universe_dict : Dict[str, List[str]]\n",
    "        A dictionary where keys are dates in 'YYYY-MM-DD' format and values are lists of asset names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The subsetted DataFrame.\n",
    "    \"\"\"\n",
    "    # Check that the required columns are present in the DataFrame\n",
    "    if not set(['date', 'asset']).issubset(df.columns):\n",
    "        raise ValueError('Input DataFrame must contain \"date\" and \"asset\" columns.')\n",
    "\n",
    "    # Ensure that the 'date' column is of datetime type\n",
    "    if df['date'].dtype != 'datetime64[ns]':\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Loop over all months with their relevant assets\n",
    "    for key, values in asset_universe_dict.items():\n",
    "        # Extract the year and month from the key\n",
    "        year, month = key.split('-')[:2]\n",
    "\n",
    "        # Drop rows from the dataframe which match the year and month but not the assets\n",
    "        df = df[~((df.date.dt.year == int(year)) \n",
    "                    & (df.date.dt.month == int(month)) \n",
    "                    & (~df.asset.isin(values)))]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17784795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windsorize(df: pd.DataFrame, lhs_col: str, clip_prctl: float=0.01) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Windsorize the values in the specified column of a DataFrame.\n",
    "    \n",
    "    This function replaces values below the clip_prctl percentile with that percentile's\n",
    "    value, and the same for the values about the 1-clip_prctl percentile.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the column to be windsorized.\n",
    "    - lhs_col: Name of the column to be windsorized.\n",
    "    - clip_prctl: Percentile of left tail to clip below and \n",
    "                    same for 1-clip_prctl on the right tail.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with windsorized values.\n",
    "    \"\"\"\n",
    "    # Calculate quantiles\n",
    "    p_left = df[lhs_col].quantile(clip_prctl)\n",
    "    p_right = df[lhs_col].quantile(1-clip_prctl)\n",
    "\n",
    "    # Windsorize\n",
    "    df.loc[df[lhs_col] < p_left, lhs_col] = p_left\n",
    "    df.loc[df[lhs_col] > p_right, lhs_col] = p_right\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7188ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPortfolioSortResultsTable(df: pd.DataFrame, rhs_col: str, lhs_col: str, \n",
    "    ts_avg_method: str, annualized: bool, periods_in_year: int, num_quantiles: int,\n",
    "    mcap_weighted: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Forms a portfolio sort results table based given number of quantiles sorted by given RHS col for the input DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - rhs_col (str): The right-hand side column name to use to form quantiles.\n",
    "    - lhs_col (str): The left-hand side column name of future returns.\n",
    "    - ts_avg_method (str): The method for calculating the time series average, either 'arithmetic' or 'geometric'.\n",
    "    - annualized (bool): If True, the results are annualized.\n",
    "    - periods_in_year (int): Number of periods in a year (e.g., 52 for weekly data).\n",
    "    - num_quantiles (int): Number of quantiles for ranking.\n",
    "    - mcap_weighted (bool): Whether to mcap weight or just equally weight.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the portfolio sort results.\n",
    "    \"\"\"\n",
    "    # Check for valid input\n",
    "    assert ts_avg_method in ['arithmetic', 'geometric'], \"Incorrect input for the ts_avg_method.\"\n",
    "\n",
    "    # Form relevant df\n",
    "    if mcap_weighted == True:\n",
    "        t_df = df[['date', 'asset', lhs_col, rhs_col, 'mcap']].copy()\n",
    "    else:\n",
    "        t_df = df[['date', 'asset', lhs_col, rhs_col]].copy()\n",
    "\n",
    "    # Randomly sort all rows of the dataframe\n",
    "    t_df = t_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Sort the dataframe by 'date' and rhs_col column\n",
    "    t_df = t_df.sort_values(['date', rhs_col])\n",
    "\n",
    "    # Form quantiles\n",
    "    t_df['rank_within_date'] = t_df.groupby('date')[rhs_col].rank(method='first')\n",
    "    t_df['rank_ratio'] = t_df.groupby('date')['rank_within_date'].transform(lambda x: x / x.max())\n",
    "    quantile_bins = list(np.arange(0, num_quantiles+1)/num_quantiles)\n",
    "    t_df['quant'] = 1+pd.cut(t_df['rank_ratio'], bins=quantile_bins, labels=False, include_lowest=True)\n",
    "    t_df = t_df.drop(columns=['rank_within_date', 'rank_ratio'])\n",
    "\n",
    "    # Calculate the average return for each quantile within each date\n",
    "    if mcap_weighted == True:\n",
    "        t_df['weighted_return'] = t_df[lhs_col] * t_df.mcap\n",
    "        grouped_df = t_df.groupby(['date', 'quant'])[['weighted_return', 'mcap']].sum().reset_index()\n",
    "        grouped_df[lhs_col] = grouped_df['weighted_return'] / grouped_df['mcap']\n",
    "        date_quantile_avg_returns_df = grouped_df[['date', 'quant', lhs_col]].copy()\n",
    "    else:\n",
    "        date_quantile_avg_returns_df = t_df.groupby(['date', 'quant'])[[lhs_col]].mean().reset_index()\n",
    "\n",
    "    # Calculate the time series average of each quantile's average returns\n",
    "    if ts_avg_method == 'geometric':\n",
    "        quantile_avg_returns = date_quantile_avg_returns_df.groupby('quant')[lhs_col].apply(lambda x: QuantTools.calcGeomAvg(x, annualized=annualized, periods_in_year=periods_in_year))\n",
    "    else:\n",
    "        quantile_avg_returns = date_quantile_avg_returns_df.groupby('quant')[lhs_col].apply(lambda x: QuantTools.calcTSAvgReturn(x, annualized=annualized, periods_in_year=periods_in_year))\n",
    "\n",
    "    # Calculate the time series average for each year\n",
    "    date_quantile_avg_returns_df['year'] = date_quantile_avg_returns_df['date'].dt.year\n",
    "    if ts_avg_method == 'geometric':\n",
    "        yearly_avg_returns = date_quantile_avg_returns_df.groupby(['year', 'quant'])[lhs_col].apply(lambda x: QuantTools.calcGeomAvg(x, annualized=annualized, periods_in_year=periods_in_year)).unstack(level=1)\n",
    "    else:\n",
    "        yearly_avg_returns = date_quantile_avg_returns_df.groupby(['year', 'quant'])[lhs_col].apply(lambda x: QuantTools.calcTSAvgReturn(x, annualized=annualized, periods_in_year=periods_in_year)).unstack(level=1)\n",
    "\n",
    "    # Calculate the t statistics for the overall period\n",
    "    t_stats = (np.sqrt(len(date_quantile_avg_returns_df)/num_quantiles)*date_quantile_avg_returns_df.groupby('quant')[lhs_col].apply(lambda x: QuantTools.calcTSAvgReturn(x, annualized=False)) \n",
    "                / date_quantile_avg_returns_df.groupby('quant')[lhs_col].apply(lambda x: QuantTools.calcSD(x, annualized=False)))\n",
    "\n",
    "    # Calculate the time series average of the difference between the top and bottom quantile's average returns\n",
    "    top_quantile = num_quantiles\n",
    "    bottom_quantile = 1\n",
    "    diff_date_quantile_avg_returns_df = date_quantile_avg_returns_df.pivot_table(index='date', columns='quant', values=lhs_col)\n",
    "    diff_date_quantile_avg_returns_df['year'] = diff_date_quantile_avg_returns_df.index.year\n",
    "    diff_date_quantile_avg_returns_df['top_bottom_diff'] = (diff_date_quantile_avg_returns_df[top_quantile] \n",
    "                                                            - diff_date_quantile_avg_returns_df[bottom_quantile])\n",
    "    if ts_avg_method == 'geometric':\n",
    "        top_bottom_diff_average = QuantTools.calcGeomAvg(diff_date_quantile_avg_returns_df['top_bottom_diff'], annualized=annualized, periods_in_year=periods_in_year)\n",
    "    else:\n",
    "        top_bottom_diff_average = QuantTools.calcTSAvgReturn(diff_date_quantile_avg_returns_df['top_bottom_diff'], annualized=annualized, periods_in_year=periods_in_year)\n",
    "\n",
    "    # Calculate the yearly top_bottom_diff\n",
    "    if ts_avg_method == 'geometric':\n",
    "        yearly_diff_avg_returns = diff_date_quantile_avg_returns_df.groupby('year')['top_bottom_diff'].apply(lambda x: QuantTools.calcGeomAvg(x, annualized=annualized, periods_in_year=periods_in_year))\n",
    "    else:\n",
    "        yearly_diff_avg_returns = diff_date_quantile_avg_returns_df.groupby('year')['top_bottom_diff'].apply(lambda x: QuantTools.calcTSAvgReturn(x, annualized=annualized, periods_in_year=periods_in_year))\n",
    "\n",
    "    # Calculate the overall t stat for the top minus bottom portfolio\n",
    "    t_stat_top_bottom_diff = (np.sqrt(len(diff_date_quantile_avg_returns_df))*QuantTools.calcTSAvgReturn(diff_date_quantile_avg_returns_df['top_bottom_diff'], annualized=False)\n",
    "                    / QuantTools.calcSD(diff_date_quantile_avg_returns_df['top_bottom_diff'], annualized=False))\n",
    "\n",
    "    # Combine results\n",
    "    results = np.round(yearly_avg_returns.copy(), 4)\n",
    "    results.loc['all'] = np.round(quantile_avg_returns, 4)\n",
    "    results.loc['t_stat'] = np.round(t_stats, 2)\n",
    "    top_bottom_diff_col = str(top_quantile)+'-'+str(bottom_quantile)\n",
    "    results[top_bottom_diff_col] = yearly_diff_avg_returns\n",
    "    results.loc['t_stat', top_bottom_diff_col] = np.round(t_stat_top_bottom_diff, 2)\n",
    "    top_bottom_diff_avg_rounded = np.round(top_bottom_diff_average, 4)\n",
    "    results['rhs_col'] = rhs_col\n",
    "    results['sig'] = 'yes'\n",
    "    if (np.abs(t_stat_top_bottom_diff) > 2.576):\n",
    "        results.loc['all', top_bottom_diff_col] = str(top_bottom_diff_avg_rounded)+\"***\"\n",
    "    elif (np.abs(t_stat_top_bottom_diff) > 1.96):\n",
    "        results.loc['all', top_bottom_diff_col] = str(top_bottom_diff_avg_rounded)+\"**\"\n",
    "    elif (np.abs(t_stat_top_bottom_diff) > 1.645):\n",
    "        results.loc['all', top_bottom_diff_col] = str(top_bottom_diff_avg_rounded)+\"*\"\n",
    "    else:\n",
    "        results.loc['all', top_bottom_diff_col] = str(top_bottom_diff_avg_rounded)\n",
    "        results['sig'] = 'no'\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f60e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcUnivariateFactorResults(df: pd.DataFrame, rhs_cols: List[str],\n",
    "    lhs_col: str, ts_avg_method: str, annualized: bool, periods_in_year: int,\n",
    "    num_quantiles: int, mcap_weighted: bool, out_fp: str, out_sheet: str) -> None:\n",
    "    # Remove vc column as will handle separately\n",
    "    if 'char_vc_t' in rhs_cols:\n",
    "        rhs_cols.remove('char_vc_t')\n",
    "        \n",
    "    # Form results\n",
    "    results_df = pd.DataFrame()\n",
    "    for rhs_col in rhs_cols:\n",
    "        result = formPortfolioSortResultsTable(df, rhs_col, lhs_col, \n",
    "                    ts_avg_method, annualized, periods_in_year, num_quantiles, mcap_weighted)\n",
    "        results_df = pd.concat([results_df, result])\n",
    "\n",
    "    # Separate results\n",
    "    sig_chars = list(set(results_df[results_df.sig=='yes'].rhs_col.values))\n",
    "    sig_results_df = results_df[results_df.rhs_col.isin(sig_chars)]\n",
    "    insig_results_df = results_df[~results_df.rhs_col.isin(sig_chars)]\n",
    "\n",
    "    # Save results\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        sheet_name = out_sheet+'_sig'\n",
    "        sig_results_df.to_excel(writer, sheet_name=sheet_name)\n",
    "        sheet_name = out_sheet+'_insig'\n",
    "        insig_results_df.to_excel(writer, sheet_name=sheet_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4895bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    PANEL_IN_FP     = '../data/clean/panel_weekly.pkl' \n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    OUT_FP          = '../output/low_dim_fm/univariate_factor_analysis.xlsx'\n",
    "    OUT_SHEET       = 'raw_uni'\n",
    "    PERIODS_IN_YEAR = 52\n",
    "    TS_AVG_METHOD   = 'arithmetic'\n",
    "    LHS_COL         = 'r_ex_tp7'\n",
    "    ANNUALIZED      = False\n",
    "    NUM_QUANTILES   = 5\n",
    "    WINDSORIZE      = False\n",
    "    MCAP_WEIGHTED   = True\n",
    "    RHS_COLS = ['char_tx_volume_tm7',\n",
    "        'char_addr_active_tm7',\n",
    "        'char_addr_new_log_delta_tm14_tm7',\n",
    "        'char_addr_new_tm7',\n",
    "        'char_addr_total_t',\n",
    "        'char_circulation_tm7',\n",
    "        'char_age_destroyed_tm7',\n",
    "        'char_delta_flow_dist_tm7',\n",
    "        'char_delta_holders_dist_tm7',\n",
    "        'char_prct_supply_in_profit_t',\n",
    "        'char_cex_prct_circ_supply_t',\n",
    "        'char_dex_prct_circ_supply_t',\n",
    "        'char_defi_prct_circ_supply_t',\n",
    "        'char_traders_prct_circ_supply_t',\n",
    "        'char_exchange_inflow_tm7',\n",
    "        'char_exchange_outflow_tm7',\n",
    "        'char_num_pairs_t',\n",
    "        'char_social_volume_tm7',\n",
    "        'char_social_volume_reddit_tm7',\n",
    "        'char_social_volume_twitter_tm7',\n",
    "        'char_sent_pos_reddit_tm7',\n",
    "        'char_sent_pos_twitter_tm7',\n",
    "        'char_sent_neg_reddit_tm7',\n",
    "        'char_sent_neg_twitter_tm7',\n",
    "        'char_dev_activity_tm7',\n",
    "        'char_vc_t',\n",
    "        'char_r_tm7',\n",
    "        'char_r_tm14',\n",
    "        'char_r_tm30',\n",
    "        'char_r_tm60',\n",
    "        'char_r_tm90',\n",
    "        'char_r_tm14_tm7',\n",
    "        'char_r_tm30_tm14',\n",
    "        'char_r_tm90_tm30',\n",
    "        'char_r_ath_t',\n",
    "        'char_r_atl_t',\n",
    "        'char_r_industry_tm30',\n",
    "        'char_r_industry_tm60',\n",
    "        'char_trades_sum_tm7',\n",
    "        'char_volume_sum_tm7',\n",
    "        'char_spread_bps_t',\n",
    "        'char_ask_size_t',\n",
    "        'char_bid_size_t',\n",
    "        'char_illiq_tm7',\n",
    "        'char_turnover_tm7',\n",
    "        'char_price_t',\n",
    "        'char_size_t',\n",
    "        'char_mvrv_t',\n",
    "        'char_alpha_tm7',\n",
    "        'char_alpha_tm30',\n",
    "        'char_beta_tm7',\n",
    "        'char_beta_tm30',\n",
    "        'char_beta_downside_tm30',\n",
    "        'char_coskew_tm30',\n",
    "        'char_iskew_tm30',\n",
    "        'char_shortfall5_tm7',\n",
    "        'char_var5_tm7',\n",
    "        'char_vol_tm7',\n",
    "        'char_vol_tm30',\n",
    "        'char_vol_tm90',\n",
    "        'char_ivol_tm7',\n",
    "        'char_ivol_tm30',\n",
    "        'char_ivol_tm90']\n",
    "\n",
    "    # edit fp based on args\n",
    "    if NUM_QUANTILES == 2:\n",
    "        OUT_SHEET += '_bi'\n",
    "    elif NUM_QUANTILES == 3:\n",
    "        OUT_SHEET += '_ter'\n",
    "    elif NUM_QUANTILES == 5:\n",
    "        OUT_SHEET += '_quin'\n",
    "    else:\n",
    "        assert(1==0),('set new quantiles naming of sheet!')\n",
    "    if MCAP_WEIGHTED:\n",
    "        OUT_SHEET += '_mcap'\n",
    "\n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # drop columns not needed in weekly panel\n",
    "    df = df.rename(columns={'char_mcap_t': 'mcap'})\n",
    "    df = df[['date', 'asset', LHS_COL, 'mcap']+RHS_COLS].copy()\n",
    "\n",
    "    # drop rows that are not in the asset universe\n",
    "    df = subsetToAssetUniverse(df, asset_universe_dict)\n",
    "\n",
    "    # windsorize\n",
    "    if WINDSORIZE:\n",
    "        OUT_SHEET += '_wind'\n",
    "        df = windsorize(df, LHS_COL)\n",
    "    \n",
    "    # calculate results\n",
    "    calcUnivariateFactorResults(df, RHS_COLS,\n",
    "        LHS_COL, TS_AVG_METHOD, ANNUALIZED, PERIODS_IN_YEAR,\n",
    "        NUM_QUANTILES, MCAP_WEIGHTED, OUT_FP, OUT_SHEET)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
