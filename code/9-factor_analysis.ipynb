{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ca0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE WITH NEW PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99b42fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f95dd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN DATA\n",
    "with open('../3-data/clean/asset_universe_dates_and_lists.pkl', 'rb') as handle:\n",
    "    asset_universe_dict = pickle.load(handle) \n",
    "df = pd.read_csv('../3-data/panel_factor_analysis.csv')\n",
    "\n",
    "# CLEAN UP CSV FILE GIVEN IT ISNT PKL\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "# LOOP OVER TOKEN UNIVERSE TO SUBSET DF TO VALID ROWS\n",
    "included_df = pd.DataFrame()\n",
    "\n",
    "# Loop over asset_universe_dict len with i\n",
    "for i in range(len(asset_universe_dict)):\n",
    "    # Extract this quarter and its included assets\n",
    "    date = list(asset_universe_dict.keys())[i]\n",
    "    assets = asset_universe_dict[date]\n",
    "\n",
    "    # Form start and end date for this window\n",
    "    start_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    \n",
    "    end_date   = datetime.strptime(date, '%Y-%m-%d') + relativedelta(months=3)\n",
    "    \n",
    "    # Extract asset dates\n",
    "    temp_df = df[(df.asset.isin(assets)) & (df.date >= start_date) & (df.date < end_date)][['date', 'asset']]\n",
    "    included_df = pd.concat((included_df, temp_df.reset_index(drop=True)))\n",
    "\n",
    "# Keep just those included rows\n",
    "assert(included_df.shape[0] == included_df[~included_df.duplicated()].shape[0]) # ensure included df is unique\n",
    "df = df.merge(included_df,\n",
    "              on=['date', 'asset'],\n",
    "              how='inner',\n",
    "              validate='one_to_one')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8bfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ADD YEAR COLUMN AND BUILD LIST OF RHS\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "rhs_cols = df.columns.values.tolist()\n",
    "to_remove = ['date', 'asset', 'mcap_t', 'week_idx', 'r_tplus7', 'year']\n",
    "for var in to_remove:\n",
    "    rhs_cols.remove(var)   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f377fb",
   "metadata": {},
   "source": [
    "Extra argument geoavg (0: simple time series average, 1: geometric average)\\\n",
    "Extra argument flip_tertiles that will optionally flip tertiles or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2019c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariateFactorAnalaysis(df, factor, geoavg=True, flip_tertiles=False):\n",
    "    \n",
    "    # create result df\n",
    "    res_col = ['1', '2', '3', '3-1' , 'factor', 'sort']\n",
    "    res_row = ['2016', '2017', '2018', '2019', '2020', 'all', 'all_tstat']\n",
    "    res_df = pd.DataFrame(np.nan, index=res_row, columns=res_col)\n",
    "    res_df = res_df.assign(factor=factor)\n",
    "\n",
    "    \n",
    "    # drop cols from df\n",
    "    temp_df = df[['date', 'asset', 'r_tplus7', 'mcap_t', 'year', factor]].copy()\n",
    "    \n",
    "    \n",
    "    # Form the tertiles based on given factor\n",
    "    if(flip_tertiles):\n",
    "        temp_df = temp_df.sort_values(by=['date', factor], ascending=False)\n",
    "    else:\n",
    "        temp_df = temp_df.sort_values(by=['date', factor])\n",
    "    temp_df['ranking'] = temp_df.groupby(['date']).cumcount()+1\n",
    "    temp_df['counts'] = 1\n",
    "    temp_df['assets_per_week'] = temp_df.groupby('date').counts.transform(lambda x: x.sum())\n",
    "    temp_df['ranking'] = temp_df.ranking / temp_df.assets_per_week\n",
    "    temp_df.loc[temp_df.ranking <= 1/3, 'tertile'] = 1\n",
    "    temp_df.loc[(temp_df.ranking > 1/3) & (temp_df.ranking <= 2/3), 'tertile'] = 2\n",
    "    temp_df.loc[(temp_df.ranking > 2/3) & (temp_df.ranking <= 1), 'tertile'] = 3\n",
    "\n",
    "    \n",
    "    # Calculate value-weighted average returns for each tertile\n",
    "    temp_df['mcap_sum']    = temp_df.groupby(['date', 'tertile'])['mcap_t'].transform('sum')\n",
    "    temp_df['weight']      = temp_df.mcap_t / temp_df.mcap_sum\n",
    "    temp_df['tertile_r_tplus7'] = temp_df.weight * temp_df.r_tplus7\n",
    "    temp_df['tertile_r_tplus7'] = temp_df.groupby(['date', 'tertile'])['tertile_r_tplus7'].transform('sum')\n",
    "    temp_df.drop_duplicates(['date', 'tertile'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Determine if factor is pos or neg corr with returns\n",
    "    long_short_diff_overall = (np.mean(temp_df[temp_df.tertile==3].tertile_r_tplus7.values) \n",
    "                               - np.mean(temp_df[temp_df.tertile==1].tertile_r_tplus7.values))\n",
    "    if long_short_diff_overall > 0:\n",
    "        res_df['sort'] = '1_low_to_3_high'\n",
    "    else:\n",
    "        temp_df.loc[temp_df.tertile == 3, 'tertile'] = 4 # change to something else first\n",
    "        temp_df.loc[temp_df.tertile == 1, 'tertile'] = 3\n",
    "        temp_df.loc[temp_df.tertile == 4, 'tertile'] = 1\n",
    "        res_df['sort'] = '1_high_to_3_low'\n",
    "\n",
    "    \n",
    "    # Take the geometric average (and t_stat) of tertiles 1-3\n",
    "    # by year\n",
    "    for year in [2016, 2017, 2018, 2019, 2020]: \n",
    "        year_df = temp_df[temp_df['year'] == year]\n",
    "        for t in [1, 2, 3]:\n",
    "            tertile_df = year_df[year_df['tertile'] == t]\n",
    "            tertile_simple_returns = tertile_df.tertile_r_tplus7.values\n",
    "            mean = 0\n",
    "            if(geoavg): # geometric mean\n",
    "                gmean = np.prod(tertile_simple_returns+1)**(1/len(tertile_simple_returns))-1\n",
    "                mean = gmean\n",
    "            else: # arithmetic mean\n",
    "                amean = np.average(tertile_simple_returns+1)\n",
    "                mean = amean\n",
    "            res_df.iloc[int(year-2016),int(t-1)] = mean\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    # and overall\n",
    "    for t in [1.0, 2.0, 3.0]:\n",
    "        tertile_df = temp_df[temp_df['tertile'] == t]\n",
    "        tertile_simple_returns = tertile_df.tertile_r_tplus7.values\n",
    "        # mean\n",
    "        mean = 0\n",
    "        if(geoavg): # geometric mean\n",
    "            gmean = np.prod(tertile_simple_returns+1)**(1/len(tertile_simple_returns))-1\n",
    "            mean = gmean\n",
    "        else: # arithmetic mean\n",
    "            amean = np.average(tertile_simple_returns+1)\n",
    "            mean = amean\n",
    "        res_df.iloc[int(year-2016),int(t-1)] = mean\n",
    "        # t-stat\n",
    "        t_stat = (np.mean(tertile_simple_returns) / \n",
    "                   np.std(tertile_simple_returns)*np.sqrt(len(tertile_simple_returns)))\n",
    "        res_df.iloc[6,int(t-1)] = t_stat\n",
    "\n",
    "        \n",
    "    # Form the week by week 3-1 portfolio\n",
    "    t3_df = temp_df[temp_df['tertile'] == 3]\n",
    "    t1_df = temp_df[temp_df['tertile'] == 1]\n",
    "    t3_df.rename(columns = {'tertile_r_tplus7':'t3_r'}, inplace = True)\n",
    "    t1_df.rename(columns = {'tertile_r_tplus7':'t1_r'}, inplace = True)\n",
    "    t3_1_df = t3_df.merge(t1_df,on='date',how='inner',validate='one_to_one')\n",
    "    t3_1_df['tertile_r_3_1_t'] = t3_1_df.t3_r - t3_1_df.t1_r \n",
    "    t3_1_df = t3_1_df[['date','tertile_r_3_1_t']]\n",
    "    t3_1_df['tertile'] = '3-1'\n",
    "    t3_1_df['year'] = t3_1_df['date'].dt.year\n",
    "    \n",
    "    \n",
    "    # fix returns below -1 to just blow up the strategy with a strict -1\n",
    "    t3_1_df.loc[t3_1_df.tertile_r_3_1_t < -1, 'tertile_r_3_1_t'] = -1 \n",
    "    \n",
    "\n",
    "    # Take the average (and t_stat) of 3-1 portfolio\n",
    "    # by year\n",
    "    for year in [2016, 2017, 2018, 2019, 2020]: \n",
    "        year_df = t3_1_df[t3_1_df['year'] == year]\n",
    "        simple_returns = year_df.tertile_r_3_1_t.values\n",
    "        mean = 0\n",
    "        if(geoavg): # geometric mean\n",
    "            gmean = np.prod(simple_returns+1)**(1/len(simple_returns))-1\n",
    "            mean = gmean\n",
    "        else: # arithmetic mean\n",
    "            amean = np.average(simple_returns+1)\n",
    "            mean = amean\n",
    "        res_df.iloc[int(year-2016),3] = mean\n",
    "        \n",
    "    # overall \n",
    "    simple_returns = t3_1_df.tertile_r_3_1_t.values\n",
    "    mean = 0\n",
    "    if(geoavg): # geometric mean\n",
    "        gmean = np.prod(simple_returns+1)**(1/len(simple_returns))-1\n",
    "        mean = gmean\n",
    "    else: # arithmetic mean\n",
    "        amean = np.average(simple_returns+1)\n",
    "        mean = amean\n",
    "    res_df.iloc[5,3] = mean\n",
    "\n",
    "\n",
    "    # t-stat\n",
    "    t_stats = np.mean(simple_returns)/np.std(simple_returns)*np.sqrt(len(simple_returns))\n",
    "    res_df.iloc[6,3] = t_stats\n",
    "\n",
    "\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd507de",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "for factor in rhs_cols:\n",
    "    output_df = univariateFactorAnalaysis(df, factor)\n",
    "    final_df = pd.concat((final_df, output_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fp = '../4-output/univariate_factor_analysis.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "    final_df.to_excel(writer,sheet_name='raw_univariate')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
