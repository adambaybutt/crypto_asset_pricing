{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279af7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE WITH NEW PANEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944cf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sklearn as sk\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import initializers\n",
    "from keras.models import Model\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e77cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP TO RUN ON CPU\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6bbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropRowsAndColsForCA(df):\n",
    "    # Cut to just covar columns of interest\n",
    "    df = df[['asset', 'week_idx', 'r_tplus7', 'covar_mcap_t', \n",
    "             'covar_age_t',\n",
    "             'covar_alexa_rank_t', \n",
    "             'covar_circulating_supply_t',\n",
    "             'covar_dev_activ_t', \n",
    "             'covar_dev_activity_contributors_count_t',\n",
    "             'covar_github_activity_contributors_count_t', \n",
    "             'covar_kurt_r_daily_tm7',\n",
    "             'covar_num_market_pairs_t', \n",
    "             'covar_p_volume_log_t',\n",
    "             'covar_r_daily_t', \n",
    "             'covar_r_t',\n",
    "             'covar_r_tm28',\n",
    "             'covar_rank_cmc_t', \n",
    "             'covar_sentiment_balance_bitcointalk_t',\n",
    "             'covar_sentiment_negative_bitcointalk_avg_daily_tm7',\n",
    "             'covar_sentiment_volume_consumed_bitcointalk_avg_daily_tm7',\n",
    "             'covar_skew_r_daily_tm7',\n",
    "             'covar_social_dominance_total_t',\n",
    "             'covar_social_volume_total_t',\n",
    "             'covar_total_supply_t',\n",
    "             'covar_twitter_followers_t',\n",
    "             'covar_vol_r_daily_tm7']]\n",
    "\n",
    "    # Cut panel to the first week where there are two as many tokens as RHS vars\n",
    "    df['counts'] = 1\n",
    "    df['coins_per_week'] = df.groupby(['date'])['counts'].transform(sum)\n",
    "    df = df[df.coins_per_week >= (df.shape[1]-2)*4]\n",
    "    df = df.drop(columns = ['counts', 'coins_per_week'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e12308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPortfolioReturnCovariates(df):\n",
    "    # Obtain the weeks of the dataframe\n",
    "    df = df.sort_values(by = 'date')\n",
    "    weeks = np.unique(df.index)\n",
    "\n",
    "    # Form new covariate names\n",
    "    column_names = list(df.columns.values)\n",
    "    column_names.remove('asset')\n",
    "    column_names.remove('r_tplus7')\n",
    "    covariates = column_names\n",
    "    new_covariates = ['x_' + cov for cov in covariates]\n",
    "\n",
    "    for current_week in weeks: \n",
    "        # Obtain the week's returns and the previous week's covariates\n",
    "        returns = df[df.index == current_week].r_tplus7.values\n",
    "        z_t_1   = df[df.index == current_week][covariates].values\n",
    "\n",
    "        # Calculate the characteristic managed portfolio returns\n",
    "        design = np.linalg.inv(np.matmul(np.transpose(z_t_1), z_t_1))\n",
    "        x_s    = np.matmul(np.matmul(design, np.transpose(z_t_1)), returns)\n",
    "\n",
    "        # Set the new columns to this week's vector's value\n",
    "        df.loc[df.index == current_week, new_covariates] = x_s\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aeec626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week):\n",
    "    # Determine what quarter the oos_week is in\n",
    "    oos_mnth = pd.to_datetime(oos_week).month\n",
    "    oos_yr   = pd.to_datetime(oos_week).year\n",
    "    mnth_qtr = int(np.floor((oos_mnth-1)/3)*3+1)\n",
    "    if mnth_qtr == 10:\n",
    "        oos_qtr = str(oos_yr)+'-'+str(mnth_qtr)+'-01'\n",
    "    else:\n",
    "        oos_qtr = str(oos_yr)+'-0'+str(mnth_qtr)+'-01'\n",
    "\n",
    "    # Determine the asset universe\n",
    "    asset_universe = asset_universe_dict[oos_qtr]\n",
    "    \n",
    "    # Subset the training data to the asset universe\n",
    "    temp_df = temp_df[temp_df.asset.isin(asset_universe)]\n",
    "    \n",
    "    return temp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b60527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetToAssetUniverseFull(df, asset_universe_dict, train_or_test):\n",
    "    # determine the asset universe to use for whether train or test data\n",
    "    if train_or_test == 'train':\n",
    "        index_start = 0\n",
    "        index_end   = len(asset_universe_dict)-4\n",
    "    elif train_or_test=='test':\n",
    "        index_start = len(asset_universe_dict)-4\n",
    "        index_end   = len(asset_universe_dict)\n",
    "    else:\n",
    "        assert(False),('get wit zee program')\n",
    "        \n",
    "    # subset to included assets\n",
    "    for i in range(index_start, index_end):\n",
    "        # extract this quarter and its included assets\n",
    "        date = list(asset_universe_dict.keys())[i]\n",
    "        assets = asset_universe_dict[date]\n",
    "\n",
    "        # form start and end date for this window\n",
    "        start_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "        end_date   = datetime.strptime(date, '%Y-%m-%d') + relativedelta(months=3)\n",
    "\n",
    "        # drop rows in this time period that are not the included assets\n",
    "        df = df[~(((df.index>=start_date) & (df.index<end_date)) & (~df.asset.isin(assets)))]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60956e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAutoencoder(train_df, hps_yhats_dict, val_df=None, early_stopping=True):\n",
    "    # Obtain the covariates\n",
    "    column_names = list(df.columns.values)\n",
    "    column_names.remove('week_idx')\n",
    "    column_names.remove('asset')\n",
    "    column_names.remove('r_tplus7')\n",
    "    covariates = column_names\n",
    "\n",
    "    # Obtain the covariates on x_t and b_t_1 sides of the network\n",
    "    x_t   = [covar for covar in covariates if covar[:2] == 'x_']\n",
    "    b_t_1 = [covar for covar in covariates if covar[:2] != 'x_']\n",
    "\n",
    "    # Extract the hyperparameters\n",
    "    number_hidden_layer = hps_yhats_dict['number_hidden_layer']\n",
    "    number_factor       = hps_yhats_dict['number_factor']\n",
    "    learning_rate       = hps_yhats_dict['learning_rate']\n",
    "    l1_penalty          = hps_yhats_dict['l1_penalty']\n",
    "    batch_size          = hps_yhats_dict['batch_size']\n",
    "    number_ensemble     = hps_yhats_dict['number_ensemble']\n",
    "    bootstrap_pct       = hps_yhats_dict['bootstrap_pct']\n",
    "    epoch               = hps_yhats_dict['epoch']\n",
    "\n",
    "    # Initialize the models\n",
    "    models = []\n",
    "\n",
    "    # Loop over the ensembles to build models for each\n",
    "    assert(number_ensemble <= 5)\n",
    "    for i in range(0, number_ensemble):\n",
    "        \n",
    "        # Bootstrap the rows so different models in the ensemble are less correlated\n",
    "        train_df = sk.utils.resample(train_df, replace = True, \n",
    "                                     n_samples = int(train_df.shape[0]*bootstrap_pct),\n",
    "                                     random_state = i)\n",
    "        \n",
    "        # Obtain the training input and output data and, if passed, validation data\n",
    "        train_b_t_1 = train_df[b_t_1]\n",
    "        train_x_t   = train_df[x_t]  \n",
    "        train_y     = train_df[['r_tplus7']]\n",
    "        if val_df is not None:\n",
    "            val_b_t_1   = val_df[b_t_1]\n",
    "            val_x_t     = val_df[x_t]  \n",
    "            val_y       = val_df[['r_tplus7']]\n",
    "        \n",
    "        # According to which model in the ensemble it is, initialize parameters.\n",
    "        if i==0:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.GlorotUniform(seed=i)\n",
    "        elif i==1:\n",
    "            weight_initializer=initializers.GlorotUniform(seed=i)\n",
    "            bias_initializer=initializers.RandomUniform(seed=i)\n",
    "        elif i==2:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.RandomUniform(seed=i)\n",
    "        elif i==3:\n",
    "            weight_initializer=initializers.RandomUniform(seed=i)\n",
    "            bias_initializer=initializers.GlorotUniform(seed=i)\n",
    "        elif i==4:\n",
    "            weight_initializer=initializers.GlorotUniform(seed=i)\n",
    "            bias_initializer=initializers.HeNormal(seed=i)\n",
    "        else:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.RandomUniform(seed=i)\n",
    "        \n",
    "\n",
    "        # Build the betas model from the t minus 1 covariates\n",
    "        model_b = tf.keras.models.Sequential()\n",
    "        model_b.add(tf.keras.Input(shape=(len(b_t_1),)))\n",
    "        model_b.add(Dense(6, activation='relu',\n",
    "                          kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                          kernel_initializer=weight_initializer,\n",
    "                          bias_initializer=bias_initializer))\n",
    "        model_b.add(BatchNormalization())\n",
    "        if number_hidden_layer >= 2:\n",
    "            model_b.add(Dense(5, activation='relu',\n",
    "                              kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                              kernel_initializer=weight_initializer,\n",
    "                              bias_initializer=bias_initializer))\n",
    "            model_b.add(BatchNormalization())\n",
    "        if number_hidden_layer == 3:\n",
    "            model_b.add(Dense(4, activation='relu',\n",
    "                              kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                              kernel_initializer=weight_initializer,\n",
    "                              bias_initializer=bias_initializer))\n",
    "            model_b.add(BatchNormalization())\n",
    "        model_b.add(Dense(number_factor, activation='linear',\n",
    "                          kernel_initializer=weight_initializer,\n",
    "                          bias_initializer=bias_initializer))\n",
    "\n",
    "        # Form the x model from time t returns\n",
    "        model_x = tf.keras.models.Sequential()\n",
    "        model_x.add(tf.keras.Input(shape=(len(x_t),)))\n",
    "        model_x.add(Dense(number_factor, activation='linear',\n",
    "                          kernel_initializer=weight_initializer,\n",
    "                          bias_initializer=bias_initializer))\n",
    "\n",
    "        # Form the dot product output for the combination of the two neurals\n",
    "        mergedOut = Dot(axes=(1,1))([model_b.output, model_x.output])\n",
    "\n",
    "        # Form the entire model\n",
    "        model = Model([model_b.input, model_x.input], mergedOut)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='mean_squared_error',\n",
    "                      metrics=['mse'])\n",
    "\n",
    "        # Prepare early stopping object \n",
    "        es = EarlyStopping(monitor='val_mse', mode='min', verbose=0, patience = 2) # VERBOSE 2\n",
    "        \n",
    "        # Fit the model\n",
    "        with tf.device('/CPU:0'):\n",
    "            if early_stopping == True:\n",
    "                model.fit(x=[train_b_t_1, train_x_t], y=train_y, \n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=([val_b_t_1, val_x_t], val_y), \n",
    "                          epochs=epoch, verbose=0,\n",
    "                          workers=4, callbacks=[es]) # VERBOSE 1\n",
    "            else:\n",
    "                model.fit(x=[train_b_t_1, train_x_t], y=train_y, \n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epoch, verbose=1,\n",
    "                          workers=4)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df66ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genYhats(df, models, oos_week, number_factor):\n",
    "    # Obtain the covariates on x_t and b_t_1 sides of the network\n",
    "    column_names = list(df.columns.values)\n",
    "    column_names.remove('asset')\n",
    "    column_names.remove('r_tplus7')\n",
    "    column_names.remove('week_idx')\n",
    "    covariates = column_names\n",
    "    x_t   = [covar for covar in covariates if covar[:2] == 'x_']\n",
    "    b_t_1 = [covar for covar in covariates if covar[:2] != 'x_']\n",
    "\n",
    "    # Obtain the oos data\n",
    "    oos_df = df[df.index == oos_week].copy()\n",
    "    oos_x_t = oos_df[x_t]\n",
    "    oos_b_t_1 = oos_df[b_t_1]\n",
    "\n",
    "    # For each model form the beta hats\n",
    "    b_hats = np.zeros((oos_df.shape[0],number_factor))\n",
    "    for model in models:\n",
    "        layer_name = model.layers[-3]._name \n",
    "        assert(model.layers[-3].output_shape[1] == number_factor)\n",
    "        b_hat_layer = Model(inputs=model.input[0],\n",
    "                            outputs=model.get_layer(layer_name).output)\n",
    "        b_hat = b_hat_layer.predict(oos_b_t_1)\n",
    "        b_hats += b_hat\n",
    "        \n",
    "    # Obtain the average b_hat across the models\n",
    "    b_hats = b_hats/len(models)\n",
    "    \n",
    "    # output statistics- 5 stat summary of Beta Hat\n",
    "#     b_hats_min = b_hats.min()\n",
    "#     b_hats_max = b_hats.max()\n",
    "#     b_hats_25_percentile = np.percentile(b_hats,25)\n",
    "#     b_hats_75_percentile = np.percentile(b_hats,75)\n",
    "#     b_hats_mean = b_hats.mean()\n",
    "#     print('Min beta_hats: %.3f'% b_hats_min)\n",
    "#     print('Q1 beta_hats: %.3f'% b_hats_25_percentile)\n",
    "#     print('Mean beta_hats: %.3f'% b_hats_mean)\n",
    "#     print('Q3 beta_hats: %.3f'% b_hats_75_percentile)\n",
    "#     print('Max beta_hats: %.3f'% b_hats_max)\n",
    "#     print('\\n')\n",
    "\n",
    "    # Form the sample average of the estimated factors for BEFORE the OOS week\n",
    "    weeks = np.unique(df[df.index < oos_week].index)\n",
    "    lambda_hats = np.zeros(number_factor)\n",
    "    for prev_week in weeks:\n",
    "        prev_x_t = df[df.index == prev_week][x_t]\n",
    "        f_hats = np.zeros(number_factor)\n",
    "        for model in models:\n",
    "            layer_name = model.layers[-2]._name \n",
    "            assert(model.layers[-2].output_shape[1] == number_factor)\n",
    "            f_hat_layer = Model(inputs=model.input[1],\n",
    "                                outputs=model.get_layer(layer_name).output)\n",
    "            f_hat = f_hat_layer.predict(prev_x_t)\n",
    "            assert(all(np.isclose(f_hat[0,:], f_hat[1,:])))\n",
    "            f_hats += f_hat[0,:]\n",
    "        f_hats = f_hats / len(models)\n",
    "        lambda_hats += f_hats\n",
    "        lambda_hats = lambda_hats / len(weeks)\n",
    "        \n",
    "    # summary of lambda_hats - 5 stat summary\n",
    "#     lambda_hats_min = lambda_hats.min()\n",
    "#     lambda_hats_max = lambda_hats.max()\n",
    "#     lambda_hats_25_percentile = np.percentile(lambda_hats,25)\n",
    "#     lambda_hats_75_percentile = np.percentile(lambda_hats,75)\n",
    "#     lambda_hats_mean = lambda_hats.mean()\n",
    "#     print('Min lambda_hats: %.3f'% lambda_hats_min)\n",
    "#     print('Q1 lambda_hats: %.3f'% lambda_hats_25_percentile)\n",
    "#     print('Mean lambda_hats: %.3f'% lambda_hats_mean)\n",
    "#     print('Q3 lambda_hats: %.3f'% lambda_hats_75_percentile)\n",
    "#     print('Max lambda_hats: %.3f'% lambda_hats_max) \n",
    "#     print('\\n')\n",
    "        \n",
    "    # Form yhats\n",
    "    yhats = np.matmul(b_hats, lambda_hats)\n",
    "    \n",
    "    # output stats of yhats - 5 stat summary \n",
    "#     y_hats_min = yhats.min()\n",
    "#     y_hats_max = yhats.max()\n",
    "#     y_hats_25_percentile = np.percentile(yhats,25)\n",
    "#     y_hats_75_percentile = np.percentile(yhats,75)\n",
    "#     y_hats_mean = yhats.mean()\n",
    "#     print('Min y_hats: %.3f'% y_hats_min)\n",
    "#     print('Q1 y_hats: %.3f'% y_hats_25_percentile)\n",
    "#     print('Mean y_hats: %.3f'% y_hats_mean)\n",
    "#     print('Q3 y_hats: %.3f'% y_hats_75_percentile)\n",
    "#     print('Max y_hats: %.3f'% y_hats_max)\n",
    "#     print('\\n')\n",
    "    \n",
    "\n",
    "    return yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472f1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df, asset_universe_dict, last_train_year=2018, val_end_year=2020):\n",
    "    # Initialize hp result objects\n",
    "    hps_yhats_dict_list = []\n",
    "    hps_mse_df_list     = []\n",
    "    \n",
    "    # Initialize the hyperparameter grid  \n",
    "    number_hidden_layers = [3, 1]\n",
    "    number_factors       = [3, 2, 1]\n",
    "    learning_rates       = [1e-5, 1e-3, 1e-1]\n",
    "    batch_sizes          = [128] # note: ensure powers of 2 for eff, [64,256]\n",
    "    l1_penalties         = [1, 1e-1, 1e-2, 1e-3]\n",
    "    number_ensembles     = [5] \n",
    "    early_stopping       = [True]\n",
    "    bootstrap_pcts       = [0.99]\n",
    "    epochs               = [9]\n",
    "\n",
    "    # Determine the weeks in the validation window  \n",
    "    val_weeks = np.unique(df[(df.index.year > last_train_year)  \n",
    "                             & (df.index.year <= val_end_year)].index.values)  \n",
    "\n",
    "    # Generate yhats for every hyperparameter grid point\n",
    "    for hps in itertools.product(number_hidden_layers,\n",
    "                                 number_factors,\n",
    "                                 learning_rates,\n",
    "                                 batch_sizes,\n",
    "                                 l1_penalties,\n",
    "                                 number_ensembles,\n",
    "                                 early_stopping,\n",
    "                                 bootstrap_pcts,\n",
    "                                 epochs):\n",
    "        hps_yhats_dict = {'number_hidden_layer': hps[0],\n",
    "                          'number_factor': hps[1],\n",
    "                          'learning_rate': hps[2],\n",
    "                          'batch_size': hps[3],\n",
    "                          'l1_penalty': hps[4],\n",
    "                          'number_ensemble': hps[5],\n",
    "                          'early_stopping': hps[6],\n",
    "                          'bootstrap_pct': hps[7],\n",
    "                          'epoch': hps[8],\n",
    "                          'yhats': np.array([]),\n",
    "                          'ys':    np.array([])}\n",
    "        print(hps_yhats_dict, '\\n')\n",
    "        tic = time.perf_counter()\n",
    "        for val_week in val_weeks:\n",
    "#             print(val_week, '\\n')\n",
    "            temp_df = df[df.index <= val_week].copy()\n",
    "            temp_df = subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week=val_week)\n",
    "            train_df = temp_df[temp_df.index < val_week].copy()\n",
    "            val_df   = temp_df[temp_df.index == val_week].copy()\n",
    "            \n",
    "            models = fitAutoencoder(train_df, hps_yhats_dict, val_df=val_df, \n",
    "                                    early_stopping=hps_yhats_dict['early_stopping'])\n",
    "            yhats  = genYhats(temp_df, models, val_week, hps_yhats_dict['number_factor'])\n",
    "            ys     = val_df.r_tplus7.values\n",
    "\n",
    "            hps_yhats_dict['yhats'] = np.append(hps_yhats_dict['yhats'], yhats)\n",
    "            hps_yhats_dict['ys']    = np.append(hps_yhats_dict['ys'], ys)\n",
    "\n",
    "            val_ys_todate = hps_yhats_dict['ys']\n",
    "            rw_val_mse    = np.mean(np.square(val_ys_todate))\n",
    "            model_val_mse = np.mean(np.square(val_ys_todate - hps_yhats_dict['yhats']))\n",
    "#             print('\\n val random walk mse: ' + str(rw_val_mse))\n",
    "#             print('\\n val model mse: ' + str(model_val_mse))\n",
    "#             print('\\n val model mse winning?: ' + str(model_val_mse < rw_val_mse))\n",
    "#             print('\\n\\n')\n",
    "        # Save run time and space out result print out\n",
    "        toc = time.perf_counter()\n",
    "#         print('\\n\\n\\n')\n",
    "\n",
    "        # Update lists of results\n",
    "        hps_yhats_dict_list.append(hps_yhats_dict)\n",
    "        cv_results_dict = hps_yhats_dict.copy()\n",
    "        del cv_results_dict['yhats']\n",
    "        del cv_results_dict['ys']\n",
    "        cv_results_dict['runtime_mins'] = round((toc - tic)/60, 0)\n",
    "        cv_results_dict['mse'] = model_val_mse\n",
    "        hps_mse_df_list.append(pd.DataFrame(cv_results_dict, index=[0]))\n",
    "\n",
    "        # Save CV results\n",
    "        cv_df = pd.concat(hps_mse_df_list, ignore_index=True)\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = '../4-output/cv-results-autoencoder-' + timestr + '.csv'\n",
    "        cv_df.to_csv(fp, index=False)\n",
    "\n",
    "    return hps_yhats_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0acab751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenTestYhats(df, opt_hps, test_year=2021): \n",
    "    test_weeks = np.unique(df[df.index.year == test_year].index.values)\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "    for test_week in test_weeks:\n",
    "        print(test_week, '\\n')\n",
    "        temp_df  = df[df.index <= test_week].copy()\n",
    "        temp_df  = subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week=test_week)\n",
    "        train_df = temp_df[temp_df.index < test_week].copy()\n",
    "        oos_df   = temp_df[temp_df.index == test_week].copy()\n",
    "        \n",
    "        models   = fitAutoencoder(train_df, opt_hps, val_df=None, early_stopping=False)\n",
    "        yhats    = genYhats(oos_df, models, test_week, opt_hps['number_factor'])\n",
    "       \n",
    "        oos_df = oos_df[['asset', 'r_tplus7']]\n",
    "        oos_df['yhat'] = yhats\n",
    "        test_df = pd.concat((test_df, oos_df))\n",
    "        rw_mse = np.mean(np.square(test_df.r_tplus7.values))\n",
    "        model_mse = np.mean(np.square(test_df.r_tplus7.values - test_df.yhat.values))\n",
    "        print('\\n test random walk mse: ' + str(rw_mse))\n",
    "        print('\\n test model mse: ' + str(model_mse))\n",
    "        print('winning?: ' + str(model_mse < rw_mse))\n",
    "        print('\\n')\n",
    "    \n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902d2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelPortfolioWeights(df):\n",
    "    # assign tertiles\n",
    "    np.random.seed(42)\n",
    "    df['rand']    = np.random.uniform(size=df.shape[0])\n",
    "    df = df.sort_values(by=['date', 'yhat'])\n",
    "    df['ranking'] = df.groupby(['date']).cumcount()\n",
    "    df['counts']  = 1\n",
    "    df['total_assets_per_week'] = df.groupby('date').counts.transform('sum')\n",
    "    df['ranking']               = df.ranking/df.total_assets_per_week\n",
    "    df.loc[df.ranking < 1/3, 'prtfl_wght'] = 0\n",
    "    df.loc[(df.ranking>=1/3) & \n",
    "           (df.ranking<2/3), 'prtfl_wght'] = 1/6\n",
    "    df.loc[df.ranking>=2/3,  'prtfl_wght'] = 5/6\n",
    "    df['prtfl_wght'] = 3*df.prtfl_wght/df.total_assets_per_week\n",
    "    \n",
    "    # clean up\n",
    "    df = df.drop(['rand', 'ranking', 'counts',\n",
    "                  'total_assets_per_week'], axis=1)\n",
    "    \n",
    "    # confirm portfolio weights roughly sum to 1 for each week\n",
    "    assert(len(np.unique(df.index)) == \n",
    "           np.sum(np.isclose(df.groupby(['date']).prtfl_wght.sum(), 1,\n",
    "                             rtol=1e-2, atol=1e-2)))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09430e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioReturn(df):\n",
    "    num_wks  = df.shape[0]\n",
    "    if np.sum(df.r.values <= -1)>=1:\n",
    "        return -1\n",
    "    else:\n",
    "        tot_ret  = np.product(df.r.values+1)-1\n",
    "        wkly_ret = (tot_ret+1)**(1/num_wks)-1\n",
    "        annl_ret = (wkly_ret+1)**(52.18)-1\n",
    "        return annl_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f24ff5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAnnualTransactionCosts(df):\n",
    "    # merge on the previous week's holdings to the new holdings\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df.week_idx<np.max(temp_df.week_idx)]\n",
    "    temp_df['week_idx'] = temp_df.week_idx+1\n",
    "    temp_df = temp_df[['week_idx', 'asset', 'prtfl_wght']]\n",
    "    temp_df = temp_df.rename(columns={'prtfl_wght': 'prtfl_wght_tm7'})\n",
    "    df = df.merge(temp_df,\n",
    "                  on=['week_idx', 'asset'],\n",
    "                  how='outer',\n",
    "                  validate='one_to_one')\n",
    "\n",
    "    # calc weekly turnover and ensure it has the appropriate range\n",
    "    df['asset_to'] = np.abs(df.prtfl_wght - df.prtfl_wght_tm7)\n",
    "    to_df = df.groupby('week_idx')[['asset_to']].sum().reset_index()\n",
    "    assert((np.min(to_df.asset_to)>=0) & (np.max(to_df.asset_to<=2)))\n",
    "\n",
    "    # correct the first and last week valid for buying the initial port and liquidating\n",
    "    to_df.loc[to_df.week_idx==106, 'asset_to'] = 1\n",
    "    to_df = pd.concat((to_df, pd.DataFrame(data={'week_idx': [262],\n",
    "                                                 'asset_to': 1})))\n",
    "    to_df = to_df.reset_index(drop=True)\n",
    "\n",
    "    # add transaction costs assuming maker and taker fee of 20 bps each\n",
    "    to_df['tc'] = to_df.asset_to*0.002\n",
    "\n",
    "    # return annualize transaction cost\n",
    "    return -np.sum(to_df.tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07acb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPortfolioSharpe(df):\n",
    "    wkly_sharpe = np.mean(df.r.values)/np.std(df.r.values)\n",
    "    annl_sharpe = wkly_sharpe*np.sqrt(52.18)\n",
    "    return annl_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d9db1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_draw_down(df):\n",
    "    cumulative_ret=(df.r+1).cumprod()\n",
    "    roll_max=cumulative_ret.rolling(len(cumulative_ret), min_periods=1).max()\n",
    "    daily_drawdown=cumulative_ret/roll_max\n",
    "    max_daily_drawdown=daily_drawdown.min() - 1\n",
    "    return max_daily_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c86f7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_1_month_loss(df):\n",
    "    max_loss=(df['r']+1).rolling(4).apply(np.prod)\n",
    "    max_loss_minus=max_loss.min()-1\n",
    "    return max_loss_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f703b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "# load in the data\n",
    "input_fp = '../3-data/clean/panel_train.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "df = df.sort_values(by=['date', 'asset'])\n",
    "with open('../3-data/clean/asset_universe_dates_and_lists.pkl', 'rb') as handle:\n",
    "    asset_universe_dict = pickle.load(handle)\n",
    "\n",
    "# drop rows and columns such that data will work for conditional autoencoder (CA)\n",
    "df = dropRowsAndColsForCA(df)\n",
    "\n",
    "# form the char-sorted portfolios for factor side of CA input\n",
    "df = formPortfolioReturnCovariates(df)\n",
    "\n",
    "# sort the data\n",
    "df = df.sort_values(by=['date', 'asset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c648bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "# load in the data\n",
    "input_fp = '../3-data/clean/panel_train.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "df = df.sort_values(by=['date', 'asset'])\n",
    "with open('../3-data/clean/asset_universe_dates_and_lists.pkl', 'rb') as handle:\n",
    "    asset_universe_dict = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run CV\n",
    "hps_yhats_list = runCV(df, asset_universe_dict, last_train_year = 2018, val_end_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add code to select the best hps combo from the above list\n",
    "#       i just ran manually with setting the best CV point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form validation period results\n",
    "df = df[['asset', 'week_idx', 'r_tplus7']]\n",
    "df = subsetToAssetUniverseFull(df, asset_universe_dict, train_or_test='train')\n",
    "df = df[df.index.year >= 2019]\n",
    "df['yhat'] = hps_yhats_list[0]['yhats']\n",
    "df      = labelPortfolioWeights(df)\n",
    "annl_tc = calcAnnualTransactionCosts(df)\n",
    "df['r'] = df.prtfl_wght*df.r_tplus7\n",
    "r_df    = df.groupby(['date'])[['r']].sum()\n",
    "annl_ret = calcPortfolioReturn(r_df)\n",
    "annl_sharpe = calcPortfolioSharpe(r_df)\n",
    "max_dd = max_draw_down(r_df)\n",
    "max_1mo_loss = max_1_month_loss(r_df)\n",
    "print('annual transaction costs in simple return terms: ' + str(np.round(annl_tc, 4)))\n",
    "print('annual simple return before trans costs: ' + str(np.round(annl_ret, 4)))\n",
    "print('annual sharpe: '+str(np.round(annl_sharpe, 2)))\n",
    "print('max drawdown : '+str(np.round(max_dd, 2)))\n",
    "print('max one month loss : '+str(np.round(max_1mo_loss, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO GO SCOPE OLD NOTEBOOK FOR LATEST NUMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cdeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean up data for test fitting\n",
    "input_fp = '../3-data/clean/panel_oos.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "df = df.sort_values(by=['date', 'asset'])\n",
    "opt_hps = hps_yhats_list[0].copy()\n",
    "del opt_hps['yhats']\n",
    "del opt_hps['ys']\n",
    "test_df = GenTestYhats(df, opt_hps) \n",
    "test_df = test_df.merge(df[['asset', 'week_idx']],\n",
    "                        on=['date', 'asset'],\n",
    "                        how='inner', \n",
    "                        validate='one_to_one')\n",
    "\n",
    "# form test period results\n",
    "test_df = labelPortfolioWeights(test_df)\n",
    "annl_tc = calcAnnualTransactionCosts(test_df)\n",
    "test_df['r'] = test_df.prtfl_wght*test_df.r_tplus7\n",
    "r_df    = test_df.groupby(['date'])[['r']].sum()\n",
    "annl_ret = calcPortfolioReturn(r_df)\n",
    "annl_sharpe = calcPortfolioSharpe(r_df)\n",
    "max_dd = max_draw_down(r_df)\n",
    "max_1mo_loss = max_1_month_loss(r_df)\n",
    "print('annual transaction costs in simple return terms: ' + str(np.round(annl_tc, 4)))\n",
    "print('annual simple return before trans costs: ' + str(np.round(annl_ret, 4)))\n",
    "print('annual sharpe: '+str(np.round(annl_sharpe, 2)))\n",
    "print('max drawdown : '+str(np.round(max_dd, 2)))\n",
    "print('max one month loss : '+str(np.round(max_1mo_loss, 2)))\n",
    "\n",
    "# TODO GO SCOPE OLD NOTEBOOK FOR LATEST NUMBERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Figure out why all the test period yhats are close to zero\n",
    "#       use the comments to report out the distribution \n",
    "#       to figure out if factor side or beta side that is turning all to zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
