{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6359323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "from requests import Session\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
    "import json\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import json.decoder\n",
    "from typing import Dict, Any, Optional\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcf65d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiateAPI(base_url: str) -> Session:\n",
    "    \"\"\" confirm the cmc api is working for the set api key.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): the url for the pro api at cmc. \n",
    "    \n",
    "    Returns:\n",
    "        session (requests.Session): request class for pinging cmc.\n",
    "    \"\"\"\n",
    "    endpoint = '/v1/key/info'\n",
    "    headers = {'Accepts': 'application/json',\n",
    "               'X-CMC_PRO_API_KEY': API_KEY}\n",
    "    final_url = base_url + endpoint\n",
    "    session = Session()\n",
    "    session.headers.update(headers)\n",
    "    r = session.get(final_url)\n",
    "    print(r.json())\n",
    "\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fad6930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCMCApiCall(session: Session, url: str, params: dict, retries: int=3) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\" makes an API call to CoinMarketCap using the provided requests.Session object.\n",
    "    \n",
    "    Args:\n",
    "        session (requests.Session): A requests.Session object that will be used to make the API call.\n",
    "        url (str): The API endpoint URL to call.\n",
    "        params (dict): A dictionary of parameters to include in the API call.\n",
    "        retries (int): The number of times to retry the API call if it fails. Default is 3.\n",
    "        \n",
    "    Returns:\n",
    "        data (dict): the data from the api response, or None if the api call failed.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        response = session.get(url, params=params)\n",
    "        if response.ok:\n",
    "            try:\n",
    "                return response.json()['data']\n",
    "            except json.decoder.JSONDecodeError as e:\n",
    "                print(f'Error decoding JSON response: {str(e)}')\n",
    "        else:\n",
    "            # There was an error, retry after a short delay\n",
    "            print(f'The API call failed with status code {response.status_code}, retrying...')\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    print('The api call failed after 3 attempts.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10cec1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainTopCMCAssets(base_url: str, session: Session, start_date: date, end_date: date) -> list:\n",
    "    \"\"\" obtain the top cmc assets for each month of the study peiod.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The url for the pro api at cmc. \n",
    "        session (Session): A requests.Session object that will be used to make the API call.\n",
    "        start_date (date): A datetime.date object representing the start date for the study period.\n",
    "        end_date (date): A datetime.date object representing the end date for the study period.\n",
    "    \n",
    "    Returns:\n",
    "        unique_token_cmc_ids (list): unique cmc asset integer ids.\n",
    "    \"\"\"\n",
    "    # specify the dates to obtain\n",
    "    dates = [start_date]\n",
    "    current_date = start_date+relativedelta(months=1)\n",
    "    while current_date <= end_date:\n",
    "        dates.append(current_date)\n",
    "        current_date += relativedelta(months=1)\n",
    "\n",
    "    # set up target url\n",
    "    endpoint = '/v1/cryptocurrency/listings/historical'\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    # obtain the top 500 assets by cmc ranking for each month in the study period\n",
    "    asset_cmc_ids = []\n",
    "    for date in dates:\n",
    "        # set up params for call\n",
    "        if date.year <= 2016:\n",
    "            limit = 50\n",
    "        elif date.year <= 2019:\n",
    "            limit = 300\n",
    "        else:\n",
    "            limit = 500\n",
    "        params = {'date': date,\n",
    "                  'limit': limit,\n",
    "                  'convert': 'USD',\n",
    "                  'aux': 'cmc_rank'}\n",
    "\n",
    "        # make the call\n",
    "        data = makeCMCApiCall(session, url, params)\n",
    "\n",
    "        # extract the asset ids\n",
    "        new_assets = [asset['id'] for asset in data]\n",
    "        asset_cmc_ids.extend(new_assets)\n",
    "\n",
    "        # space out calls\n",
    "        time.sleep(1)\n",
    "        print(date)\n",
    "\n",
    "    # drop redundant assets\n",
    "    unique_asset_cmc_ids = list(np.unique(np.array(asset_cmc_ids)))\n",
    "\n",
    "    return unique_asset_cmc_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d764cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formDataframeOfTopCMCAssets(base_url: str, session: Session, cmc_ids: list) -> pd.DataFrame():\n",
    "    \"\"\" pull all cmc meta data for assets and merge onto universe of top assets in cmc_ids.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): the url for the pro api at cmc. \n",
    "        session (Session): A requests.Session object that will be used to make the API call.\n",
    "        cmc_ids (list): top assets by cmc ranking.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        cw_df (pd.DataFrame): dataframe of asset meta data for top assets by cmc ranking.\n",
    "    \"\"\"\n",
    "\n",
    "    # set up target url for obtaining mapping from id to asset info\n",
    "    endpoint = '/v1/cryptocurrency/map'\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    # obtain the CMC mapping of IDs to asset info\n",
    "    full_data = []\n",
    "    starts = [1, 5001, 10001, 15001]\n",
    "    for start in starts:\n",
    "        # set up params for call\n",
    "        params = {'listing_status': 'active,inactive,untracked',\n",
    "                  'limit': 5000,\n",
    "                  'start': start,\n",
    "                  'aux': 'platform,first_historical_data,last_historical_data'}\n",
    "\n",
    "        # make the call\n",
    "        data = makeCMCApiCall(session, url, params)\n",
    "\n",
    "        # Append the results\n",
    "        full_data.extend(data)\n",
    "\n",
    "        # space out calls\n",
    "        time.sleep(1)\n",
    "\n",
    "    # clean up asset info dictionaries\n",
    "    clean_full_data = []\n",
    "    for asset_dict in full_data:\n",
    "        new_dict = {}\n",
    "        new_dict['cmc_id'] = asset_dict['id']\n",
    "        new_dict['cmc_symbol'] = asset_dict['symbol']\n",
    "        new_dict['name'] = asset_dict['name']\n",
    "        new_dict['cmc_slug'] = asset_dict['slug']\n",
    "        try:\n",
    "            new_dict['cmc_first_date'] = asset_dict['first_historical_data']\n",
    "            new_dict['cmc_last_date'] = asset_dict['last_historical_data']\n",
    "        except KeyError:\n",
    "            new_dict['cmc_first_date'] = None\n",
    "            new_dict['cmc_last_date'] = None\n",
    "        if asset_dict['platform'] != None:\n",
    "            new_dict['platform_cmc_slug'] = asset_dict['platform']['slug']\n",
    "        else:\n",
    "            new_dict['platform_cmc_slug'] = None\n",
    "        clean_full_data.append(new_dict)\n",
    "\n",
    "    cmc_assets_df = pd.DataFrame(clean_full_data)\n",
    "\n",
    "    # Merge down to just the assets of interest\n",
    "    target_assets_df = pd.DataFrame(data = {'cmc_id': cmc_ids})\n",
    "    cw_df = cmc_assets_df.merge(target_assets_df,\n",
    "                                on='cmc_id',\n",
    "                                how='inner',\n",
    "                                validate='one_to_one')\n",
    "\n",
    "    # reset index and sort\n",
    "    cw_df = cw_df.sort_values(by='cmc_id', ignore_index=True)\n",
    "\n",
    "    return cw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bb6a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullPriceMcapVolume(base_url: str, session: Session, \n",
    "        cw_df: pd.DataFrame, start_date: date, end_date: date) -> pd.DataFrame:\n",
    "    \"\"\" pulls historical price, volume, and mcap data for asset ids in cw_df.\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): The base URL for the CoinMarketCap API.\n",
    "        session (requests.Session): A requests.Session object to be used to make the API calls.\n",
    "        cw_df (pd.DataFrame): A pandas DataFrame that contains information about the assets to \n",
    "                                 retrieve data for. Must include columns 'cmc_id' and 'cmc_slug'.\n",
    "        start_date (date): A datetime.date object representing the start date for the study period.\n",
    "        end_date (date): A datetime.date object representing the end date for the study period.\n",
    "        \n",
    "    Returns:\n",
    "        df (pd.DataFrame): price, volume, and mcap for target assets within specified date range. \n",
    "                           The DataFrame has columns 'cmc_id', 'date', 'usd_per_asset', 'usd_mcap',\n",
    "                           and 'usd_volume_24h'.\n",
    "    \"\"\"\n",
    "    # initialize list to build\n",
    "    cw_dfs = []\n",
    "\n",
    "    # set up target url\n",
    "    endpoint = '/v1/cryptocurrency/quotes/historical'\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    # loop over assets\n",
    "    asset_ids = list(cw_df.cmc_id.values)\n",
    "    asset_names = list(cw_df.cmc_slug.values)\n",
    "    for i, (asset_id, asset_name) in enumerate(zip(asset_ids, asset_names)):\n",
    "        # monitor progress\n",
    "        print(f\"Processing the {i+1}th asset ({(i+1)/len(asset_ids)*100:.2f}%): {asset_name}\")\n",
    "\n",
    "        # build parameters\n",
    "        params = {'id': str(asset_id),\n",
    "                'time_start': start_date.strftime('%Y-%m-%d'),\n",
    "                'time_end': end_date.strftime('%Y-%m-%d'),\n",
    "                'count': 1,\n",
    "                'interval': '1d',\n",
    "                'convert': 'USD'} \n",
    "        \n",
    "        # make the api call\n",
    "        data = makeCMCApiCall(session, url, params, retries=3)\n",
    "\n",
    "        # clean the data\n",
    "        if data != None:\n",
    "            if data['is_fiat'] == 0:\n",
    "                asset_quote_dict_list = []\n",
    "                for quote in data['quotes']:\n",
    "                    new_dict = {}\n",
    "                    new_dict['date']           = quote['quote']['USD']['timestamp'][:10]\n",
    "                    new_dict['usd_per_asset']  = quote['quote']['USD']['price']\n",
    "                    new_dict['usd_volume_24h'] = quote['quote']['USD']['volume_24h']\n",
    "                    new_dict['usd_mcap']       = quote['quote']['USD']['market_cap']\n",
    "                    asset_quote_dict_list.append(new_dict)\n",
    "\n",
    "                cw_df = pd.DataFrame(asset_quote_dict_list)\n",
    "                cw_df['cmc_id'] = data['id']\n",
    "                cw_dfs.append(cw_df)\n",
    "            else:\n",
    "                print(f\"{data['name']} is fiat\")        \n",
    "\n",
    "        # space out calls\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # build final dataframe\n",
    "    df = pd.concat(cw_dfs)\n",
    "\n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "221b6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialCleanAssetMetadata(cw_df: pd.DataFrame, column_map: dict, dropna: bool = True) -> pd.DataFrame:\n",
    "    \"\"\" Clean asset metadata to return cleaned dataframe. \n",
    "\n",
    "    Args:\n",
    "        cw_df (pd.DataFrame): DataFrame containing asset metadata to be cleaned.\n",
    "        column_map (dict): a mapping of the current column names to the desired column names.\n",
    "        dropna (bool): whether to drop any rows with missing values in key columns.\n",
    "    \n",
    "    Returns:\n",
    "        cw_df (pd.DataFrame): cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # confirm has required columns\n",
    "    for k, v in column_map.items():\n",
    "        if k not in cw_df.columns:\n",
    "            raise ValueError(f\"Input DataFrame must contain '{k}' column.\")\n",
    "        \n",
    "    # apply column map renaming\n",
    "    cw_df = cw_df.rename(columns=column_map)\n",
    "\n",
    "    # subset to useful columns\n",
    "    cw_df = cw_df[['cmc_id', 'slug_cmc', 'symbol_cmc', 'first_date_cmc', 'last_date_cmc']]\n",
    "\n",
    "    # convert date columns to date type\n",
    "    cw_df['first_date_cmc'] = pd.to_datetime(cw_df.first_date_cmc, format='%Y-%m-%d', utc=False)\n",
    "    cw_df['last_date_cmc'] = pd.to_datetime(cw_df.last_date_cmc, format='%Y-%m-%d', utc=False)\n",
    "\n",
    "    # drop rows with missing values in key columns\n",
    "    if dropna:\n",
    "        cw_df = cw_df.dropna(subset=['cmc_id', 'slug_cmc'])\n",
    "    \n",
    "    # assert that each row has a unique `cmc_id` and `slug_cmc` value (if desired)\n",
    "    if len(cw_df) != len(cw_df['cmc_id'].unique()):\n",
    "        raise ValueError(\"Input DataFrame has non-unique 'cmc_id' values.\")\n",
    "    if len(cw_df) != len(cw_df['slug_cmc'].unique()):\n",
    "        raise ValueError(\"Input DataFrame has non-unique 'slug_cmc' values.\")\n",
    "    \n",
    "    # sort values and reset index\n",
    "    cw_df = cw_df.sort_values(by='cmc_id', ignore_index=True)\n",
    "\n",
    "    return cw_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdbeaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialCleanPanel(panel_df: pd.DataFrame, start_year: int=2015, end_date: str='2023-02-02', ) -> pd.DataFrame:\n",
    "    \"\"\" clean panel of cmc prices, volume, and mcap data.\n",
    "     \n",
    "    Args:\n",
    "        panel_df (pandas.DataFrame): panel data to clean.\n",
    "        start_year (int): the minimum year to include in the DataFrame (default: 2015).\n",
    "        end_date (str): the maximum date (inclusive) to include in the DataFrame (default: '2023-02-02').\n",
    "\n",
    "    Returns:\n",
    "        (pd.DataFrame): The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # confirm has the right columns\n",
    "    expected_cols = ['date', 'cmc_id', 'usd_per_token', 'usd_mcap', 'usd_volume_24h']\n",
    "    if not all(col in panel_df.columns for col in expected_cols):\n",
    "        raise ValueError(f\"Missing expected columns: {expected_cols}\")\n",
    "    \n",
    "    # rename columns to standard convention (with data source name in it)\n",
    "    panel_df = panel_df.rename(columns = {'usd_per_token': 'usd_per_token_cmc',\n",
    "                                          'usd_mcap': 'usd_mcap_cmc',\n",
    "                                          'usd_volume_24h': 'usd_volume_24h_cmc'})\n",
    "\n",
    "    # convert columns to correct data type\n",
    "    panel_df['date'] = pd.to_datetime(panel_df.date, format='%Y-%m-%d', utc=False)\n",
    "\n",
    "    # set column order\n",
    "    panel_df = panel_df[['date', 'cmc_id', 'usd_per_token_cmc', 'usd_mcap_cmc', 'usd_volume_24h_cmc']]\n",
    "\n",
    "    # drop rows\n",
    "    panel_df = panel_df[(panel_df.date.dt.year >= 2015) & (panel_df.date <= '2023-02-02')]\n",
    "    panel_df = panel_df.dropna(how='any', subset=['date', 'cmc_id'])\n",
    "    panel_df = panel_df.dropna(how='all', subset=['usd_per_token_cmc', 'usd_mcap_cmc', 'usd_volume_24h_cmc'])\n",
    "\n",
    "    # form list of data columns to work with\n",
    "    data_cols = list(panel_df.columns.values)\n",
    "    data_cols.remove('date')\n",
    "    data_cols.remove('cmc_id')\n",
    "\n",
    "    # set negative values to missing and too large values to missing\n",
    "    for col in data_cols:\n",
    "        panel_df.loc[panel_df[col] < 0, col] = np.nan\n",
    "        panel_df.loc[panel_df[col] > 2e12, col] = np.nan\n",
    "\n",
    "    # drop duplicated rows across id columns\n",
    "    panel_df = panel_df.drop_duplicates(subset=['date', 'cmc_id'])\n",
    "\n",
    "    # sort values and reset index\n",
    "    panel_df = panel_df.sort_values(by=['date', 'cmc_id'], \n",
    "                                    ignore_index=True)\n",
    "\n",
    "    return panel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57682fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGeomAvg(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the geometric average of a vector of simple returns.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar geometric average.\n",
    "    \"\"\"\n",
    "    geom_avg_at_given_freq = np.prod(1+returns)**(1/len(returns))-1\n",
    "    if annualized==False:\n",
    "        return geom_avg_at_given_freq\n",
    "    else:\n",
    "        return (geom_avg_at_given_freq+1)**periods_in_year-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bafebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepPanelForInitialInclusiveCriteria(panel_df: pd.DataFrame, cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" performs various ad hoc cleaning to prep the panel further for applying inclusion criteria.\n",
    "    \n",
    "    Args:\n",
    "        panel_df (pd.DataFrame): panel of asset prices, trading volumes, and mcaps from cmc.\n",
    "        cw_df (pd.DataFrame): identifying variables for the assets.\n",
    "        \n",
    "    Returns:\n",
    "        panel_df (pd.DataFrame): cleaned panel.\n",
    "    \"\"\"\n",
    "    # manually remove tokens from panel\n",
    "    tokens_to_remove = [770, 776, 3787, 8644, 9103]\n",
    "    panel_df = panel_df[~panel_df.cmc_id.isin(tokens_to_remove)]\n",
    "\n",
    "    # merge on cmc slug and drop the cmc id\n",
    "    panel_df = panel_df.merge(cw_df[['cmc_id', 'slug_cmc']],\n",
    "                              on='cmc_id',\n",
    "                              how='inner',\n",
    "                              validate='many_to_one')\n",
    "    panel_df = panel_df.drop('cmc_id', axis=1)\n",
    "    panel_df = panel_df[['date', 'slug_cmc', 'usd_per_token_cmc', 'usd_mcap_cmc', 'usd_volume_24h_cmc']]\n",
    "    panel_df = panel_df.sort_values(by=['date', 'slug_cmc'], ignore_index=True)\n",
    "\n",
    "    # adjust particular values\n",
    "    panel_df.loc[(panel_df.slug_cmc=='uquid-coin') & \n",
    "                  panel_df.usd_volume_24h_cmc.isnull(), 'usd_volume_24h_cmc'] = 0\n",
    "\n",
    "    # ensure no missing in the df\n",
    "    assert(0==panel_df.isnull().sum().sum())\n",
    "\n",
    "    # ensure unique on key columns\n",
    "    dups = panel_df.duplicated(subset=['date', 'slug_cmc'])\n",
    "    assert(~dups.any()),('there are duplicates in the data on keys date and slug_cmc')\n",
    "\n",
    "    # drop more tokens manually\n",
    "    # NOTES: ampleforth is a stablecoin, pax gold is a gold stablecoin, index, and wrapped tokens\n",
    "    wrapped_tokens_to_drop = ['ampleforth', 'cryptoindex-com-100', 'pax-gold',\n",
    "                            'wrapped-centrifuge', 'wrapped-luna-token', 'wrapped-ncg', 'wrapped-nxm']\n",
    "    panel_df = panel_df[~panel_df.slug_cmc.isin(wrapped_tokens_to_drop)]\n",
    "\n",
    "    return panel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGeomAvg(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the geometric average of a vector of simple returns.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar geometric average.\n",
    "    \"\"\"\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        raise TypeError(\"Input 'returns' must be a NumPy array\")\n",
    "    if annualized and periods_in_year is None:\n",
    "        raise ValueError(\"Input 'periods_in_year' must be provided if 'annualized' is True\")\n",
    "    geom_avg_at_given_freq = np.prod(1 + returns) ** (1 / np.size(returns)) - 1\n",
    "    return (geom_avg_at_given_freq + 1) ** periods_in_year - 1 if annualized else geom_avg_at_given_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eaae27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildInitialAssetUniverse(panel_df: pd.DataFrame, start_date: date, end_date: date) -> dict:\n",
    "    \"\"\" build an initial universe of assets to pull data for.\n",
    "    \n",
    "    Args:\n",
    "        panel_df (pd.DataFrame): panel of asset prices, trading volumes, and mcaps from cmc.\n",
    "        start_date (date): A datetime.date object representing the start date for the study period.\n",
    "        end_date (date): A datetime.date object representing the end date for the study period.\n",
    "    \n",
    "    Returns:\n",
    "        asset_universe (dict): keys of start of each month in study period with associated value\n",
    "                               of list of asset names to include.\n",
    "    \"\"\"\n",
    "    # specify the dates to obtain\n",
    "    dates = [start_date.strftime('%Y-%m-%d')]\n",
    "    current_date = start_date+relativedelta(months=1)\n",
    "    while current_date <= end_date:\n",
    "        dates.append(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date += relativedelta(months=1)\n",
    "\n",
    "    # apply suff data, volume, and mcap filters\n",
    "    asset_universe_per_month = []\n",
    "    for i in range(len(dates)-1):\n",
    "        # determine start and end dates for window\n",
    "        start_window = dates[i]\n",
    "        end_window   = dates[i+1]\n",
    "\n",
    "        # build temporary dataframe for this time period\n",
    "        temp_df = panel_df[(panel_df.date >= start_window) & (panel_df.date <= end_window)].copy()\n",
    "\n",
    "        # obtain list of tokens to consider\n",
    "        assets_included = list(np.unique(temp_df[temp_df.date == end_window].slug_cmc.values))\n",
    "\n",
    "        # figure out tokens removed due to insuff data\n",
    "        # note: 28 days ensures at least 4 weeks of data \n",
    "        asset_ns_df = temp_df.groupby('slug_cmc').size()\n",
    "        assets_lost_given_insuff_data = list(asset_ns_df[asset_ns_df < 28].index.values)\n",
    "        for asset in assets_lost_given_insuff_data:\n",
    "            if asset in assets_included:\n",
    "                assets_included.remove(asset)\n",
    "\n",
    "        # Figure out tokens removed due to volume threshold\n",
    "        temp_vol_df = temp_df.groupby('slug_cmc').usd_volume_24h_cmc.min()\n",
    "        assets_lost_given_insuff_vol = list(temp_vol_df[temp_vol_df < 10000].index.values)\n",
    "        for asset in assets_lost_given_insuff_vol:\n",
    "            if asset in assets_included:\n",
    "                assets_included.remove(asset)\n",
    "\n",
    "        # Figure out assets removed due to mcap threshold\n",
    "        current_year = int(end_window[:4]) \n",
    "        if current_year <= 2016:\n",
    "            mcap_threshold = 750000\n",
    "        elif current_year == 2017:\n",
    "            mcap_threshold = 2e6\n",
    "        elif current_year == 2018:\n",
    "            mcap_threshold = 30e6\n",
    "        elif current_year in [2019, 2020]:\n",
    "            mcap_threshold = 15e6\n",
    "        elif current_year >= 2021:\n",
    "            mcap_threshold = 75e6\n",
    "        temp_mcap_df = temp_df.groupby('slug_cmc').usd_mcap_cmc.min()\n",
    "        assets_lost_given_mcap_threshold = list(temp_mcap_df[temp_mcap_df < mcap_threshold].index.values)\n",
    "        for asset in assets_lost_given_mcap_threshold:\n",
    "            if asset in assets_included:\n",
    "                assets_included.remove(asset)\n",
    "\n",
    "        # Report out new asset ever\n",
    "        print('New assets that we have never had are ')\n",
    "        if i != 0:\n",
    "            all_assets = []\n",
    "            for j in range(i-1,-1,-1):\n",
    "                all_assets += asset_universe_per_month[j]\n",
    "            print(np.unique(set(assets_included).difference(set(all_assets))))\n",
    "        else:\n",
    "            print(np.unique(assets_included))\n",
    "        print('\\n')\n",
    "\n",
    "        # Report out assets for this month\n",
    "        print(f'This month\\'s ({end_window}) {len(assets_included)} assets are:')\n",
    "        print(np.unique(assets_included))\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # Add assets to list\n",
    "        asset_universe_per_month.append(list(np.unique(assets_included)))\n",
    "\n",
    "    # build asset universe\n",
    "    asset_universe_dict = {}\n",
    "    for i in range(len(dates)-1):\n",
    "        asset_universe_dict[dates[i+1]] = asset_universe_per_month[i]\n",
    "\n",
    "    return asset_universe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineUniqueAssets(asset_universe_dict) -> list:\n",
    "    \"\"\" determine the unique assets in the universe to return as a list. \"\"\"\n",
    "    assets = []\n",
    "    for k, v in asset_universe_dict.items():\n",
    "        assets.extend(v)\n",
    "    assets = list(np.unique(np.array(assets)))\n",
    "    assets.sort()\n",
    "    return assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullCMCMacro(base_url: str, session: Session, start_date: date, end_date: date) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        base_url (str): The base URL for the CoinMarketCap API.\n",
    "        session (requests.Session): A requests.Session object to be used to make the API calls.\n",
    "        start_window (date): A datetime.date object representing the start date for the study period.\n",
    "        end_date (date): A datetime.date object representing the end date for the study period.\n",
    "\n",
    "    Returns:\n",
    "        macro_df (pd.DataFrame): time series data of cmc macro covariates.\n",
    "    \"\"\"\n",
    "        \n",
    "    # set up the call\n",
    "    endpoint = '/v1/global-metrics/quotes/historical'\n",
    "    url      = f\"{base_url}{endpoint}\"\n",
    "    params = {'time_start': start_date.strftime('%Y-%m-%d'),\n",
    "            'time_end': end_date.strftime('%Y-%m-%d'),\n",
    "            'count': 10,\n",
    "            'interval': '1d',\n",
    "            'convert': 'USD',\n",
    "            'aux': 'btc_dominance,active_cryptocurrencies,active_exchanges,active_market_pairs,total_volume_24h,altcoin_market_cap,altcoin_volume_24h'}\n",
    "\n",
    "    # make the call\n",
    "    data = makeCMCApiCall(session, url, params, retries=3)\n",
    "\n",
    "    # initialize dictionary for the data\n",
    "    cmc_macro_dict = {'date': [],\n",
    "                    'total_market_cap': [],\n",
    "                    'total_volume_24h': [],\n",
    "                    'altcoin_market_cap': [],\n",
    "                    'altcoin_volume_24h': [],\n",
    "                    'btc_dominance': [],\n",
    "                    'active_cryptocurrencies': [],\n",
    "                    'active_exchanges': [],\n",
    "                    'active_market_pairs': []}\n",
    "\n",
    "    # convert JSON into dictionary\n",
    "    for days_data in data['quotes']:\n",
    "        cmc_macro_dict['date'].append(days_data['timestamp'])\n",
    "        cmc_macro_dict['total_market_cap'].append(days_data['quote']['USD']['total_market_cap'])\n",
    "        cmc_macro_dict['total_volume_24h'].append(days_data['quote']['USD']['total_volume_24h'])\n",
    "        cmc_macro_dict['altcoin_market_cap'].append(days_data['quote']['USD']['altcoin_market_cap'])\n",
    "        cmc_macro_dict['altcoin_volume_24h'].append(days_data['quote']['USD']['altcoin_volume_24h'])\n",
    "        cmc_macro_dict['btc_dominance'].append(days_data['btc_dominance'])\n",
    "        cmc_macro_dict['active_cryptocurrencies'].append(days_data['active_cryptocurrencies'])\n",
    "        cmc_macro_dict['active_exchanges'].append(days_data['active_exchanges'])\n",
    "        cmc_macro_dict['active_market_pairs'].append(days_data['active_market_pairs'])\n",
    "\n",
    "    # clean up the dataframe to have all study period dates and interpolate missing dates\n",
    "    macro_df = pd.DataFrame(cmc_macro_dict)\n",
    "    macro_df['date'] = pd.to_datetime(macro_df.date).dt.ceil('D')\n",
    "    macro_df['date'] = macro_df.date.dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    api_fp = '../../admin/cmc.txt'\n",
    "    start_date = date(2015, 1, 1)\n",
    "    end_date   = date(2023, 2, 1)\n",
    "    base_url = \"https://pro-api.coinmarketcap.com\"\n",
    "    asset_fp = \"../data/raw/cmc_asset_universe.pkl\"\n",
    "    panel_fp = \"../data/raw/cmc_price_volume_mcap_panel.pkl\"\n",
    "    cw_new_old_col_mapping  = {'cmc_symbol': 'symbol_cmc',\n",
    "                               'cmc_slug': 'slug_cmc',\n",
    "                               'cmc_first_date': 'first_date_cmc',\n",
    "                               'cmc_last_date': 'last_date_cmc'}\n",
    "\n",
    "    # import api key\n",
    "    with open(api_fp) as f:\n",
    "        API_KEY = f.readlines()\n",
    "        API_KEY = API_KEY[0].strip()\n",
    "    \n",
    "    # confirm api is working\n",
    "    session = initiateAPI(base_url)\n",
    "\n",
    "    # obtain potential asset ids to include in study\n",
    "    cmc_ids  = obtainTopCMCAssets(base_url, session, start_date, end_date)\n",
    "    cw_df = formDataframeOfTopCMCAssets(base_url, session, cmc_ids)\n",
    "\n",
    "    # obtain price, volume, and mcap data for target assets\n",
    "    panel_df = pullPriceMcapVolume(base_url, session, cw_df, start_date, end_date)\n",
    "\n",
    "    # clean the data\n",
    "    cw_df = initialCleanAssetMetadata(cw_df, cw_new_old_col_mapping)\n",
    "    panel_df = initialCleanPanel(panel_df)\n",
    "\n",
    "    # cut down to initial inclusion criteria so i pull just these across other providers\n",
    "    panel_df = prepPanelForInitialInclusiveCriteria(panel_df, cw_df)\n",
    "    asset_universe_dict = buildInitialAssetUniverse(panel_df, start_date, end_date)\n",
    "    asset_universe_list = determineUniqueAssets(asset_universe_dict)\n",
    "\n",
    "    # pull remaining cmc data\n",
    "    # TODO historical crypto metadata\n",
    "    macro_df = pullCMCMacro(base_url, session, start_date, end_date)\n",
    "    # TODO historical exchange data\n",
    "\n",
    "    # save the data\n",
    "    cw_df.to_pickle(asset_fp)\n",
    "    panel_df.to_pickle(panel_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "88acaddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cf448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ddc61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a5546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORE RETURNS OF EQUAL- AND MCAP- WEIGHTED PORTFOLIOS\n",
    "\n",
    "# Cut the panel down to just the assets of interest\n",
    "asset_universe_unique = list(np.unique([asset \n",
    "                                        for sublist in token_universe_per_month \n",
    "                                        for asset in sublist]))\n",
    "df = df[df.slug_cmc.isin(asset_universe_unique)]\n",
    "\n",
    "# Drop rows that do not have previous day information\n",
    "df = df.sort_values(by=['slug_cmc', 'date'], ignore_index=True)\n",
    "df.loc[1:, 'day_diff'] = (df.date[1:].values - df.date[:-1]).values.astype('timedelta64[D]').astype(int)\n",
    "df['day_diff2'] = df.day_diff.shift(-1)\n",
    "num_rows = df[df.day_diff == 1].shape[0]\n",
    "df = df[(df.day_diff == 1) | (df.day_diff2 == 1)]\n",
    "assert(num_rows <= df.shape[0])\n",
    "df = df.drop(['day_diff2'], axis=1)\n",
    "\n",
    "# Calculate day over day return\n",
    "df['r_t'] = df.groupby('slug_cmc')['usd_per_token_cmc'].apply(pd.Series.pct_change)\n",
    "df = df[df.day_diff == 1]\n",
    "tokens_to_drop = np.unique(df[df.r_t.isnull()].slug_cmc.values)\n",
    "df = df[~df.slug_cmc.isin(tokens_to_drop)]\n",
    "df = df.drop('day_diff', axis=1)\n",
    "\n",
    "# Cut down to time period of interest\n",
    "df = df[df.date.dt.year >= 2016]\n",
    "df = df[df.date.dt.year <= 2022]\n",
    "\n",
    "# Ensure no missings\n",
    "assert(0 == df.isnull().sum().sum())\n",
    "\n",
    "# Clean up index and resort\n",
    "df = df.sort_values(by=['date', 'slug_cmc'], ignore_index=True)\n",
    "\n",
    "# Calculate equal and mcap weighted returns by quarter\n",
    "equal_df = pd.DataFrame()\n",
    "mcap_df  = pd.DataFrame()\n",
    "for i in range(1,len(dates)):\n",
    "    # Set up dates and asset universe\n",
    "    date = dates[i]\n",
    "    date_plus_1mo = datetime.datetime.strptime(date, '%Y-%m-%d') + relativedelta(months=1)\n",
    "    asset_universe = asset_universe_dict[date]\n",
    "\n",
    "    # Subset to relevant data\n",
    "    temp_df = df[(df.date >= date) & (df.date < date_plus_1mo)]\n",
    "    temp_df = temp_df[temp_df.slug_cmc.isin(asset_universe)]\n",
    "\n",
    "    # Form equal weighted returns\n",
    "    temp_eq_df = temp_df.groupby('date')[['r_t']].mean()\n",
    "    equal_df = pd.concat((equal_df, temp_eq_df))\n",
    "\n",
    "    # Form mcap weighted returns\n",
    "    temp_df['mcap_sum'] = temp_df.groupby('date')['usd_mcap_cmc'].transform('sum')\n",
    "    temp_df['mcap_weight'] = temp_df.usd_mcap_cmc / temp_df.mcap_sum\n",
    "    temp_df['mcap_r_t'] = temp_df.r_t * temp_df.mcap_weight\n",
    "    temp_mcap_df = temp_df.groupby('date')[['mcap_r_t']].sum()\n",
    "    mcap_df = pd.concat((mcap_df, temp_mcap_df))\n",
    "\n",
    "# Ensure no missing\n",
    "assert(0==equal_df.isnull().sum().values)\n",
    "assert(0==mcap_df.isnull().sum().values)\n",
    "\n",
    "# Report returns\n",
    "print('equal weighted return:')\n",
    "print(equal_df.apply(geometricAverageSimpleReturns, axis=0).values[0])\n",
    "print('sharpe:')\n",
    "print(np.mean(equal_df.r_t.values)/np.std(equal_df.r_t.values))\n",
    "print('mcap weighted return:')\n",
    "print(mcap_df.apply(geometricAverageSimpleReturns, axis=0).values[0])\n",
    "print('sharpe:')\n",
    "print(np.mean(mcap_df.mcap_r_t.values)/np.std(mcap_df.mcap_r_t.values))\n",
    "\n",
    "# Form the returns by year\n",
    "equal_df['year'] = equal_df.index.year\n",
    "mcap_df['year'] = mcap_df.index.year\n",
    "print('equal weighted return:')\n",
    "print(equal_df.groupby('year').apply(geometricAverageSimpleReturns))\n",
    "print('mcap weighted return:')\n",
    "print(mcap_df.groupby('year').apply(geometricAverageSimpleReturns))\n",
    "equal_df = equal_df.drop('year', axis=1)\n",
    "mcap_df  = mcap_df.drop('year', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CLEANING SCRIPT:\n",
    "\n",
    "# ensure each asset does not appear before first date nor after last date\n",
    "# cmc_ids = np.unique(panel_df.cmc_id.values)\n",
    "# for cmc_id in cmc_ids:\n",
    "#     print(cmc_id)\n",
    "#     first_date = cw_df[cw_df.cmc_id==cmc_id].first_date_cmc.values[0]\n",
    "#     last_date  = cw_df[cw_df.cmc_id==cmc_id].last_date_cmc.values[0]\n",
    "#     assert(0==panel_df[(panel_df.cmc_id==cmc_id)&(panel_df.date<first_date)].shape[0])\n",
    "#     assert(0==panel_df[(panel_df.cmc_id==cmc_id)&(panel_df.date>last_date)].shape[0])\n",
    "\n",
    "# ensure each asset has consecutive data, interpolate where needed with forward fill\n",
    "\n",
    "# group the data by cmc_id to loop over\n",
    "grouped = panel_df.groupby('cmc_id')\n",
    "\n",
    "# interate through each cmc_id\n",
    "dfs = []\n",
    "for name, group in grouped:\n",
    "    # find the first and last dates for the current id\n",
    "    first_date = group['date'].min()\n",
    "    last_date  = group['date'].max()\n",
    "\n",
    "    # create a new dataframe with all the possible combinations of cmc_id and date\n",
    "    dates = pd.date_range(first_date, last_date)\n",
    "    index = pd.MultiIndex.from_product([[name], dates], names=['cmc_id', 'date'])\n",
    "    full_df = pd.DataFrame(index=index).reset_index()\n",
    "\n",
    "    # merge the full dataframe with the original dataframe to fill in missing values with NaNs\n",
    "    merged_df = pd.merge(full_df, group, on=['cmc_id', 'date'], how='left')\n",
    "\n",
    "    # interpolate the missing values using forward fill for up to 7 consecutive observations\n",
    "    interpolated_df = merged_df.fillna(method='ffill', limit=21)\n",
    "\n",
    "    # Check if there are any missing values in the remaining columns for the current id and date range\n",
    "    if interpolated_df.isnull().values.any():\n",
    "        print(f\"ID {name} has missing values in the given date range, precisely: {int(interpolated_df.isnull().sum().sum()/3)}.\")\n",
    "        break\n",
    "\n",
    "    # combine    \n",
    "    dfs.append(interpolated_df)\n",
    "\n",
    "# Combine all the dataframes and drop the 'cmc_id' index level\n",
    "result_df = pd.concat(dfs).reset_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffa43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR INCLUSION CRIT SCRIPT\n",
    "\n",
    "\n",
    "# Jan 1 2015 - $5B - $500k\n",
    "# Jan 1 2016 - $7B - $700k\n",
    "# Jan 1 2017 - $18B - $1.8M\n",
    "# Jan 1 2018 - $600B - $60M\n",
    "# Apr 1 2018 - $300B - $30M\n",
    "# Jul 1 2018 - $250B - $25M\n",
    "# Jan 1 2019 - $125B - $12M\n",
    "# Apr 1 2019 - $145B - $14M\n",
    "# Jul 1 2019 - $330B - $33M\n",
    "# Oct 1 2019 - $220B - $22M\n",
    "# Jan 1 2020 - $200B - $20M\n",
    "# Apr 1 2020 - $175B - $17M\n",
    "# Jul 1 2020 - $260B - $26M\n",
    "# Oct 1 2020 - $340B - $34M\n",
    "# Jan 1 2021 - $770B - $77M\n",
    "# Apr 1 2021 - $1.9T - $190M\n",
    "# Jul 1 2021 - $1.4T - $140M\n",
    "# Oct 1 2021 - $2T - $200M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "d730593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-22\n",
      "\n",
      "\n",
      "2020-09-23\n",
      "\n",
      "\n",
      "2020-09-24\n",
      "\n",
      "\n",
      "2020-09-25\n",
      "\n",
      "\n",
      "2020-09-26\n",
      "\n",
      "\n",
      "2020-09-27\n",
      "\n",
      "\n",
      "2020-09-28\n",
      "\n",
      "\n",
      "2020-09-29\n",
      "\n",
      "\n",
      "2020-09-30\n",
      "\n",
      "\n",
      "2020-10-01\n",
      "\n",
      "\n",
      "2020-10-02\n",
      "\n",
      "\n",
      "2020-10-03\n",
      "\n",
      "\n",
      "2020-10-04\n",
      "\n",
      "\n",
      "2020-10-05\n",
      "\n",
      "\n",
      "2020-10-06\n",
      "\n",
      "\n",
      "2020-10-07\n",
      "\n",
      "\n",
      "2020-10-08\n",
      "\n",
      "\n",
      "2020-10-09\n",
      "\n",
      "\n",
      "2020-10-10\n",
      "\n",
      "\n",
      "2020-10-11\n",
      "\n",
      "\n",
      "2020-10-12\n",
      "\n",
      "\n",
      "2020-10-13\n",
      "\n",
      "\n",
      "2020-10-14\n",
      "\n",
      "\n",
      "2020-10-15\n",
      "\n",
      "\n",
      "2020-10-16\n",
      "\n",
      "\n",
      "2020-10-17\n",
      "\n",
      "\n",
      "2020-10-18\n",
      "\n",
      "\n",
      "2020-10-19\n",
      "\n",
      "\n",
      "2020-10-20\n",
      "\n",
      "\n",
      "2020-10-21\n",
      "\n",
      "\n",
      "2020-10-22\n",
      "\n",
      "\n",
      "2020-10-23\n",
      "\n",
      "\n",
      "2020-10-24\n",
      "\n",
      "\n",
      "2020-10-25\n",
      "\n",
      "\n",
      "2020-10-26\n",
      "\n",
      "\n",
      "2020-10-27\n",
      "\n",
      "\n",
      "2020-10-28\n",
      "\n",
      "\n",
      "2020-10-29\n",
      "\n",
      "\n",
      "2020-10-30\n",
      "\n",
      "\n",
      "2020-10-31\n",
      "\n",
      "\n",
      "2020-11-01\n",
      "\n",
      "\n",
      "2020-11-02\n",
      "\n",
      "\n",
      "2020-11-03\n",
      "\n",
      "\n",
      "2020-11-04\n",
      "\n",
      "\n",
      "2020-11-05\n",
      "\n",
      "\n",
      "2020-11-06\n",
      "\n",
      "\n",
      "2020-11-07\n",
      "\n",
      "\n",
      "2020-11-08\n",
      "\n",
      "\n",
      "2020-11-09\n",
      "\n",
      "\n",
      "2020-11-10\n",
      "\n",
      "\n",
      "2020-11-11\n",
      "\n",
      "\n",
      "2020-11-12\n",
      "\n",
      "\n",
      "2020-11-13\n",
      "\n",
      "\n",
      "2020-11-14\n",
      "\n",
      "\n",
      "2020-11-15\n",
      "\n",
      "\n",
      "2020-11-16\n",
      "\n",
      "\n",
      "2020-11-17\n",
      "\n",
      "\n",
      "2020-11-18\n",
      "\n",
      "\n",
      "2020-11-19\n",
      "\n",
      "\n",
      "2020-11-20\n",
      "\n",
      "\n",
      "2020-11-21\n",
      "\n",
      "\n",
      "2020-11-22\n",
      "\n",
      "\n",
      "2020-11-23\n",
      "\n",
      "\n",
      "2020-11-24\n",
      "\n",
      "\n",
      "2020-11-25\n",
      "\n",
      "\n",
      "2020-11-26\n",
      "\n",
      "\n",
      "2020-11-27\n",
      "\n",
      "\n",
      "2020-11-28\n",
      "\n",
      "\n",
      "2020-11-29\n",
      "\n",
      "\n",
      "2020-11-30\n",
      "\n",
      "\n",
      "2020-12-01\n",
      "\n",
      "\n",
      "2020-12-02\n",
      "\n",
      "\n",
      "2020-12-03\n",
      "\n",
      "\n",
      "2020-12-04\n",
      "\n",
      "\n",
      "2020-12-05\n",
      "\n",
      "\n",
      "2020-12-06\n",
      "\n",
      "\n",
      "2020-12-07\n",
      "\n",
      "\n",
      "2020-12-08\n",
      "\n",
      "\n",
      "2020-12-09\n",
      "\n",
      "\n",
      "2020-12-10\n",
      "\n",
      "\n",
      "2020-12-11\n",
      "\n",
      "\n",
      "2020-12-12\n",
      "\n",
      "\n",
      "2020-12-13\n",
      "\n",
      "\n",
      "2020-12-14\n",
      "\n",
      "\n",
      "2020-12-15\n",
      "\n",
      "\n",
      "2020-12-16\n",
      "\n",
      "\n",
      "2020-12-17\n",
      "\n",
      "\n",
      "2020-12-18\n",
      "\n",
      "\n",
      "2020-12-19\n",
      "\n",
      "\n",
      "2020-12-20\n",
      "\n",
      "\n",
      "2020-12-21\n",
      "\n",
      "\n",
      "2020-12-22\n",
      "\n",
      "\n",
      "error due to out of range\n",
      "2020-12-23\n",
      "\n",
      "\n",
      "2020-12-24\n",
      "\n",
      "\n",
      "2020-12-25\n",
      "\n",
      "\n",
      "2020-12-26\n",
      "\n",
      "\n",
      "2020-12-27\n",
      "\n",
      "\n",
      "2020-12-28\n",
      "\n",
      "\n",
      "2020-12-29\n",
      "\n",
      "\n",
      "2020-12-30\n",
      "\n",
      "\n",
      "2020-12-31\n",
      "\n",
      "\n",
      "2021-01-01\n",
      "\n",
      "\n",
      "2021-01-02\n",
      "\n",
      "\n",
      "2021-01-03\n",
      "\n",
      "\n",
      "2021-01-04\n",
      "\n",
      "\n",
      "2021-01-05\n",
      "\n",
      "\n",
      "2021-01-06\n",
      "\n",
      "\n",
      "2021-01-07\n",
      "\n",
      "\n",
      "2021-01-08\n",
      "\n",
      "\n",
      "2021-01-09\n",
      "\n",
      "\n",
      "2021-01-10\n",
      "\n",
      "\n",
      "2021-01-11\n",
      "\n",
      "\n",
      "2021-01-12\n",
      "\n",
      "\n",
      "2021-01-13\n",
      "\n",
      "\n",
      "2021-01-14\n",
      "\n",
      "\n",
      "2021-01-15\n",
      "\n",
      "\n",
      "2021-01-16\n",
      "\n",
      "\n",
      "2021-01-17\n",
      "\n",
      "\n",
      "2021-01-18\n",
      "\n",
      "\n",
      "2021-01-19\n",
      "\n",
      "\n",
      "2021-01-20\n",
      "\n",
      "\n",
      "2021-01-21\n",
      "\n",
      "\n",
      "2021-01-22\n",
      "\n",
      "\n",
      "2021-01-23\n",
      "\n",
      "\n",
      "2021-01-24\n",
      "\n",
      "\n",
      "2021-01-25\n",
      "\n",
      "\n",
      "2021-01-26\n",
      "\n",
      "\n",
      "2021-01-27\n",
      "\n",
      "\n",
      "2021-01-28\n",
      "\n",
      "\n",
      "2021-01-29\n",
      "\n",
      "\n",
      "2021-01-30\n",
      "\n",
      "\n",
      "2021-01-31\n",
      "\n",
      "\n",
      "2021-02-01\n",
      "\n",
      "\n",
      "2021-02-02\n",
      "\n",
      "\n",
      "2021-02-03\n",
      "\n",
      "\n",
      "2021-02-04\n",
      "\n",
      "\n",
      "2021-02-05\n",
      "\n",
      "\n",
      "2021-02-06\n",
      "\n",
      "\n",
      "2021-02-07\n",
      "\n",
      "\n",
      "2021-02-08\n",
      "\n",
      "\n",
      "2021-02-09\n",
      "\n",
      "\n",
      "2021-02-10\n",
      "\n",
      "\n",
      "2021-02-11\n",
      "\n",
      "\n",
      "2021-02-12\n",
      "\n",
      "\n",
      "2021-02-13\n",
      "\n",
      "\n",
      "2021-02-14\n",
      "\n",
      "\n",
      "2021-02-15\n",
      "\n",
      "\n",
      "2021-02-16\n",
      "\n",
      "\n",
      "2021-02-17\n",
      "\n",
      "\n",
      "2021-02-18\n",
      "\n",
      "\n",
      "2021-02-19\n",
      "\n",
      "\n",
      "2021-02-20\n",
      "\n",
      "\n",
      "2021-02-21\n",
      "\n",
      "\n",
      "2021-02-22\n",
      "\n",
      "\n",
      "2021-02-23\n",
      "\n",
      "\n",
      "2021-02-24\n",
      "\n",
      "\n",
      "2021-02-25\n",
      "\n",
      "\n",
      "2021-02-26\n",
      "\n",
      "\n",
      "2021-02-27\n",
      "\n",
      "\n",
      "2021-02-28\n",
      "\n",
      "\n",
      "2021-03-01\n",
      "\n",
      "\n",
      "2021-03-02\n",
      "\n",
      "\n",
      "2021-03-03\n",
      "\n",
      "\n",
      "2021-03-04\n",
      "\n",
      "\n",
      "2021-03-05\n",
      "\n",
      "\n",
      "2021-03-06\n",
      "\n",
      "\n",
      "2021-03-07\n",
      "\n",
      "\n",
      "2021-03-08\n",
      "\n",
      "\n",
      "2021-03-09\n",
      "\n",
      "\n",
      "2021-03-10\n",
      "\n",
      "\n",
      "2021-03-11\n",
      "\n",
      "\n",
      "2021-03-12\n",
      "\n",
      "\n",
      "2021-03-13\n",
      "\n",
      "\n",
      "2021-03-14\n",
      "\n",
      "\n",
      "2021-03-15\n",
      "\n",
      "\n",
      "2021-03-16\n",
      "\n",
      "\n",
      "2021-03-17\n",
      "\n",
      "\n",
      "2021-03-18\n",
      "\n",
      "\n",
      "2021-03-19\n",
      "\n",
      "\n",
      "2021-03-20\n",
      "\n",
      "\n",
      "2021-03-21\n",
      "\n",
      "\n",
      "2021-03-22\n",
      "\n",
      "\n",
      "2021-03-23\n",
      "\n",
      "\n",
      "2021-03-24\n",
      "\n",
      "\n",
      "2021-03-25\n",
      "\n",
      "\n",
      "2021-03-26\n",
      "\n",
      "\n",
      "2021-03-27\n",
      "\n",
      "\n",
      "2021-03-28\n",
      "\n",
      "\n",
      "2021-03-29\n",
      "\n",
      "\n",
      "2021-03-30\n",
      "\n",
      "\n",
      "2021-03-31\n",
      "\n",
      "\n",
      "2021-04-01\n",
      "\n",
      "\n",
      "2021-04-02\n",
      "\n",
      "\n",
      "2021-04-03\n",
      "\n",
      "\n",
      "2021-04-04\n",
      "\n",
      "\n",
      "2021-04-05\n",
      "\n",
      "\n",
      "2021-04-06\n",
      "\n",
      "\n",
      "2021-04-07\n",
      "\n",
      "\n",
      "2021-04-08\n",
      "\n",
      "\n",
      "2021-04-09\n",
      "\n",
      "\n",
      "2021-04-10\n",
      "\n",
      "\n",
      "2021-04-11\n",
      "\n",
      "\n",
      "2021-04-12\n",
      "\n",
      "\n",
      "2021-04-13\n",
      "\n",
      "\n",
      "2021-04-14\n",
      "\n",
      "\n",
      "2021-04-15\n",
      "\n",
      "\n",
      "2021-04-16\n",
      "\n",
      "\n",
      "2021-04-17\n",
      "\n",
      "\n",
      "2021-04-18\n",
      "\n",
      "\n",
      "2021-04-19\n",
      "\n",
      "\n",
      "2021-04-20\n",
      "\n",
      "\n",
      "2021-04-21\n",
      "\n",
      "\n",
      "2021-04-22\n",
      "\n",
      "\n",
      "2021-04-23\n",
      "\n",
      "\n",
      "2021-04-24\n",
      "\n",
      "\n",
      "2021-04-25\n",
      "\n",
      "\n",
      "2021-04-26\n",
      "\n",
      "\n",
      "2021-04-27\n",
      "\n",
      "\n",
      "2021-04-28\n",
      "\n",
      "\n",
      "2021-04-29\n",
      "\n",
      "\n",
      "2021-04-30\n",
      "\n",
      "\n",
      "2021-05-01\n",
      "\n",
      "\n",
      "2021-05-02\n",
      "\n",
      "\n",
      "2021-05-03\n",
      "\n",
      "\n",
      "2021-05-04\n",
      "\n",
      "\n",
      "2021-05-05\n",
      "\n",
      "\n",
      "2021-05-06\n",
      "\n",
      "\n",
      "2021-05-07\n",
      "\n",
      "\n",
      "2021-05-08\n",
      "\n",
      "\n",
      "2021-05-09\n",
      "\n",
      "\n",
      "2021-05-10\n",
      "\n",
      "\n",
      "2021-05-11\n",
      "\n",
      "\n",
      "2021-05-12\n",
      "\n",
      "\n",
      "2021-05-13\n",
      "\n",
      "\n",
      "2021-05-14\n",
      "\n",
      "\n",
      "2021-05-15\n",
      "\n",
      "\n",
      "2021-05-16\n",
      "\n",
      "\n",
      "2021-05-17\n",
      "\n",
      "\n",
      "2021-05-18\n",
      "\n",
      "\n",
      "2021-05-19\n",
      "\n",
      "\n",
      "2021-05-20\n",
      "\n",
      "\n",
      "2021-05-21\n",
      "\n",
      "\n",
      "2021-05-22\n",
      "\n",
      "\n",
      "2021-05-23\n",
      "\n",
      "\n",
      "2021-05-24\n",
      "\n",
      "\n",
      "2021-05-25\n",
      "\n",
      "\n",
      "2021-05-26\n",
      "\n",
      "\n",
      "2021-05-27\n",
      "\n",
      "\n",
      "2021-05-28\n",
      "\n",
      "\n",
      "2021-05-29\n",
      "\n",
      "\n",
      "2021-05-30\n",
      "\n",
      "\n",
      "2021-05-31\n",
      "\n",
      "\n",
      "2021-06-01\n",
      "\n",
      "\n",
      "2021-06-02\n",
      "\n",
      "\n",
      "2021-06-03\n",
      "\n",
      "\n",
      "2021-06-04\n",
      "\n",
      "\n",
      "2021-06-05\n",
      "\n",
      "\n",
      "2021-06-06\n",
      "\n",
      "\n",
      "2021-06-07\n",
      "\n",
      "\n",
      "2021-06-08\n",
      "\n",
      "\n",
      "2021-06-09\n",
      "\n",
      "\n",
      "2021-06-10\n",
      "\n",
      "\n",
      "2021-06-11\n",
      "\n",
      "\n",
      "2021-06-12\n",
      "\n",
      "\n",
      "2021-06-13\n",
      "\n",
      "\n",
      "2021-06-14\n",
      "\n",
      "\n",
      "2021-06-15\n",
      "\n",
      "\n",
      "2021-06-16\n",
      "\n",
      "\n",
      "2021-06-17\n",
      "\n",
      "\n",
      "2021-06-18\n",
      "\n",
      "\n",
      "2021-06-19\n",
      "\n",
      "\n",
      "2021-06-20\n",
      "\n",
      "\n",
      "2021-06-21\n",
      "\n",
      "\n",
      "2021-06-22\n",
      "\n",
      "\n",
      "2021-06-23\n",
      "\n",
      "\n",
      "2021-06-24\n",
      "\n",
      "\n",
      "2021-06-25\n",
      "\n",
      "\n",
      "2021-06-26\n",
      "\n",
      "\n",
      "2021-06-27\n",
      "\n",
      "\n",
      "2021-06-28\n",
      "\n",
      "\n",
      "2021-06-29\n",
      "\n",
      "\n",
      "2021-06-30\n",
      "\n",
      "\n",
      "2021-07-01\n",
      "\n",
      "\n",
      "2021-07-02\n",
      "\n",
      "\n",
      "2021-07-03\n",
      "\n",
      "\n",
      "2021-07-04\n",
      "\n",
      "\n",
      "2021-07-05\n",
      "\n",
      "\n",
      "2021-07-06\n",
      "\n",
      "\n",
      "2021-07-07\n",
      "\n",
      "\n",
      "2021-07-08\n",
      "\n",
      "\n",
      "2021-07-09\n",
      "\n",
      "\n",
      "2021-07-10\n",
      "\n",
      "\n",
      "2021-07-11\n",
      "\n",
      "\n",
      "2021-07-12\n",
      "\n",
      "\n",
      "2021-07-13\n",
      "\n",
      "\n",
      "2021-07-14\n",
      "\n",
      "\n",
      "2021-07-15\n",
      "\n",
      "\n",
      "2021-07-16\n",
      "\n",
      "\n",
      "2021-07-17\n",
      "\n",
      "\n",
      "2021-07-18\n",
      "\n",
      "\n",
      "2021-07-19\n",
      "\n",
      "\n",
      "2021-07-20\n",
      "\n",
      "\n",
      "2021-07-21\n",
      "\n",
      "\n",
      "2021-07-22\n",
      "\n",
      "\n",
      "2021-07-23\n",
      "\n",
      "\n",
      "2021-07-24\n",
      "\n",
      "\n",
      "2021-07-25\n",
      "\n",
      "\n",
      "2021-07-26\n",
      "\n",
      "\n",
      "2021-07-27\n",
      "\n",
      "\n",
      "2021-07-28\n",
      "\n",
      "\n",
      "2021-07-29\n",
      "\n",
      "\n",
      "2021-07-30\n",
      "\n",
      "\n",
      "2021-07-31\n",
      "\n",
      "\n",
      "2021-08-01\n",
      "\n",
      "\n",
      "2021-08-02\n",
      "\n",
      "\n",
      "2021-08-03\n",
      "\n",
      "\n",
      "2021-08-04\n",
      "\n",
      "\n",
      "2021-08-05\n",
      "\n",
      "\n",
      "2021-08-06\n",
      "\n",
      "\n",
      "2021-08-07\n",
      "\n",
      "\n",
      "2021-08-08\n",
      "\n",
      "\n",
      "2021-08-09\n",
      "\n",
      "\n",
      "2021-08-10\n",
      "\n",
      "\n",
      "2021-08-11\n",
      "\n",
      "\n",
      "2021-08-12\n",
      "\n",
      "\n",
      "2021-08-13\n",
      "\n",
      "\n",
      "2021-08-14\n",
      "\n",
      "\n",
      "2021-08-15\n",
      "\n",
      "\n",
      "2021-08-16\n",
      "\n",
      "\n",
      "2021-08-17\n",
      "\n",
      "\n",
      "2021-08-18\n",
      "\n",
      "\n",
      "2021-08-19\n",
      "\n",
      "\n",
      "2021-08-20\n",
      "\n",
      "\n",
      "2021-08-21\n",
      "\n",
      "\n",
      "2021-08-22\n",
      "\n",
      "\n",
      "2021-08-23\n",
      "\n",
      "\n",
      "2021-08-24\n",
      "\n",
      "\n",
      "2021-08-25\n",
      "\n",
      "\n",
      "2021-08-26\n",
      "\n",
      "\n",
      "2021-08-27\n",
      "\n",
      "\n",
      "2021-08-28\n",
      "\n",
      "\n",
      "2021-08-29\n",
      "\n",
      "\n",
      "2021-08-30\n",
      "\n",
      "\n",
      "2021-08-31\n",
      "\n",
      "\n",
      "2021-09-01\n",
      "\n",
      "\n",
      "2021-09-02\n",
      "\n",
      "\n",
      "2021-09-03\n",
      "\n",
      "\n",
      "2021-09-04\n",
      "\n",
      "\n",
      "2021-09-05\n",
      "\n",
      "\n",
      "2021-09-06\n",
      "\n",
      "\n",
      "2021-09-07\n",
      "\n",
      "\n",
      "2021-09-08\n",
      "\n",
      "\n",
      "2021-09-09\n",
      "\n",
      "\n",
      "2021-09-10\n",
      "\n",
      "\n",
      "2021-09-11\n",
      "\n",
      "\n",
      "2021-09-12\n",
      "\n",
      "\n",
      "2021-09-13\n",
      "\n",
      "\n",
      "2021-09-14\n",
      "\n",
      "\n",
      "2021-09-15\n",
      "\n",
      "\n",
      "2021-09-16\n",
      "\n",
      "\n",
      "error due to out of range\n",
      "2021-09-17\n",
      "\n",
      "\n",
      "2021-09-18\n",
      "\n",
      "\n",
      "2021-09-19\n",
      "\n",
      "\n",
      "2021-09-20\n",
      "\n",
      "\n",
      "2021-09-21\n",
      "\n",
      "\n",
      "2021-09-22\n",
      "\n",
      "\n",
      "2021-09-23\n",
      "\n",
      "\n",
      "2021-09-24\n",
      "\n",
      "\n",
      "2021-09-25\n",
      "\n",
      "\n",
      "2021-09-26\n",
      "\n",
      "\n",
      "2021-09-27\n",
      "\n",
      "\n",
      "2021-09-28\n",
      "\n",
      "\n",
      "2021-09-29\n",
      "\n",
      "\n",
      "2021-09-30\n",
      "\n",
      "\n",
      "2021-10-01\n",
      "\n",
      "\n",
      "2021-10-02\n",
      "\n",
      "\n",
      "2021-10-03\n",
      "\n",
      "\n",
      "2021-10-04\n",
      "\n",
      "\n",
      "2021-10-05\n",
      "\n",
      "\n",
      "2021-10-06\n",
      "\n",
      "\n",
      "2021-10-07\n",
      "\n",
      "\n",
      "2021-10-08\n",
      "\n",
      "\n",
      "2021-10-09\n",
      "\n",
      "\n",
      "2021-10-10\n",
      "\n",
      "\n",
      "2021-10-11\n",
      "\n",
      "\n",
      "2021-10-12\n",
      "\n",
      "\n",
      "2021-10-13\n",
      "\n",
      "\n",
      "2021-10-14\n",
      "\n",
      "\n",
      "2021-10-15\n",
      "\n",
      "\n",
      "2021-10-16\n",
      "\n",
      "\n",
      "2021-10-17\n",
      "\n",
      "\n",
      "2021-10-18\n",
      "\n",
      "\n",
      "2021-10-19\n",
      "\n",
      "\n",
      "2021-10-20\n",
      "\n",
      "\n",
      "2021-10-21\n",
      "\n",
      "\n",
      "2021-10-22\n",
      "\n",
      "\n",
      "2021-10-23\n",
      "\n",
      "\n",
      "2021-10-24\n",
      "\n",
      "\n",
      "2021-10-25\n",
      "\n",
      "\n",
      "2021-10-26\n",
      "\n",
      "\n",
      "2021-10-27\n",
      "\n",
      "\n",
      "2021-10-28\n",
      "\n",
      "\n",
      "2021-10-29\n",
      "\n",
      "\n",
      "2021-10-30\n",
      "\n",
      "\n",
      "2021-10-31\n",
      "\n",
      "\n",
      "2021-11-01\n",
      "\n",
      "\n",
      "2021-11-02\n",
      "\n",
      "\n",
      "2021-11-03\n",
      "\n",
      "\n",
      "2021-11-04\n",
      "\n",
      "\n",
      "2021-11-05\n",
      "\n",
      "\n",
      "2021-11-06\n",
      "\n",
      "\n",
      "2021-11-07\n",
      "\n",
      "\n",
      "2021-11-08\n",
      "\n",
      "\n",
      "2021-11-09\n",
      "\n",
      "\n",
      "2021-11-10\n",
      "\n",
      "\n",
      "2021-11-11\n",
      "\n",
      "\n",
      "2021-11-12\n",
      "\n",
      "\n",
      "2021-11-13\n",
      "\n",
      "\n",
      "2021-11-14\n",
      "\n",
      "\n",
      "2021-11-15\n",
      "\n",
      "\n",
      "2021-11-16\n",
      "\n",
      "\n",
      "2021-11-17\n",
      "\n",
      "\n",
      "2021-11-18\n",
      "\n",
      "\n",
      "2021-11-19\n",
      "\n",
      "\n",
      "2021-11-20\n",
      "\n",
      "\n",
      "2021-11-21\n",
      "\n",
      "\n",
      "2021-11-22\n",
      "\n",
      "\n",
      "2021-11-23\n",
      "\n",
      "\n",
      "2021-11-24\n",
      "\n",
      "\n",
      "2021-11-25\n",
      "\n",
      "\n",
      "2021-11-26\n",
      "\n",
      "\n",
      "2021-11-27\n",
      "\n",
      "\n",
      "2021-11-28\n",
      "\n",
      "\n",
      "2021-11-29\n",
      "\n",
      "\n",
      "2021-11-30\n",
      "\n",
      "\n",
      "2021-12-01\n",
      "\n",
      "\n",
      "2021-12-02\n",
      "\n",
      "\n",
      "2021-12-03\n",
      "\n",
      "\n",
      "2021-12-04\n",
      "\n",
      "\n",
      "2021-12-05\n",
      "\n",
      "\n",
      "2021-12-06\n",
      "\n",
      "\n",
      "2021-12-07\n",
      "\n",
      "\n",
      "2021-12-08\n",
      "\n",
      "\n",
      "2021-12-09\n",
      "\n",
      "\n",
      "2021-12-10\n",
      "\n",
      "\n",
      "2021-12-11\n",
      "\n",
      "\n",
      "2021-12-12\n",
      "\n",
      "\n",
      "2021-12-13\n",
      "\n",
      "\n",
      "2021-12-14\n",
      "\n",
      "\n",
      "2021-12-15\n",
      "\n",
      "\n",
      "2021-12-16\n",
      "\n",
      "\n",
      "2021-12-17\n",
      "\n",
      "\n",
      "2021-12-18\n",
      "\n",
      "\n",
      "2021-12-19\n",
      "\n",
      "\n",
      "2021-12-20\n",
      "\n",
      "\n",
      "2021-12-21\n",
      "\n",
      "\n",
      "2021-12-22\n",
      "\n",
      "\n",
      "2021-12-23\n",
      "\n",
      "\n",
      "2021-12-24\n",
      "\n",
      "\n",
      "2021-12-25\n",
      "\n",
      "\n",
      "2021-12-26\n",
      "\n",
      "\n",
      "2021-12-27\n",
      "\n",
      "\n",
      "2021-12-28\n",
      "\n",
      "\n",
      "2021-12-29\n",
      "\n",
      "\n",
      "2021-12-30\n",
      "\n",
      "\n",
      "2021-12-31\n",
      "\n",
      "\n",
      "2022-01-01\n",
      "\n",
      "\n",
      "2022-01-02\n",
      "\n",
      "\n",
      "2022-01-03\n",
      "\n",
      "\n",
      "2022-01-04\n",
      "\n",
      "\n",
      "2022-01-05\n",
      "\n",
      "\n",
      "2022-01-06\n",
      "\n",
      "\n",
      "2022-01-07\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OBTAIN CMC COVARIATES AT DAILY LEVEL FOR ALL TOKENS\n",
    "# NOTE: THIS TAKES 40K CREDITS AND ABOUT 60 MINUTES!\n",
    "\n",
    "# Form list of strings of all dates in study period\n",
    "dates = list(pd.date_range('2015-01-01', '2022-01-07', freq='D').strftime('%Y-%m-%d'))\n",
    "             \n",
    "# Initialize dictionary for the data\n",
    "cmc_covars_dict = {'date': [],\n",
    "                   'cmc_id': [],\n",
    "                   'num_market_pairs': [],\n",
    "                   'max_supply': [],\n",
    "                   'circulating_supply': [],\n",
    "                   'total_supply': [],\n",
    "                   'cmc_rank': [],\n",
    "                   'tags': []}\n",
    "\n",
    "for date in dates: \n",
    "    # Update where we are\n",
    "    print(date)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Set up the call\n",
    "    endpoint = '/v1/cryptocurrency/listings/historical'\n",
    "    final_url = base_url+endpoint\n",
    "    parameters = {'date': date,\n",
    "                  'limit': 5000,\n",
    "                  'convert': 'USD',\n",
    "                  'aux': 'tags,circulating_supply,total_supply,max_supply,cmc_rank,num_market_pairs'}\n",
    "\n",
    "    # Make the call\n",
    "    nb_tries = 3\n",
    "    while True:\n",
    "        nb_tries -= 1\n",
    "        try:\n",
    "            response = session.get(final_url, params=parameters)\n",
    "            r_json = json.loads(response.text)\n",
    "            if (r_json['status']['error_message'] == None):\n",
    "                break\n",
    "            elif (r_json['status']['error_message'][:29] == 'Search query is out of range.'):\n",
    "                print('error due to out of range')\n",
    "                time.sleep(1)\n",
    "                if nb_tries <= 0:\n",
    "                    assert(1==0),'out of range error occured several times'\n",
    "            else:\n",
    "                assert(1==0),'json has error'\n",
    "\n",
    "        except (ConnectionError, Timeout, TooManyRedirects) as err:\n",
    "            if nb_tries <= 0:\n",
    "                raise err\n",
    "            else:\n",
    "                print('error due to connection, timeout, or redirect')\n",
    "                time.sleep(1)\n",
    "\n",
    "    # Add the data for that day to the dictionary\n",
    "    for token in r_json['data']:\n",
    "        cmc_covars_dict['date'].append(date)\n",
    "        cmc_covars_dict['cmc_id'].append(token['id'])\n",
    "        cmc_covars_dict['num_market_pairs'].append(token['num_market_pairs'])\n",
    "        cmc_covars_dict['max_supply'].append(token['max_supply'])\n",
    "        cmc_covars_dict['circulating_supply'].append(token['circulating_supply'])\n",
    "        cmc_covars_dict['total_supply'].append(token['total_supply'])\n",
    "        cmc_covars_dict['cmc_rank'].append(token['cmc_rank'])\n",
    "        cmc_covars_dict['tags'].append(token['tags'])\n",
    "\n",
    "    # Delay next call to not break limits\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "133712f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINE RELEVANT EXCHANGES TO PULL HISTORICAL DATA ON\n",
    "\n",
    "# Set up the call\n",
    "endpoint = '/v1/exchange/map'\n",
    "final_url = base_url+endpoint\n",
    "parameters = {'listing_status': 'active',\n",
    "              'limit': 500,\n",
    "              'aux': 'first_historical_data'}\n",
    "\n",
    "# Make the call\n",
    "response = session.get(final_url, params=parameters)\n",
    "r_json = json.loads(response.text)\n",
    "\n",
    "# Clean it up\n",
    "exchange_df = pd.concat([pd.DataFrame(exchange, index=[0]) for exchange in r_json['data']])\n",
    "exchange_df = exchange_df.reset_index(drop=True)\n",
    "exchange_df = exchange_df.rename(columns = {'id': 'exchange_id',\n",
    "                                            'slug': 'exchange_slug'})\n",
    "exchange_df = exchange_df[['exchange_id', 'exchange_slug']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "5cfac098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTAIN METADATA\n",
    "\n",
    "# Set up the call\n",
    "exchange_ids = ','.join([str(ex_id)for ex_id in exchange_df.exchange_id.values])\n",
    "endpoint = '/v1/exchange/info'\n",
    "final_url = base_url+endpoint\n",
    "parameters = {'id': exchange_ids,\n",
    "              'aux': 'date_launched'}\n",
    "\n",
    "# Make the call\n",
    "response = session.get(final_url, params=parameters)\n",
    "r_json = json.loads(response.text)\n",
    "\n",
    "# Add date launched to the data frame\n",
    "for key in r_json['data'].keys():\n",
    "    exchange_df.loc[exchange_df.exchange_id == int(key), \n",
    "                    'date_launched'] = r_json['data'][key]['date_launched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "991459d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping exchanges that do not have historical data\n",
    "exchange_names_to_drop = ['feg-exchange', 'uniswap-v3-arbitrum', 'huckleberry', \n",
    "                          'photonswap-finance', 'maiar-exchange', 'katana', \n",
    "                          'kine-protocol-polygon', 'bit2me', 'balancer-v2-polygon',\n",
    "                          'balancer-v2-arbitrum', 'uniswap-v3-polygon', 'tinyman', \n",
    "                          'algebra', 'kine-protocol-bsc', 'btcex-exchange']\n",
    "exchange_df = exchange_df[~exchange_df.exchange_slug.isin(exchange_names_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "857fe205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poloniex\n",
      "bittrex\n",
      "kraken\n",
      "bleutrade\n",
      "bittylicious\n",
      "cex-io\n",
      "bitfinex\n",
      "hitbtc\n",
      "exmo\n",
      "okcoin\n",
      "indodax\n",
      "bitstamp\n",
      "itbit\n",
      "zaif\n",
      "therocktrading\n",
      "coinmate\n",
      "zonda\n",
      "coinbase-exchange\n",
      "bitex-la\n",
      "bitonic\n",
      "yobit\n",
      "huobi-global\n",
      "litebit\n",
      "coincheck\n",
      "liquid\n",
      "southxchange\n",
      "bitso\n",
      "btcbox\n",
      "coincorner\n",
      "bitflyer\n",
      "isx\n",
      "gemini\n",
      "dex-trade\n",
      "exrates\n",
      "bitmex\n",
      "independent-reserve\n",
      "luno\n",
      "coinone\n",
      "bisq\n",
      "korbit\n",
      "bithumb\n",
      "lykke-exchange\n",
      "kuna\n",
      "mercatox\n",
      "p2pb2b\n",
      "tidex\n",
      "heat-wallet\n",
      "freiexchange\n",
      "btc-markets\n",
      "paribu\n",
      "btc-alpha\n",
      "coingi\n",
      "ripplefox\n",
      "gatehub\n",
      "coss\n",
      "btcturk-pro\n",
      "stex\n",
      "waves-exchange\n",
      "koinim\n",
      "stellar-decentralized-exchange\n",
      "buda\n",
      "btc-trade-ua\n",
      "localtrade\n",
      "bitbank\n",
      "mercado-bitcoin\n",
      "altcoin-trader\n",
      "bancor-network\n",
      "binance\n",
      "bits-blockchain\n",
      "tidebit\n",
      "cryptomarket\n",
      "okx\n",
      "gate-io\n",
      "idex\n",
      "kucoin\n",
      "bitcointrade\n",
      "topbtc\n",
      "aex\n",
      "coinfalcon\n",
      "coinut\n",
      "satang-pro\n",
      "zb-com\n",
      "bigone\n",
      "lbank\n",
      "gopax\n",
      "bibox\n",
      "coinbene\n",
      "coinex\n",
      "upbit\n",
      "tradeogre\n",
      "c-patex\n",
      "crxzone\n",
      "fatbtc\n",
      "paymium\n",
      "ddex\n",
      "rudex\n",
      "zebpay\n",
      "bitbns\n",
      "unocoin\n",
      "latoken\n",
      "crex24\n",
      "bithesap\n",
      "cryptonex\n",
      "cointiger\n",
      "b2bx\n",
      "dragonex\n",
      "hotbit\n",
      "switcheo\n",
      "bitforex\n",
      "kyber-network\n",
      "coindeal\n",
      "bitmart\n",
      "digifinex\n",
      "tokenomy\n",
      "bilaxy\n",
      "graviex\n",
      "idcm\n",
      "abcc\n",
      "hoo\n",
      "wazirx\n",
      "bitfront\n",
      "zbg\n",
      "bitrue\n",
      "yunex\n",
      "bitkub\n",
      "bgogo\n",
      "cryptology\n",
      "coinbit\n",
      "gdac\n",
      "tokok\n",
      "coineal\n",
      "ascendex\n",
      "polonidex\n",
      "bw-com\n",
      "coinsbit\n",
      "huobi-korea\n",
      "oceanex\n",
      "probit-exchange\n",
      "bhex-bluehelix\n",
      "dcoin\n",
      "catex\n",
      "alterdice\n",
      "bkex\n",
      "hanbitco\n",
      "coinfield\n",
      "binance-dex\n",
      "vindax\n",
      "coinmetro\n",
      "bithumb-global\n",
      "safetrade\n",
      "velic\n",
      "folgory\n",
      "exnomy\n",
      "bione\n",
      "polyx\n",
      "tokenize-xchange\n",
      "whitebit\n",
      "birake-network\n",
      "zg-com\n",
      "cbx\n",
      "chainx\n",
      "bitget\n",
      "finexbox\n",
      "dydx\n",
      "coinw\n",
      "triv-pro\n",
      "bitvavo\n",
      "bybit\n",
      "deribit\n",
      "beaxy\n",
      "ftx\n",
      "xt\n",
      "btc-exchange\n",
      "bitexbook\n",
      "exmarkets\n",
      "virtuse-exchange\n",
      "coinflex\n",
      "mxc\n",
      "citex\n",
      "loex\n",
      "globitex\n",
      "paybito\n",
      "txbit\n",
      "secondbtc\n",
      "cross-exchange\n",
      "wbf-exchange\n",
      "serenity\n",
      "basefex\n",
      "bitstorage\n",
      "indoex\n",
      "bitci\n",
      "bitpanda-pro\n",
      "bankcex\n",
      "bithumb-singapore\n",
      "50x\n",
      "bitvast\n",
      "novadax\n",
      "qtrade\n",
      "ecxx\n",
      "binance-us\n",
      "mandala\n",
      "bitsten\n",
      "paritex\n",
      "exir\n",
      "coinzo\n",
      "foblgate\n",
      "bitubu\n",
      "artis-turba\n",
      "bitexlive\n",
      "timex\n",
      "omgfin\n",
      "coinjar\n",
      "btse\n",
      "bitclude\n",
      "bqt\n",
      "trontrade\n",
      "currency-com\n",
      "newdex\n",
      "delta-exchange\n",
      "tokocrypto\n",
      "bitopro\n",
      "rekeningku-com\n",
      "emirex\n",
      "vcc-exchange\n",
      "dove-wallet\n",
      "ace\n",
      "vitex\n",
      "ataix\n",
      "pionex\n",
      "bitbuy\n",
      "digitalexchange-id\n",
      "xcalibra\n",
      "decoin\n",
      "nominex\n",
      "prizmbit\n",
      "nicehash\n",
      "aax\n",
      "tokenlon\n",
      "bitcoin-com-exchange\n",
      "stakecube\n",
      "tokencan\n",
      "max-exchange\n",
      "crypterum\n",
      "bitay\n",
      "etorox\n",
      "valr\n",
      "stormgain\n",
      "namebase\n",
      "biconomy\n",
      "hotcoin-global\n",
      "coindcx\n",
      "phemex\n",
      "farhadmarket\n",
      "resfinex\n",
      "loopring-exchange\n",
      "bitribe\n",
      "huobi-japan\n",
      "chiliz\n",
      "jubi-exchange\n",
      "giottus\n",
      "coinlist-pro\n",
      "felixo\n",
      "bitturk\n",
      "coinzoom\n",
      "flybit\n",
      "compound\n",
      "gokumarket\n",
      "1inch-exchange\n",
      "deversifi\n",
      "balancer\n",
      "curve-finance\n",
      "bingx\n",
      "coincasso\n",
      "uniswap-v2\n",
      "nash-exchange\n",
      "aprobit\n",
      "crosstower\n",
      "kwenta\n",
      "zipmex\n",
      "narkasa\n",
      "ripio\n",
      "polkaswap\n",
      "mooniswap\n",
      "nexus-mutual\n",
      "aave\n",
      "kickex\n",
      "cybex-dex\n",
      "tokensets\n",
      "justswap\n",
      "azbit\n",
      "bitazza\n",
      "changelly-pro\n",
      "eqonex\n",
      "bitcoiva\n",
      "xtheta-global\n",
      "mexo-exchange\n",
      "bitexen\n",
      "serum-dex\n",
      "sushiswap\n",
      "koinbazar\n",
      "crypto-com-exchange\n",
      "sinegy-marketplace\n",
      "paraswap\n",
      "bit-com\n",
      "hotbit-korea\n",
      "pancakeswap\n",
      "sashimiswap\n",
      "ftx-us\n",
      "acdx\n",
      "aryana\n",
      "deepcoin\n",
      "mov\n",
      "bitwell\n",
      "dodo\n",
      "honeyswap\n",
      "luaswap\n",
      "bvnex\n",
      "bepswap\n",
      "mento\n",
      "bit4you\n",
      "venus\n",
      "defichain-dex\n",
      "kulap\n",
      "demex\n",
      "linkswap\n",
      "blockchain-com-exchange\n",
      "block-dx\n",
      "terraswap\n",
      "openocean\n",
      "1inch-liquidity-protocol\n",
      "wanswap\n",
      "idex-bsc\n",
      "mdex\n",
      "apeswap\n",
      "sifchain\n",
      "wootrade\n",
      "quickswap\n",
      "lcx-exchange\n",
      "perpetual-protocol\n",
      "bakeryswap\n",
      "dfyn-network\n",
      "capital-dex\n",
      "bloctoswap\n",
      "kwikswap\n",
      "put-on-your-metamask\n",
      "dodo-bsc\n",
      "swipeswap\n",
      "plasmaswap\n",
      "thorchain-erc20\n",
      "mdex-bsc\n",
      "kyberdmm\n",
      "unicly\n",
      "ubeswap\n",
      "pangolin\n",
      "raydium\n",
      "pancakeswap-v2\n",
      "polyzap\n",
      "comethswap\n",
      "uniswap-v3\n",
      "viperswap\n",
      "spiritswap\n",
      "globe-derivative-exchange\n",
      "waultswap\n",
      "binance-tr\n",
      "bunicorn\n",
      "lydia-finance\n",
      "convergence\n",
      "babyswap\n",
      "coinswap-space\n",
      "thorchain-bep20\n",
      "thorchain-btc\n",
      "balancer-v2\n",
      "probit-korea\n",
      "sovryn\n",
      "shibaswap\n",
      "nowswap\n",
      "unifi-protocol-dao\n",
      "sushiswap-bsc\n",
      "kine-protocol\n",
      "mars-ecosystem\n",
      "sushiswap-fantom\n",
      "sushiswap-polygon\n",
      "sushiswap-xdai\n",
      "kuswap\n",
      "orca\n",
      "balanced\n",
      "kswap-finance\n",
      "pandaswap\n",
      "cherryswap\n",
      "waultswap-polygon\n",
      "kalata\n",
      "polydex\n",
      "biswap\n",
      "osmosis\n",
      "apollox\n",
      "yetiswap\n",
      "kava-swap\n",
      "spookyswap\n",
      "traderjoe\n",
      "jetswap\n",
      "clipper\n",
      "jetswap-polygon\n",
      "sushiswap-arbitrum\n",
      "freeriver\n",
      "apeswap-finance-polygon\n",
      "moonswap-moonriver\n",
      "dodo-polygon\n",
      "partyswap\n",
      "sushiswap-harmony\n",
      "kyberdmm-polygon\n",
      "kyberdmm-bsc\n",
      "kyberdmm-avalanche\n",
      "autoshark-finance\n",
      "dodo-arbitrum\n",
      "mcdex\n",
      "kaidex\n",
      "aldrin\n",
      "planet-finance\n",
      "plasmaswap-bsc\n",
      "plasmaswap-polygon\n",
      "zilswap\n",
      "siennaswap\n",
      "dinosaur-eggs\n",
      "deri-protocol\n",
      "bitcoke\n",
      "ref-finance\n",
      "paintswap\n",
      "elk-finance\n",
      "wagyuswap\n",
      "sphynx-swap\n",
      "mojitoswap\n",
      "dfx-finance\n",
      "elk-finance-polygon\n",
      "elk-finance-bsc\n",
      "crodex\n",
      "vvs-finance\n",
      "apollox-dex\n",
      "sushiswap-celo\n",
      "beethovenx\n",
      "oolongswap\n",
      "cronaswap\n"
     ]
    }
   ],
   "source": [
    "# OBTAIN EXCHANGE HISTORICAL DATA\n",
    "\n",
    "ex_hist_data_dict = {'exchange_id': [],\n",
    "                     'date': [],\n",
    "                     'exchange_volume_24h': [],\n",
    "                     'num_market_pairs': []}\n",
    "\n",
    "# Loop over all exchanges\n",
    "for exchange_id in exchange_df.exchange_id.values: \n",
    "    print(exchange_df[exchange_df.exchange_id == exchange_id]['exchange_slug'].values[0])\n",
    "\n",
    "    # Set up the call\n",
    "    endpoint = '/v1/exchange/quotes/historical'\n",
    "    final_url = base_url+endpoint\n",
    "    parameters = {'id': exchange_id,\n",
    "                  'time_start': '2015-01-01',\n",
    "                  'time_end': '2021-12-31',\n",
    "                  'interval': '1d',\n",
    "                  'count': 10000,\n",
    "                  'convert': 'USD'}\n",
    "\n",
    "    # Make the call\n",
    "    response = session.get(final_url, params=parameters)\n",
    "    r_json = json.loads(response.text)\n",
    "\n",
    "    # Add the data to the dictionary\n",
    "    for ex_data in r_json['data']['quotes']:\n",
    "        ex_hist_data_dict['exchange_id'].append(exchange_id)\n",
    "        ex_hist_data_dict['date'].append(ex_data['quote']['USD']['timestamp'])\n",
    "        ex_hist_data_dict['exchange_volume_24h'].append(ex_data['quote']['USD']['volume_24h'])\n",
    "        ex_hist_data_dict['num_market_pairs'].append(ex_data['num_market_pairs'])\n",
    "        \n",
    "    # Sleep\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "e3703240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "ex_historical_df = pd.DataFrame(ex_hist_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b939d",
   "metadata": {},
   "source": [
    "## (4) Save all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "84c307d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cmc token covars panel\n",
    "cmc_covars_df.to_csv('../3-data/raw/cmc_token_covars_panel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "85ae5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cmc macro timeseries data\n",
    "macro_df.to_csv('../3-data/raw/cmc_macro_timeseries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "3d1139f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cmc exchange covariates\n",
    "exchange_df.to_csv('../3-data/raw/cmc_exchange_covar.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "fd72cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cmc exchange panel data\n",
    "ex_historical_df.to_csv('../3-data/raw/cmc_exchange_panel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVE THESE NOTES TO CLEANING\n",
    "\n",
    "# manually look through it to confirm they are legit tokens\n",
    "# or maybe give this task to jacob\n",
    "# or maybe schedule a time to do this with jacob so we 2x the speed\n",
    "\n",
    "# Lets look to see if the 0.01% mcap rule is good for the entire time period\n",
    "\n",
    "# Jan 1 2015 - $5B - $500k\n",
    "# Jan 1 2016 - $7B - $700k\n",
    "# Jan 1 2017 - $18B - $1.8M\n",
    "# Jan 1 2018 - $600B - $60M\n",
    "# Apr 1 2018 - $300B - $30M\n",
    "# Jul 1 2018 - $250B - $25M\n",
    "# Jan 1 2019 - $125B - $12M\n",
    "# Apr 1 2019 - $145B - $14M\n",
    "# Jul 1 2019 - $330B - $33M\n",
    "# Oct 1 2019 - $220B - $22M\n",
    "# Jan 1 2020 - $200B - $20M\n",
    "# Apr 1 2020 - $175B - $17M\n",
    "# Jul 1 2020 - $260B - $26M\n",
    "# Oct 1 2020 - $340B - $34M\n",
    "# Jan 1 2021 - $770B - $77M\n",
    "# Apr 1 2021 - $1.9T - $190M\n",
    "# Jul 1 2021 - $1.4T - $140M\n",
    "# Oct 1 2021 - $2T - $200M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
