{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab026fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# IMPORT PACKAGES\n",
    "from typing import List, Tuple\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from tools import QuantTools\n",
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f87f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetAndFormRhs(df: pd.DataFrame, lhs_col: str) -> pd.DataFrame:\n",
    "    # Define char columns of interest\n",
    "    char_cols = ['char_tx_volume_tm7',\n",
    "        'char_addr_active_tm7',\n",
    "        'char_addr_new_log_delta_tm14_tm7',\n",
    "        'char_addr_new_tm7',\n",
    "        'char_addr_total_t',\n",
    "        'char_circulation_tm7',\n",
    "        'char_age_destroyed_tm7',\n",
    "        'char_delta_flow_dist_tm7',\n",
    "        'char_delta_holders_dist_tm7',\n",
    "        'char_prct_supply_in_profit_t',\n",
    "        'char_cex_prct_circ_supply_t',\n",
    "        'char_dex_prct_circ_supply_t',\n",
    "        'char_defi_prct_circ_supply_t',\n",
    "        'char_traders_prct_circ_supply_t',\n",
    "        'char_exchange_inflow_tm7',\n",
    "        'char_exchange_outflow_tm7',\n",
    "        'char_num_pairs_t',\n",
    "        'char_social_volume_tm7',\n",
    "        'char_social_volume_reddit_tm7',\n",
    "        'char_social_volume_twitter_tm7',\n",
    "        'char_sent_pos_reddit_tm7',\n",
    "        'char_sent_pos_twitter_tm7',\n",
    "        'char_sent_neg_reddit_tm7',\n",
    "        'char_sent_neg_twitter_tm7',\n",
    "        'char_dev_activity_tm7',\n",
    "        'char_r_tm7',\n",
    "        'char_r_tm14',\n",
    "        'char_r_tm30',\n",
    "        'char_r_tm60',\n",
    "        'char_r_tm90',\n",
    "        'char_r_tm14_tm7',\n",
    "        'char_r_tm30_tm14',\n",
    "        'char_r_tm90_tm30',\n",
    "        'char_r_ath_t',\n",
    "        'char_r_atl_t',\n",
    "        'char_r_industry_tm30',\n",
    "        'char_r_industry_tm60',\n",
    "        'char_trades_sum_tm7',\n",
    "        'char_volume_sum_tm7',\n",
    "        'char_spread_bps_t',\n",
    "        'char_ask_size_t',\n",
    "        'char_bid_size_t',\n",
    "        'char_illiq_tm7',\n",
    "        'char_turnover_tm7',\n",
    "        'char_price_t',\n",
    "        'char_size_t',\n",
    "        'char_mvrv_t',\n",
    "        'char_alpha_tm7',\n",
    "        'char_alpha_tm30',\n",
    "        'char_beta_tm7',\n",
    "        'char_beta_tm30',\n",
    "        'char_beta_downside_tm30',\n",
    "        'char_coskew_tm30',\n",
    "        'char_iskew_tm30',\n",
    "        'char_shortfall5_tm7',\n",
    "        'char_var5_tm7',\n",
    "        'char_vol_tm7',\n",
    "        'char_vol_tm30',\n",
    "        'char_vol_tm90',\n",
    "        'char_ivol_tm7',\n",
    "        'char_ivol_tm30',\n",
    "        'char_ivol_tm90']\n",
    "    \n",
    "    # Form new rhs of the contemporaneous return and mcap to use throughout\n",
    "    df = df.rename(columns={'char_mcap_t': 'mcap'})\n",
    "    \n",
    "    # Subset to relevant columns\n",
    "    df = df[['date', 'asset', lhs_col, 'mcap']+char_cols].copy()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa466a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFactorsDf(in_df: pd.DataFrame, lhs_col: str, num_qntls_fctrs: int) -> pd.DataFrame:\n",
    "    # Build list of rhs variables: char and factor names\n",
    "    char_cols = list(in_df.columns.values)\n",
    "    for col in ['date', 'asset', lhs_col, 'mcap']:\n",
    "        char_cols.remove(col)\n",
    "    factor_cols = ['factor_'+col[5:] for col in char_cols]\n",
    "\n",
    "    # Randomly sort all rows of the dataframe, \n",
    "    #     to handle chars that have repeated values within date\n",
    "    in_df = in_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Form factors\n",
    "    factors_df = pd.DataFrame(data={'date': []})\n",
    "    for char_col, factor_col in zip(char_cols, factor_cols):\n",
    "        # subset to relevant characteristic\n",
    "        temp_df = in_df[['date', lhs_col, 'mcap', char_col]].copy() \n",
    "\n",
    "        # form quantiles by char col\n",
    "        temp_df = temp_df.sort_values(by=['date', char_col])\n",
    "        temp_df['rank_within_date'] = temp_df.groupby('date')[char_col].rank(method='first')\n",
    "        temp_df['rank_ratio'] = temp_df.groupby('date')['rank_within_date'].transform(lambda x: x / x.max())\n",
    "        quantile_bins = list(np.arange(0, num_qntls_fctrs+1)/num_qntls_fctrs)\n",
    "        temp_df['quant'] = 1+pd.cut(temp_df['rank_ratio'], bins=quantile_bins, labels=False, include_lowest=True)\n",
    "        temp_df = temp_df.drop(columns=['rank_within_date', 'rank_ratio'])\n",
    "\n",
    "        # form mcap weighted average return within date-quantiles\n",
    "        temp_df['weighted_return'] = temp_df[lhs_col] * temp_df.mcap\n",
    "        grouped_df = temp_df.groupby(['date', 'quant'])[['weighted_return', 'mcap']].sum().reset_index()\n",
    "        grouped_df[lhs_col] = grouped_df['weighted_return'] / grouped_df['mcap']\n",
    "        avg_ret_by_quant_df = grouped_df[['date', 'quant', lhs_col]].copy()\n",
    "\n",
    "        # form top minus bottom mcap weighted t+1 return\n",
    "        pivot_df = avg_ret_by_quant_df.pivot(index='date', columns='quant', values=lhs_col)\n",
    "        pivot_df['diff'] = pivot_df[num_qntls_fctrs] - pivot_df[1]\n",
    "        factor_df = pivot_df.reset_index()[['date', 'diff']]\n",
    "        factor_df = factor_df.rename(columns={'diff': factor_col})\n",
    "\n",
    "        # combine results\n",
    "        factors_df = factors_df.merge(factor_df, on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    return factors_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1454b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formMcapWeightedReturn(in_df, new_col_name, lhs_col) -> pd.DataFrame:\n",
    "    in_df['weighted_return'] = in_df[lhs_col] * in_df.mcap\n",
    "    grouped_df = in_df.groupby(['date'])[['weighted_return', 'mcap']].sum().reset_index()\n",
    "    grouped_df['bin_return'] = grouped_df['weighted_return'] / grouped_df['mcap']\n",
    "    grouped_df = grouped_df.rename(columns={'bin_return': new_col_name})\n",
    "    return grouped_df[['date', new_col_name]].copy()\n",
    "\n",
    "def formFamaFactors(all_df: pd.DataFrame, df: pd.DataFrame, lhs_col: str) -> pd.DataFrame:\n",
    "    # Form cmkt factor\n",
    "    cmkt_df = all_df[['date', 'macro_cmkt_tm7']].copy().drop_duplicates()\n",
    "    cmkt_df = cmkt_df.rename(columns={'macro_cmkt_tm7': 'factor_cmkt_t'})\n",
    "\n",
    "    # Form size factor\n",
    "    factors_df = df[['date', 'asset', lhs_col, 'mcap', 'char_size_t']].copy()\n",
    "    size_df = buildFactorsDf(factors_df, lhs_col, 3)\n",
    "    size_df['factor_csmb_t'] = -1*size_df.factor_size_t # NOTE: flip it so it is now small minus big\n",
    "    size_df = size_df.drop('factor_size_t', axis=1)\n",
    "    \n",
    "    # Form cmom factor\n",
    "    temp_df = df[['date', 'asset', lhs_col, 'mcap', 'char_size_t', 'char_r_tm30']].copy()\n",
    "\n",
    "    # top or bottom size\n",
    "    temp_df = temp_df.sort_values(by=['date', 'char_size_t'])\n",
    "    temp_df['rank_within_date'] = temp_df.groupby('date')['char_size_t'].rank(method='first')\n",
    "    temp_df['rank_ratio'] = temp_df.groupby('date')['rank_within_date'].transform(lambda x: x / x.max())\n",
    "    temp_df['bitile_size'] = 1+pd.cut(temp_df['rank_ratio'], bins=2, labels=False, include_lowest=True)\n",
    "    temp_df = temp_df.drop(columns=['rank_within_date', 'rank_ratio'])\n",
    "\n",
    "    # mom tertiles WITHIN size\n",
    "    temp_df = temp_df.sort_values(by=['date', 'bitile_size', 'char_r_tm30'])\n",
    "    temp_df['rank_within_date_size'] = temp_df.groupby(['date', 'bitile_size'])['char_r_tm30'].rank(method='first')\n",
    "    temp_df['rank_ratio'] = temp_df.groupby(['date', 'bitile_size'])['rank_within_date_size'].transform(lambda x: x / x.max())\n",
    "    temp_df['tertile_mom'] = 1+pd.cut(temp_df['rank_ratio'], bins=3, labels=False, include_lowest=True)\n",
    "    temp_df = temp_df.drop(columns=['rank_within_date_size', 'rank_ratio'])\n",
    "\n",
    "    # extract relevant relevant bins and form their mcap weighted return by date\n",
    "    small_high_df = formMcapWeightedReturn(temp_df[(temp_df.bitile_size == 1) \n",
    "        & (temp_df.tertile_mom == 3)][['date', 'asset', 'mcap', lhs_col]].copy(), 'small_high_return', lhs_col)\n",
    "    big_high_df   = formMcapWeightedReturn(temp_df[(temp_df.bitile_size == 2) \n",
    "        & (temp_df.tertile_mom == 3)][['date', 'asset', 'mcap', lhs_col]].copy(), 'big_high_return', lhs_col)\n",
    "    small_low_df  = formMcapWeightedReturn(temp_df[(temp_df.bitile_size == 1) \n",
    "        & (temp_df.tertile_mom == 1)][['date', 'asset', 'mcap', lhs_col]].copy(), 'small_low_return', lhs_col)\n",
    "    big_low_df    = formMcapWeightedReturn(temp_df[(temp_df.bitile_size == 2) \n",
    "        & (temp_df.tertile_mom == 1)][['date', 'asset', 'mcap', lhs_col]].copy(), 'big_low_return', lhs_col)\n",
    "\n",
    "    # merge all bin returns together\n",
    "    cmom_df = small_high_df.merge(big_high_df, on='date', how='inner', validate='one_to_one')\n",
    "    cmom_df = cmom_df.merge(small_low_df, on='date', how='inner', validate='one_to_one')\n",
    "    cmom_df = cmom_df.merge(big_low_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    # form cmom factor return \n",
    "    cmom_df['factor_cmom_t'] = (0.5*(cmom_df.small_high_return + cmom_df.big_high_return) \n",
    "        - 0.5*(cmom_df.small_low_return + cmom_df.big_low_return))\n",
    "    cmom_df = cmom_df[['date', 'factor_cmom_t']].copy()\n",
    "\n",
    "    # Merge together all the factors\n",
    "    fama_df = cmkt_df.merge(size_df, on='date', how='inner', validate='one_to_one')\n",
    "    fama_df = fama_df.merge(cmom_df, on='date', how='inner', validate='one_to_one')\n",
    "\n",
    "    return fama_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af36366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndPredict(lhs_df: pd.DataFrame, lhs_col: str, factors_df: pd.DataFrame, factors_combo: List[str], test_start_date_str: str) -> pd.DataFrame:\n",
    "    # Calc each asset's beta hats\n",
    "    asset_universe = list(np.unique(lhs_df[lhs_df.date<test_start_date_str].asset.values))\n",
    "    asset_beta_hats_dict = {key: None for key in asset_universe}\n",
    "    for asset in asset_universe:\n",
    "        # form relevant data\n",
    "        train_lhs_df = lhs_df[(lhs_df.asset==asset) & (lhs_df.date < test_start_date_str)][['date', lhs_col]].copy()\n",
    "        train_df = train_lhs_df.merge(factors_df, on='date', how='left', validate='one_to_one')\n",
    "        train_rhs = train_df[factors_combo]\n",
    "        train_lhs = train_df[lhs_col]\n",
    "        train_rhs = sm.add_constant(train_rhs)\n",
    "\n",
    "        # calc beta hats\n",
    "        model = sm.OLS(train_lhs, train_rhs)\n",
    "        results = model.fit()\n",
    "\n",
    "        # save beta hats\n",
    "        asset_beta_hats_dict[asset] = list(results.params.values)\n",
    "\n",
    "    # Calc avg beta hat in case we need it\n",
    "    avg_beta_hats = np.mean(list(asset_beta_hats_dict.values()), axis=0)\n",
    "\n",
    "    # Calculate the lambda hats: ts avg of factors up to previous period\n",
    "    lambdas_df = factors_df.copy()\n",
    "    lambdas_df.set_index('date', inplace=True)\n",
    "    lambdas_df = lambdas_df.shift(1).expanding().mean()\n",
    "    lambdas_df = lambdas_df.dropna()\n",
    "\n",
    "    # Calc test yhats\n",
    "    test_df = lhs_df[lhs_df.date>=test_start_date_str][['date', 'asset', lhs_col]].copy()\n",
    "    test_df = test_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "    test_df['yhats'] = np.zeros(len(test_df))\n",
    "    test_assets = list(np.unique(test_df.asset.values))\n",
    "    for asset in test_assets:\n",
    "        # if we did not have the test asset in training data, take average of beta hats\n",
    "        if asset not in asset_beta_hats_dict.keys():\n",
    "            asset_beta_hats = avg_beta_hats\n",
    "        else:\n",
    "            asset_beta_hats = np.array(asset_beta_hats_dict[asset])\n",
    "        \n",
    "        # form asset test rhs of factor time series avg up to previous time period\n",
    "        asset_test_dates = list(np.unique(test_df[test_df.asset==asset].date.values))\n",
    "        asset_test_rhs = lambdas_df[lambdas_df.index.isin(asset_test_dates)][factors_combo]\n",
    "        asset_test_rhs = sm.add_constant(asset_test_rhs)\n",
    "        \n",
    "        # calc asset yhats\n",
    "        asset_yhats = np.matmul(asset_test_rhs.values, asset_beta_hats)\n",
    "\n",
    "        # save values\n",
    "        test_df.loc[test_df.asset==asset, 'yhats'] = asset_yhats\n",
    "\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27b833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineFirstDayOfMonthsInTestPeriod(df: pd.DataFrame, test_start_date: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Determine the first day of each month within the test period starting from the given date.\n",
    "\n",
    "    :param df: DataFrame containing the date column for determining the test period range.\n",
    "    :param test_start_date: The test start date in the format '%Y-%m-%d'.\n",
    "\n",
    "    :return: A list of dates representing the first day of each month within the test period.\n",
    "    \"\"\"\n",
    "    test_start_datetime = datetime.strptime(test_start_date, '%Y-%m-%d')\n",
    "    assert test_start_datetime.day == 1, \"Test start date does not start on first day of a month.\"\n",
    "    \n",
    "    test_period_months = []\n",
    "    max_date = np.max(df.date)\n",
    "    current_date = test_start_datetime\n",
    "    while current_date <= max_date:\n",
    "        test_period_months.append(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date += pd.DateOffset(months=1)\n",
    "        \n",
    "    return test_period_months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d61fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvFactorCombos(df: pd.DataFrame, asset_universe_dict: dict, lhs_col: str, test_start_date: str, \n",
    "    num_qntls_fctrs: int, num_cpus: int, num_factors: int) -> Tuple[list, float]:\n",
    "    \"\"\"\n",
    "    Compute optimal combination of characteristics with the maximum predicted r-squared value.\n",
    "\n",
    "    :param df: Input DataFrame with necessary columns.\n",
    "    :param asset_universe_dict: Dictionary of keys of study period months and values of lists of assets.\n",
    "    :param lhs_col: Column name in df for left-hand side variable.\n",
    "    :param test_start_date: The test start date in the format '%Y-%m-%d'.\n",
    "    :param num_qntls_fctrs: Number of quantiles for factors.\n",
    "    :param num_cpus: Number of CPU cores to use for parallel processing.\n",
    "    :param num_factors: Number of factors for combinations.\n",
    "\n",
    "    :return: A tuple containing a list of optimal characteristic combinations and the associated max r-squared value.\n",
    "    \"\"\"\n",
    "    # Build list of all the char cols\n",
    "    char_cols = [col for col in df.columns if col not in ['date', 'asset', lhs_col, 'mcap']]\n",
    "\n",
    "    # Form all combinations of factors\n",
    "    char_combos = list(combinations(char_cols, num_factors))\n",
    "\n",
    "    # Determine the first day of the month for all test period months\n",
    "    test_period_months = determineFirstDayOfMonthsInTestPeriod(df, test_start_date)\n",
    "\n",
    "    # Parallel loop over all combinations of characteristics to find combo with opt r2_pred\n",
    "    def findOptCharCombo(char_combo):\n",
    "        # convert char_combo to a list and form factor names\n",
    "        char_combo = list(char_combo)\n",
    "        factors_combo = ['factor_'+col[5:] for col in char_combo]\n",
    "\n",
    "        # iterate over all months in the test period to use the appropriate assets\n",
    "        test_dfs = []\n",
    "        for test_period_month in test_period_months:\n",
    "            # form the relevant dataset\n",
    "            asset_universe = asset_universe_dict[test_period_month]\n",
    "            one_month_ahead = datetime.strptime(test_period_month, '%Y-%m-%d') + pd.DateOffset(months=1)\n",
    "            rel_df = df[(df.asset.isin(asset_universe)) \n",
    "                & (df.date < one_month_ahead)][\n",
    "                ['date', 'asset', lhs_col, 'mcap']+char_combo].copy()\n",
    "\n",
    "            # form factors\n",
    "            factors_df = buildFactorsDf(rel_df, lhs_col, num_qntls_fctrs)\n",
    "\n",
    "            # fit and predict\n",
    "            lhs_df = rel_df[['date', 'asset', lhs_col]].copy()\n",
    "            test_df = fitAndPredict(lhs_df, lhs_col, factors_df, factors_combo, test_period_month)\n",
    "\n",
    "            # save results across the test months\n",
    "            test_dfs.append(test_df)\n",
    "        \n",
    "        # aggregate results across test period for this combo\n",
    "        result_df = pd.concat(test_dfs)\n",
    "\n",
    "        # calc r2 pred \n",
    "        ys = result_df[lhs_col].values\n",
    "        yhats = result_df['yhats'].values\n",
    "        r2_pred = 1-np.mean(np.square(ys-yhats))/np.mean(np.square(ys))\n",
    "\n",
    "        # return results\n",
    "        return r2_pred\n",
    "\n",
    "    # Run loop in parallel\n",
    "    r2_pred_list = Parallel(n_jobs=num_cpus)(delayed(findOptCharCombo)(char_combo) for char_combo in tqdm(char_combos))\n",
    "\n",
    "    # Determine optimal r2 pred and return it with associated combination\n",
    "    max_index = np.argmax(np.array(r2_pred_list))\n",
    "    return list(char_combos[max_index]), r2_pred_list[max_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1738e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitTestPeriod(df: pd.DataFrame, asset_universe_dict: dict, lhs_col: str, test_start_date: str, \n",
    "    num_qntls_fctrs: int, num_factors: int, opt_chars_combos_list: List[list]) -> None:\n",
    "    # Determine the first day of the month for all test period months\n",
    "    test_period_months = determineFirstDayOfMonthsInTestPeriod(df, test_start_date)\n",
    "\n",
    "    # Generate yhats for each model\n",
    "    test_df = df[df.date >= test_start_date][['date', 'asset', lhs_col, 'mcap']].reset_index(drop=True).copy()\n",
    "    for i, opt_chars_combo in enumerate(opt_chars_combos_list):\n",
    "        # form factor names\n",
    "        factors_combo = ['factor_'+col[5:] for col in opt_chars_combo]\n",
    "        num_factors = i+1\n",
    "\n",
    "        # iterate over all the test period months to gen yhats\n",
    "        temp_dfs = []\n",
    "        for test_period_month in test_period_months:\n",
    "            # form the relevant dataset\n",
    "            asset_universe = asset_universe_dict[test_period_month]\n",
    "            one_month_ahead = datetime.strptime(test_period_month, '%Y-%m-%d') + pd.DateOffset(months=1)\n",
    "            rel_df = df[(df.asset.isin(asset_universe)) \n",
    "                & (df.date < one_month_ahead)][\n",
    "                ['date', 'asset', lhs_col, 'mcap']+opt_chars_combo].copy()\n",
    "\n",
    "            # form factors\n",
    "            factors_df = buildFactorsDf(rel_df, lhs_col, num_qntls_fctrs)\n",
    "\n",
    "            # fit and predict\n",
    "            lhs_df = rel_df[['date', 'asset', lhs_col]].copy()\n",
    "            temp_df = fitAndPredict(lhs_df, lhs_col, factors_df, factors_combo, test_period_month)\n",
    "\n",
    "            # save results across the test months\n",
    "            temp_dfs.append(temp_df)\n",
    "\n",
    "        # aggregate results across the test period for this combo\n",
    "        temp_df = pd.concat(temp_dfs)\n",
    "        temp_df = temp_df.drop(lhs_col, axis=1)\n",
    "        temp_df = temp_df.rename(columns={'yhats': 'yhats_'+str(num_factors)+'_factors'})\n",
    "\n",
    "        # merge results for this combo onto the main df\n",
    "        test_df = test_df.merge(temp_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4d87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitFamaTestPeriod(df: pd.DataFrame, fama_df: pd.DataFrame,\n",
    "    asset_universe_dict: dict, lhs_col: str, test_start_date: str):\n",
    "    # Determine the first day of the month for all test period months\n",
    "    test_period_months = determineFirstDayOfMonthsInTestPeriod(df, test_start_date)\n",
    "\n",
    "    # Grab the name of factors columns\n",
    "    fama_cols = list(fama_df.columns.values)\n",
    "    fama_cols.remove('date')\n",
    "    factors_combo = fama_cols.copy()\n",
    "\n",
    "    # Form yhats for entire test period\n",
    "    temp_dfs = []\n",
    "    for test_period_month in test_period_months:\n",
    "        # form the relevant dataset\n",
    "        asset_universe = asset_universe_dict[test_period_month]\n",
    "        one_month_ahead = datetime.strptime(test_period_month, '%Y-%m-%d') + pd.DateOffset(months=1)\n",
    "        rel_df = df[(df.asset.isin(asset_universe)) & (df.date < one_month_ahead)][\n",
    "            ['date', 'asset', lhs_col]].copy()\n",
    "\n",
    "        # fit and predict\n",
    "        lhs_df = rel_df[['date', 'asset', lhs_col]].copy()\n",
    "        temp_df = fitAndPredict(lhs_df, lhs_col, fama_df, factors_combo, test_period_month)\n",
    "\n",
    "        # save results across the test months\n",
    "        temp_dfs.append(temp_df)\n",
    "\n",
    "    # Form object to return\n",
    "    test_df = pd.concat(temp_dfs)\n",
    "\n",
    "    test_df = test_df.sort_values(by=['date', 'asset'])\n",
    "    test_df = test_df.merge(df[['date', 'asset', 'mcap']], on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    test_df = test_df.rename(columns={'yhats': 'yhats_fama'})\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbfc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportResults(opt_chars_combos_list: List[list],\n",
    "    df: pd.DataFrame, lhs_col: str, num_factors: int, \n",
    "    num_qntls_prtls: int, periods_in_year: int, model_prefix: str,\n",
    "    out_fp: str, out_sheet: str, mcap_weighted: bool\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Generates and reports portfolio statistics for a given number of factors.\n",
    "\n",
    "    :param df: DataFrame containing the data to be analyzed.\n",
    "    :param lhs_col: Name of the t+1 returns column.\n",
    "    :param num_factors: The number of factors to consider.\n",
    "    :param num_qntls_prtls: Number of quantiles for portfolio.\n",
    "    :param periods_in_year: Number of periods in a year.\n",
    "    :param model_prefix: name of the model as a prefix for the results.\n",
    "    :param out_fp: Filepath for the output Excel file.\n",
    "    :param out_sheet: Sheet name for the output in the Excel file.\n",
    "    :param mcap_weighted: Boolean indicating if the results are market capitalization weighted.\n",
    "\n",
    "    :return: None. The results are saved directly to the Excel file.\n",
    "    \"\"\"\n",
    "    # Initialize results to return\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "    # Generate results for each model\n",
    "    for num_factor in range(1,1+num_factors):\n",
    "        # Rename model's yhats and name\n",
    "        df = df.rename(columns={'yhats_'+str(num_factor)+'_factors': 'yhats'})\n",
    "        model_name =  model_prefix+str(num_factor)\n",
    "\n",
    "        # Generate this model's portfolio statistics\n",
    "        temp_results_df = QuantTools.calcPortfolioStatistics(\n",
    "            df, lhs_col, 'yhats', 'macro_cmkt_tp7', model_name, \n",
    "            num_qntls_prtls, periods_in_year, mcap_weighted\n",
    "        )\n",
    "\n",
    "        # Append variables used\n",
    "        # NOTE: This is different for this notebook\n",
    "        temp_results_df['chars'] = \" \".join(opt_chars_combos_list[num_factor-1])\n",
    "\n",
    "        # Append results\n",
    "        results_df = pd.concat([results_df, temp_results_df])\n",
    "        \n",
    "        # Update yhat labels for next iteration\n",
    "        df = df.rename(columns={'yhats': 'yhats_'+str(num_factor)+'_factors'})\n",
    "\n",
    "    # Save the results\n",
    "    with pd.ExcelWriter(out_fp, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        results_df.to_excel(writer, sheet_name=out_sheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e528ac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 factors validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 20/62 [00:00<00:00, 76.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:51<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1 factors are: ['char_size_t'] \n",
      "\n",
      "Running 2 factors validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1891/1891 [46:14<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 factors are: ['char_illiq_tm7', 'char_size_t'] \n",
      "\n",
      "Running 3 factors validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37820/37820 [16:51:48<00:00,  1.61s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 factors are: ['char_r_tm30', 'char_size_t', 'char_vol_tm90'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    IN_FP           = '../data/clean/panel_weekly.pkl'\n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    OUT_FP          = '../output/low_dim_fm/low_dim_fms.xlsx'\n",
    "    DF_OUT_FP       = '../data/clean/test_yhats_multivariate.pkl'\n",
    "    FAMA_OUT_FP     = '../data/clean/test_yhats_fama.pkl'\n",
    "    LHS_COL         = 'r_ex_tp7'\n",
    "    VAL_START_DATE  = '2021-07-01'\n",
    "    TEST_START_DATE = '2022-07-01'\n",
    "    PERIODS_IN_YEAR = 52\n",
    "    NUM_QNTLS_FCTRS = 5\n",
    "    NUM_QNTLS_PRTLS = 5\n",
    "    NUM_FACTORS     = 3\n",
    "    NUM_CPUS        = 20\n",
    "    \n",
    "    # read in data\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    all_df = pd.read_pickle(IN_FP)\n",
    "\n",
    "    # remove unncessary RHS columns\n",
    "    df = subsetAndFormRhs(all_df, LHS_COL)\n",
    "\n",
    "    # determine optimal chars for 1-5 factor models\n",
    "    val_df = df[df.date<TEST_START_DATE].copy()\n",
    "    opt_chars_combos_list = []\n",
    "    opt_r2_pred_list = []\n",
    "    for i in range(1,1+NUM_FACTORS):\n",
    "        print(f'Running {i} factors validation.')\n",
    "        opt_chars_combo, opt_r2_pred = cvFactorCombos(\n",
    "            val_df, asset_universe_dict, LHS_COL, VAL_START_DATE, \n",
    "            NUM_QNTLS_FCTRS, NUM_CPUS, i)\n",
    "        opt_chars_combos_list.append(opt_chars_combo)\n",
    "        opt_r2_pred_list.append(opt_r2_pred)\n",
    "        print(f'Selected {i} factors are: {opt_chars_combo} \\n')\n",
    "\n",
    "    # generate test period yhats\n",
    "    test_df = fitTestPeriod(df, asset_universe_dict,\n",
    "        LHS_COL, TEST_START_DATE, NUM_QNTLS_FCTRS, NUM_FACTORS,\n",
    "        opt_chars_combos_list)\n",
    "    \n",
    "    # save yhats for later analysis\n",
    "    out_df = test_df.copy()\n",
    "    out_df = out_df.drop([LHS_COL, 'mcap'], axis=1)\n",
    "    out_df.to_pickle(DF_OUT_FP)\n",
    "\n",
    "    # Form cmkt over future horizon for calc 5-1 strat alpha and beta; add to test results\n",
    "    cmkt_df = all_df[['date', 'macro_cmkt_tm7']].drop_duplicates().copy()\n",
    "    cmkt_df['macro_cmkt_tp7'] = cmkt_df.macro_cmkt_tm7.shift(-1)\n",
    "    cmkt_df = cmkt_df.drop('macro_cmkt_tm7', axis=1)\n",
    "    test_df = test_df.merge(cmkt_df, on=['date'], how='left', validate='many_to_one')\n",
    "\n",
    "    # Report results\n",
    "    reportResults(opt_chars_combos_list,\n",
    "        test_df, LHS_COL, NUM_FACTORS, NUM_QNTLS_PRTLS, PERIODS_IN_YEAR, \n",
    "        'multi_', OUT_FP, 'raw_multi_mcap', True)\n",
    "    reportResults(opt_chars_combos_list,\n",
    "        test_df, LHS_COL, NUM_FACTORS, NUM_QNTLS_PRTLS, PERIODS_IN_YEAR, \n",
    "        'multi_', OUT_FP, 'raw_multi_equal', False)\n",
    "    \n",
    "    # Build and report fama results\n",
    "    fama_df = formFamaFactors(all_df, df, LHS_COL)\n",
    "    test_fama_df = fitFamaTestPeriod(df, fama_df, asset_universe_dict, LHS_COL, TEST_START_DATE)\n",
    "    test_fama_df = test_fama_df.merge(cmkt_df, on=['date'], how='left', validate='many_to_one')\n",
    "    test_fama_df.to_pickle(FAMA_OUT_FP)\n",
    "    results_df = QuantTools.calcPortfolioStatistics(\n",
    "        test_fama_df, LHS_COL, 'yhats_fama', 'macro_cmkt_tp7', 'fama', \n",
    "        NUM_QNTLS_PRTLS, PERIODS_IN_YEAR, True)\n",
    "    with pd.ExcelWriter(OUT_FP, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer: \n",
    "        results_df.to_excel(writer, sheet_name='raw_multi_fama')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f14a76dcff07d5b93f2c0fc65ce65c8ac7788ddb1ef5c63daa3feefa016fa519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
