{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCrosswalk(cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Cleans a crosswalk DataFrame by ensuring that both asset IDs are unique,\n",
    "    sorting the columns, sorting by the asset name, and resetting the index.\n",
    "\n",
    "    Args:\n",
    "        cw_df: A pandas DataFrame representing the crosswalk.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the cleaned crosswalk.\n",
    "    \"\"\"\n",
    "    # Ensure asset ids for cmc is unique\n",
    "    assert cw_df['asset_cm'].is_unique, \"asset_cm must be unique\"\n",
    "    assert cw_df['asset_coinapi'].is_unique, \"asset_coinapi must be unique\"\n",
    "\n",
    "    # Rename\n",
    "    cw_df = cw_df.rename(columns={'asset_coinapi': 'asset_ca'})\n",
    "\n",
    "    # Sort columns\n",
    "    sorted_cw_df = cw_df[['asset_cm', 'asset_ca']]\n",
    "\n",
    "    # Sort by cm asset name and reset index\n",
    "    cleaned_cw_df = sorted_cw_df.sort_values(by='asset_cm', ignore_index=True)\n",
    "\n",
    "    return cleaned_cw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPanel(df: pd.DataFrame, cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Clean the coinapi panel.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): raw panel data.\n",
    "        cw_df (pd.DataFrame): cleaned cm to a crosswalk.\n",
    "    \n",
    "    Returns:\n",
    "        (pd.DataFrame): cleaned panel data.\n",
    "    \"\"\"\n",
    "    # date col\n",
    "    assert 0 == df.date.isnull().sum()\n",
    "    assert type(df.date.values[0]) == np.datetime64, \"date is not the correct type.\"\n",
    "    assert df.shape[0] == (df.date.dt.minute == 0).sum(), \"all datetimes must be top of the hour\"\n",
    "\n",
    "    # cut down to relevant dates\n",
    "    df = df[(df['date'] >= '2016-07-01') & (df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # asset col\n",
    "    df = df.rename(columns={'asset_id': 'asset_ca'})\n",
    "    assert 0 == df.asset_ca.isnull().sum()\n",
    "    asset_ids = list(cw_df.asset_ca.values)\n",
    "    df = df[df.asset_ca.isin(asset_ids)]\n",
    "    assert df.shape[0] == df[df.asset_ca.isin(asset_ids)].shape[0]\n",
    "\n",
    "    # drop duplicated datetime and asset\n",
    "    df = df.drop_duplicates(subset=['date', 'asset_ca'])\n",
    "\n",
    "    # fill missing ref price with market price\n",
    "    df.loc[df.usd_per_token_ref.isnull(), 'usd_per_token_ref'] = df.usd_per_token_coinapi\n",
    "\n",
    "    # for price and volume columns, ensure in range, no missing, and convert type down\n",
    "    thresholds = {'usd_per_token_coinapi': 1e9, 'usd_volume_coinapi': 1e12, 'trades_coinapi': 1e9, 'usd_per_token_ref': 1e9}\n",
    "    for col, thresh in thresholds.items():\n",
    "        df.loc[df[col]>thresh, col] = np.nan\n",
    "        df[col] = df[col].ffill()\n",
    "        assert 0 == df[col].isnull().sum()\n",
    "        assert 0 == df[(df[col]<0) | (df[col] > thresh)].shape[0]\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # Loop over all assets to add any missing datetimes and fill columns\n",
    "    final_df = pd.DataFrame()\n",
    "    assets = df['asset_ca'].unique()\n",
    "    for asset in assets:    \n",
    "        # subset to asset of interest\n",
    "        asset_df = df[df.asset_ca == asset].copy()\n",
    "        asset_df.set_index('date', inplace=True)\n",
    "\n",
    "        # find the min and max datetime for the asset\n",
    "        min_dt, max_dt = asset_df.index.min(), asset_df.index.max()\n",
    "\n",
    "        # create a complete DateTimeIndex with hourly frequency between min and max datetime\n",
    "        full_date_range = pd.date_range(start=min_dt, end=max_dt, freq='1H')\n",
    "\n",
    "        # Reindex asset_df with the complete DateTimeIndex\n",
    "        asset_df = asset_df.reindex(full_date_range)\n",
    "\n",
    "        # Fill asset_ca column for newly added rows\n",
    "        asset_df['asset_ca'].fillna(asset, inplace=True)\n",
    "        \n",
    "        # Forward fill gaps that are less than 31 days\n",
    "        asset_df = asset_df.ffill(limit=31*24)\n",
    "\n",
    "        # Drop rows if missing all cols as this was a bigger gap than 31 days\n",
    "        asset_df = asset_df.dropna(subset=['usd_per_token_coinapi', 'usd_volume_coinapi', 'trades_coinapi', 'usd_per_token_ref'])\n",
    "\n",
    "        # Reset index \n",
    "        asset_df.reset_index(inplace=True)\n",
    "        asset_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "        # Add data to master df\n",
    "        final_df = pd.concat((final_df, asset_df))\n",
    "\n",
    "    # Reset index of the final DataFrame\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Reset  names\n",
    "    df = final_df.copy()\n",
    "    del final_df\n",
    "\n",
    "    # Confirm no missings in the df\n",
    "    assert(df.isnull().sum().sum() == 0)\n",
    "\n",
    "    # Reset column names\n",
    "    df = df.rename(columns={'usd_per_token_coinapi': 'usd_per_token_ca',\n",
    "                            'usd_volume_coinapi': 'usd_volume_ca',\n",
    "                            'trades_coinapi': 'trades_volume_ca',\n",
    "                            'usd_per_token_ref': 'usd_ref_price_ca'})\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not df.duplicated(subset=['date', 'asset_ca']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    df = df.sort_values(by=['date', 'asset_ca']).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMacro(m_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Cleans and preprocesses a macro data DataFrame.\n",
    "    This function checks the integrity of the input data, removes duplicates, filters by date, \n",
    "    validates and adjusts column types, renames columns, and sorts the data by date.\n",
    "\n",
    "    Args:\n",
    "        m_df (pd.DataFrame): A DataFrame containing macro data, with the following columns:\n",
    "            - date (np.datetime64): Datetime of the data point.\n",
    "            - usd_per_usdc (float): USD price per USDC.\n",
    "            - usd_per_usdt (float): USD price per USDT.\n",
    "            \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and preprocessed macro data DataFrame.\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: If input data has missing or invalid values, or if there are duplicates in the date column.\n",
    "    \"\"\"\n",
    "    # date col\n",
    "    assert 0 == m_df.date.isnull().sum()\n",
    "    assert type(m_df.date.values[0]) == np.datetime64, \"date is not the correct type.\"\n",
    "    assert m_df.shape[0] == (m_df.date.dt.minute == 0).sum(), \"all datetimes must be top of the hour\"\n",
    "\n",
    "    # cut down to relevant dates\n",
    "    m_df = m_df[(m_df['date'] >= '2016-07-01') & (m_df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # drop duplicated datetime \n",
    "    m_df = m_df.drop_duplicates(subset=['date'])\n",
    "\n",
    "    # for price and volume columns, ensure in range, no missing, and convert type down\n",
    "    thresholds = {'usd_per_usdc': 1e5, 'usd_per_usdt': 1e5}\n",
    "    for col, thresh in thresholds.items():\n",
    "        assert 0 == m_df[(m_df[col]<0) | (m_df[col] > thresh)].shape[0]\n",
    "        m_df[col] = m_df[col].astype('float32')\n",
    "\n",
    "    # Ensure all dates are present\n",
    "    min_dt, max_dt = m_df.date.min(), m_df.date.max()\n",
    "    full_date_range = pd.date_range(start=min_dt, end=max_dt, freq='1H')\n",
    "    assert len(m_df) == len(full_date_range)\n",
    "\n",
    "    # Reset column names\n",
    "    m_df = m_df.rename(columns={'usd_per_usdc': 'usd_per_usdc_ca',\n",
    "                                'usd_per_usdt': 'usd_per_usdt_ca'})\n",
    "\n",
    "    # ensure no duplicates by date \n",
    "    assert not m_df.duplicated(subset=['date']).any()\n",
    "\n",
    "    # Sort by date and reset index\n",
    "    m_df = m_df.sort_values(by=['date']).reset_index(drop=True)\n",
    "\n",
    "    return m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    CW_IN_FP = '../data/derived/cm_to_coinapi_cw.pkl'\n",
    "    PANEL_IN_FP = '../data/raw/coinapi_panel_hourly.pkl'\n",
    "    MACRO_IN_FP = '../data/raw/coinapi_macro_hourly.pkl'\n",
    "    PANEL_OUT_FP = '../data/derived/ca_panel.pkl'\n",
    "    CW_OUT_FP    = '../data/derived/ca_cm_cw.pkl'\n",
    "    MACRO_OUT_FP = '../data/derived/ca_macro.pkl'\n",
    "\n",
    "    # import \n",
    "    df    = pd.read_pickle(PANEL_IN_FP)\n",
    "    cw_df = pd.read_pickle(CW_IN_FP)\n",
    "    m_df  = pd.read_pickle(MACRO_IN_FP)\n",
    "\n",
    "    # clean\n",
    "    cw_df = cleanCrosswalk(cw_df)\n",
    "    df = cleanPanel(df, cw_df)\n",
    "    m_df = cleanMacro(m_df)\n",
    "\n",
    "    # save\n",
    "    df.to_pickle(PANEL_OUT_FP)\n",
    "    cw_df.to_pickle(CW_OUT_FP)\n",
    "    m_df.to_pickle(MACRO_OUT_FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
