{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f16df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4299dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCrosswalk(cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Cleans a crosswalk DataFrame by ensuring that both asset IDs are unique,\n",
    "    sorting the columns, sorting by the asset name, and resetting the index.\n",
    "\n",
    "    Args:\n",
    "        cw_df: A pandas DataFrame representing the crosswalk.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the cleaned crosswalk.\n",
    "    \"\"\"\n",
    "    # Ensure asset ids for cmc is unique\n",
    "    assert cw_df['asset_cmc'].is_unique, \"asset_cmc must be unique\"\n",
    "\n",
    "    # Sort columns\n",
    "    sorted_cw_df = cw_df[['asset_cm', 'asset_cmc']]\n",
    "\n",
    "    # Sort by cm asset name and reset index\n",
    "    cleaned_cw_df = sorted_cw_df.sort_values(by='asset_cm', ignore_index=True)\n",
    "\n",
    "    return cleaned_cw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9246ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPanel(df: pd.DataFrame, cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Clean the CMC panel.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): raw panel data.\n",
    "        cw_df (pd.DataFrame): cleaned cmc to cm crosswalk.\n",
    "    \n",
    "    Returns:\n",
    "        (pd.DataFrame): cleaned panel data.\n",
    "    \"\"\"\n",
    "    # drop useless col\n",
    "    df = df.drop(['tvl_ratio', 'platform'], axis=1)\n",
    "\n",
    "    # date col\n",
    "    assert 0 == df.date.isnull().sum()\n",
    "    assert type(df.date.values[0]) == np.datetime64, \"date is not the correct type.\"\n",
    "    df['date'] = df.date.dt.ceil('H')\n",
    "\n",
    "    # Cut down to relevant dates\n",
    "    df = df[(df['date'] >= '2016-07-01') & (df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # asset col\n",
    "    assert 0 == df.asset_cmc.isnull().sum()\n",
    "    asset_ids = list(cw_df.asset_cmc.values)\n",
    "    assert df.shape[0] == df[df.asset_cmc.isin(asset_ids)].shape[0]\n",
    "\n",
    "    # drop duplicated datetime and asset\n",
    "    df = df.drop_duplicates(subset=['date', 'asset_cmc'])\n",
    "\n",
    "    # For price, volume, and mcap columns, ensure in range, ffill if not, no missing, and convert type down\n",
    "    thresholds = {'usd_per_token': 1e9, 'usd_volume_24h': 1e12, 'usd_mcap': 1e13}\n",
    "    for col, thresh in thresholds.items():\n",
    "        df.loc[df[col]>thresh, col] = np.nan\n",
    "        df[col] = df.groupby('asset_cmc')[col].fillna(method='ffill')\n",
    "        assert 0 == df[col].isnull().sum()\n",
    "        assert 0 == df[(df[col]<0) | (df[col] > thresh)].shape[0]\n",
    "        df[col] = df[col].astype('float32')\n",
    "        \n",
    "    # rank column \n",
    "    df['rank_cmc'] = df.groupby('asset_cmc')['rank_cmc'].fillna(method='ffill')\n",
    "    df['rank_cmc'] = df.groupby('date')['rank_cmc'].transform(lambda x: x.fillna(2 * x.max()))\n",
    "    df['rank_cmc'] = df.rank_cmc.astype(int)\n",
    "    assert np.min(df.rank_cmc)>=1\n",
    "    assert np.max(df.rank_cmc)<1e4\n",
    "    assert 0 == df.rank_cmc.isnull().sum()\n",
    "\n",
    "    # num market pairs\n",
    "    df['num_market_pairs_cmc'] = df.groupby('asset_cmc')['num_market_pairs_cmc'].fillna(method='ffill') # ffill within asset\n",
    "    first_instance_idx = df[df['num_market_pairs_cmc'].isna()].groupby('asset_cmc').apply(lambda x: x.index[0])\n",
    "    df.loc[first_instance_idx, 'num_market_pairs_cmc'] = 1 # fill first index of each asset with 1 if missing\n",
    "    df['num_market_pairs_cmc'] = df.groupby('date')['num_market_pairs_cmc'].transform(lambda x: x.fillna(x.min())) # fill missing values with min value by date\n",
    "    df['num_market_pairs_cmc'] = df.groupby('asset_cmc')['num_market_pairs_cmc'].fillna(method='ffill') # ffill within asset\n",
    "    df['num_market_pairs_cmc'] = df.num_market_pairs_cmc.astype(int)\n",
    "    assert np.min(df.num_market_pairs_cmc.values) >= 1\n",
    "    assert np.max(df.num_market_pairs_cmc.values) <= 1e6\n",
    "    assert 0 == df.num_market_pairs_cmc.isnull().sum()\n",
    "\n",
    "    # supply columns\n",
    "    df.loc[df.max_supply.isnull(), 'max_supply'] = 10*df.total_supply\n",
    "    supply_cols = ['circulating_supply', 'total_supply', 'max_supply'] \n",
    "    for col in supply_cols:\n",
    "        df.loc[df.asset_cmc=='multiversx-egld', col] = (df[df.asset_cmc=='multiversx-egld'].usd_mcap / \n",
    "                                                        df[df.asset_cmc=='multiversx-egld'].usd_per_token) # fix this asset\n",
    "        df[col] = df.groupby('asset_cmc')[col].fillna(method='ffill') # ffill within asset\n",
    "    df.loc[df.circulating_supply.isnull(), 'circulating_supply'] = df.usd_mcap / df.usd_per_token\n",
    "    df.loc[df.total_supply.isnull(), 'total_supply'] = 10*df.circulating_supply\n",
    "    df.loc[df.max_supply.isnull(), 'max_supply'] = 10*df.total_supply\n",
    "    for col in supply_cols:\n",
    "        assert 0 == df[col].isnull().sum()\n",
    "        assert 1e18 > np.max(df[col])\n",
    "        assert 0 <= np.min(df[col])\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # clean missing values in tags column\n",
    "    def replace_nan_with_empty_list(value):\n",
    "        if isinstance(value, float) and np.isnan(value):\n",
    "            return []\n",
    "        else:\n",
    "            return value\n",
    "    df['tags'] = df['tags'].apply(replace_nan_with_empty_list)\n",
    "\n",
    "    # create vc column\n",
    "    df['vc_cmc'] = 0\n",
    "    df['vc_cmc'] = df['tags'].apply(lambda x: 1 if any('portfolio' in tag for tag in x) else 0)\n",
    "\n",
    "    # drop tags\n",
    "    df = df.drop('tags', axis=1)\n",
    "\n",
    "    # Loop over all assets to add any missing datetimes and fill columns\n",
    "    final_df = pd.DataFrame()\n",
    "    assets = df['asset_cmc'].unique()\n",
    "    for asset in assets:    \n",
    "        # subset to asset of interest\n",
    "        asset_df = df[df.asset_cmc == asset].copy()\n",
    "        asset_df.set_index('date', inplace=True)\n",
    "\n",
    "        # find the min and max datetime for the asset\n",
    "        min_dt, max_dt = asset_df.index.min(), asset_df.index.max()\n",
    "\n",
    "        # create a complete DateTimeIndex with hourly frequency between min and max datetime\n",
    "        full_date_range = pd.date_range(start=min_dt, end=max_dt, freq='1H')\n",
    "\n",
    "        # Reindex asset_df with the complete DateTimeIndex\n",
    "        asset_df = asset_df.reindex(full_date_range)\n",
    "\n",
    "        # Fill asset_cmc column for newly added rows\n",
    "        asset_df['asset_cmc'].fillna(asset, inplace=True)\n",
    "        \n",
    "        # Forward fill gaps that are less than 31 days\n",
    "        asset_df = asset_df.ffill(limit=31*24)\n",
    "\n",
    "        # Reset index and fill asset_cmc column for the newly added rows\n",
    "        asset_df.reset_index(inplace=True)\n",
    "        asset_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "        # Add data to master df\n",
    "        final_df = pd.concat((final_df, asset_df))\n",
    "\n",
    "    # Reset index of the final DataFrame\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Reset  names\n",
    "    df = final_df.copy()\n",
    "    del final_df\n",
    "\n",
    "    # Confirm no missings in the df\n",
    "    assert(df.isnull().sum().sum() == 0)\n",
    "\n",
    "    # Set column order\n",
    "    cols = list(df.columns.values)\n",
    "    cols_to_remove = ['date', 'asset_cmc', 'usd_per_token', 'usd_mcap', 'usd_volume_24h']\n",
    "    for col in cols_to_remove:\n",
    "        cols.remove(col)\n",
    "    cols.sort()\n",
    "    df = df[cols_to_remove+cols]\n",
    "\n",
    "    # Ensure all columns have cmc in name\n",
    "    df.columns = [col if col == 'date' or col.endswith('_cmc') else col + '_cmc' for col in df.columns]\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not df.duplicated(subset=['date', 'asset_cmc']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    df = df.sort_values(by=['date', 'asset_cmc']).reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38b9b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanMacro(global_df: pd.DataFrame, ex_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # date col in each\n",
    "    global_df['date'] = pd.to_datetime(global_df['date'], utc=True).dt.tz_localize(None)\n",
    "    assert 0 == global_df.date.isnull().sum()\n",
    "    global_df['date'] = global_df.date.dt.ceil('H')\n",
    "    ex_df['date'] = pd.to_datetime(ex_df['date'], utc=True).dt.tz_localize(None)\n",
    "    assert 0 == ex_df.date.isnull().sum()\n",
    "    ex_df['date'] = ex_df.date.dt.ceil('H')\n",
    "\n",
    "    # drop duplicates\n",
    "    global_df = global_df.drop_duplicates(subset='date')\n",
    "    ex_df = ex_df.drop_duplicates(subset='date')\n",
    "\n",
    "    # drop cols\n",
    "    global_df = global_df.drop('altcoin_usd_mcap', axis=1)\n",
    "\n",
    "    # fix ranges of global df values\n",
    "    assert 0 == global_df[(global_df.total_usd_mcap < 0) | (global_df.total_usd_mcap > 1e13)].shape[0]\n",
    "    assert 0 == global_df[(global_df.total_usd_volume_24h < 0) | (global_df.total_usd_volume_24h > 1e15)].shape[0]\n",
    "    global_df.loc[global_df.altcoin_usd_volume_24h<0, 'altcoin_usd_volume_24h'] = np.nan\n",
    "    global_df.loc[global_df.altcoin_usd_volume_24h>1e15, 'altcoin_usd_volume_24h'] = np.nan\n",
    "\n",
    "    # confirm exchange variables ranges\n",
    "    ex_df['ex_usd_volume_24h'] = ex_df.ex_usd_volume_24h.astype('float32')\n",
    "    ex_df['ex_num_market_pairs'] = ex_df.ex_num_market_pairs.astype('float32')\n",
    "    assert 0 == ex_df.isnull().sum().sum()\n",
    "    assert 0 == ex_df[(ex_df.ex_usd_volume_24h<0) | (ex_df.ex_usd_volume_24h>1e15)].shape[0]\n",
    "    assert 0 == ex_df[(ex_df.ex_num_market_pairs<0) | (ex_df.ex_num_market_pairs>1e7)].shape[0]\n",
    "\n",
    "    # identify dex and cex\n",
    "    dexs = ['bancor-network',\n",
    "            'compound',\n",
    "            'curve-finance',\n",
    "            'dydx',\n",
    "            'pancakeswap-v2',\n",
    "            'sushiswap',\n",
    "            'uniswap-v2']\n",
    "    ex_df['dex'] = 0\n",
    "    ex_df.loc[ex_df.ex_slug.isin(dexs), 'dex'] = 1\n",
    "\n",
    "    # form dex and cex dfs\n",
    "    cex_df = ex_df[ex_df.dex==0].groupby('date')[['ex_usd_volume_24h', 'ex_num_market_pairs']].sum()\n",
    "    cex_df = cex_df.rename(columns={'ex_usd_volume_24h': 'ex_usd_volume_24h_cex',\n",
    "                                    'ex_num_market_pairs': 'ex_num_pairs_cex'})\n",
    "    dex_df = ex_df[ex_df.dex==1].groupby('date')[['ex_usd_volume_24h', 'ex_num_market_pairs']].sum()\n",
    "    dex_df = dex_df.rename(columns={'ex_usd_volume_24h': 'ex_usd_volume_24h_dex',\n",
    "                                    'ex_num_market_pairs': 'ex_num_pairs_dex'})\n",
    "\n",
    "    # merge\n",
    "    macro_df = global_df.merge(cex_df, on='date', how='outer', validate='one_to_one')\n",
    "    macro_df = macro_df.merge(dex_df, on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # Cut down to relevant dates\n",
    "    macro_df = macro_df[(macro_df['date'] >= '2016-07-01') & (macro_df['date'] <= '2023-01-02')]\n",
    "\n",
    "    # Ensure all dates are present\n",
    "    macro_df.set_index('date', inplace=True)\n",
    "    min_dt, max_dt = macro_df.index.min(), macro_df.index.max()\n",
    "    full_date_range = pd.date_range(start=min_dt, end=max_dt, freq='1H')\n",
    "    macro_df = macro_df.reindex(full_date_range)\n",
    "    macro_df.reset_index(inplace=True)\n",
    "    macro_df = macro_df.rename(columns={\"index\": 'date'})\n",
    "    macro_df = macro_df.drop_duplicates(subset='date')\n",
    "\n",
    "    # Fill the first value of columns with 1 if missing\n",
    "    for col in ['active_cryptos', 'active_exchanges', \n",
    "        'active_market_pairs', 'ex_usd_volume_24h_cex', \n",
    "        'ex_num_pairs_cex', 'ex_usd_volume_24h_dex', 'ex_num_pairs_dex']:\n",
    "        macro_df.loc[(macro_df.index==np.min(macro_df.index)) \n",
    "                &(macro_df[col].isnull()), col] = 1\n",
    "\n",
    "    # Forward fill\n",
    "    macro_df = macro_df.ffill()\n",
    "\n",
    "    # Reset index and fill asset_cmc column for the newly added rows\n",
    "    macro_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "    # Reset index of the final df\n",
    "    macro_df = macro_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # Confirm no missings in the df\n",
    "    assert(macro_df.isnull().sum().sum() == 0)\n",
    "\n",
    "    # Set column order\n",
    "    cols = list(macro_df.columns.values)\n",
    "    cols.remove('date')\n",
    "    cols.sort()\n",
    "    macro_df = macro_df[['date']+cols]\n",
    "\n",
    "    # Convert all columns to float32\n",
    "    for col in cols:\n",
    "        macro_df[col] = macro_df[col].astype('float32')\n",
    "\n",
    "    # Ensure all columns have cmc in name\n",
    "    macro_df.columns = [col if col == 'date' or col.endswith('_cmc') else col + '_cmc' for col in macro_df.columns]\n",
    "\n",
    "    # ensure no duplicates by date and asset\n",
    "    assert not macro_df.duplicated(subset=['date']).any()\n",
    "\n",
    "    # Sort by date then asset and reset index\n",
    "    macro_df = macro_df.sort_values(by=['date']).reset_index(drop=True)\n",
    "\n",
    "    return macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    PANEL_IN_FP  = \"../data/raw/cmc_panel.pkl\"\n",
    "    CW_IN_FP     = '../data/raw/cmc_coinmetrics_cw.pkl'\n",
    "    MACRO_IN_FP  = '../data/raw/cmc_macro.pkl'\n",
    "    EX_IN_FP     = '../data/raw/cmc_exchange_panel.pkl'\n",
    "    PANEL_OUT_FP = '../data/derived/cmc_panel.pkl'\n",
    "    CW_OUT_FP    = '../data/derived/cmc_cm_cw.pkl'\n",
    "    MACRO_OUT_FP = '../data/derived/cmc_macro.pkl'\n",
    "\n",
    "    # import\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "    cw_df = pd.read_pickle(CW_IN_FP)\n",
    "    global_df = pd.read_pickle(MACRO_IN_FP)\n",
    "    ex_df =  pd.read_pickle(EX_IN_FP)\n",
    "    \n",
    "    # clean\n",
    "    cw_df = cleanCrosswalk(cw_df)\n",
    "    df = cleanPanel(df, cw_df)\n",
    "    macro_df = cleanMacro(global_df, ex_df)\n",
    "\n",
    "    # save\n",
    "    df.to_pickle(PANEL_OUT_FP)\n",
    "    cw_df.to_pickle(CW_OUT_FP)\n",
    "    macro_df.to_pickle(MACRO_OUT_FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
