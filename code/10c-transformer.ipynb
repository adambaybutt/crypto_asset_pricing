{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab026fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 14:28:03.155938: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-27 14:28:03.207365: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-27 14:28:04.067560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Ti, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "# To be able to use the quantools, due to my crap path names have to add to sys path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/adam/Dropbox/2-creations/2-crafts/7-buidl/0-utils/quant_tools/code')\n",
    "\n",
    "# IMPORT PACKAGES\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from tools import QuantTools\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00e177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetRowsAndColumns(\n",
    "    df: pd.DataFrame, lhs_col: str\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    ''' Subset relevant rows and columns to form needed dataframes.\n",
    "\n",
    "    Parameters:\n",
    "        df      (pd.DataFrame): The original DataFrame containing the data.\n",
    "        lhs_col (str):          The name of lhs column.\n",
    "\n",
    "    Notes:\n",
    "        - Kept characteristics based on what are most raw characteristics and \n",
    "            what work well as univariate factor to gen large spread in returns\n",
    "            in pre 2h 2022 data.\n",
    "        - Kept matching number of macro cols up to integer scalar which have\n",
    "            high corr to avg LHS after taking out variabilitity of previous\n",
    "            chosen columns.\n",
    "        - Keep also previous returns as a separate dataframe for the factor\n",
    "            side of the network.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames:\n",
    "            y_df: A DataFrame with relevant rows and lhs column.\n",
    "            char_df: A DataFrame with raw characteristics.\n",
    "            ts_df: A DataFrame containing previous returns.\n",
    "    '''\n",
    "    # Set column lists that I manually constructed\n",
    "    char_cols_to_keep = ['char_addr_active_tm1h',\n",
    "        'char_addr_new_tm1h',\n",
    "        'char_alpha_tm7',\n",
    "        'char_beta_tm7',\n",
    "        'char_bidask_t',\n",
    "        'char_delta_flow_dist_tm1h',\n",
    "        'char_delta_holders_dist_tm1h',\n",
    "        'char_exchange_inflow_tm1h',\n",
    "        'char_exchange_outflow_tm1h',\n",
    "        'char_ivol_tm30',\n",
    "        'char_prct_supply_in_profit_t',\n",
    "        'char_r_ath_t',\n",
    "        'char_r_atl_t',\n",
    "        'char_r_tm1h',\n",
    "        \"char_r_tm14\",\n",
    "        'char_sent_neg_twitter_tm1',\n",
    "        'char_sent_volume_consumed_tm1',\n",
    "        'char_size_t',\n",
    "        'char_social_volume_tm1',\n",
    "        'char_trades_t',\n",
    "        'char_turnover_tm1',\n",
    "        'char_tx_volume_t',\n",
    "        'char_var5_tm1',\n",
    "        \"char_vol_tm6h\"]\n",
    "    macro_cols_to_keep = ['macro_avg_fee_med_usd_t',\n",
    "                        'macro_btc_adr_act_cnt_t',\n",
    "                        'macro_btc_cap_act_1yr_usd_t',\n",
    "                        'macro_btc_fee_med_usd_t',\n",
    "                        'macro_btc_flow_in_ex_usd_t',\n",
    "                        'macro_btc_flow_out_ex_usd_t',\n",
    "                        'macro_btc_mvrv_t',\n",
    "                        'macro_btc_sply_act_30d_t',\n",
    "                        'macro_btc_sply_utxo_loss_t',\n",
    "                        'macro_btc_sply_utxo_prof_t',\n",
    "                        'macro_btc_tx_tfr_val_med_usd_t',\n",
    "                        'macro_btc_utxo_age_med_t',\n",
    "                        'macro_cmkt_tm1h',\n",
    "                        'macro_cp3mx_t',\n",
    "                        'macro_cpiaucsl_t',\n",
    "                        'macro_emv_inflation_t',\n",
    "                        'macro_emv_t',\n",
    "                        'macro_eth_adr_act_cnt_t',\n",
    "                        'macro_eth_cap_act_1yr_usd_t',\n",
    "                        'macro_eth_fee_med_t',\n",
    "                        'macro_eth_flow_in_ex_usd_t',\n",
    "                        'macro_eth_flow_out_ex_usd_t',\n",
    "                        'macro_eth_roi_t',\n",
    "                        'macro_eth_stakers_count_t',\n",
    "                        'macro_eth_tx_tfr_val_med_usd_t',\n",
    "                        'macro_eth_vel_cur_1yr_t',\n",
    "                        'macro_ex_open_interest_future_usd_t',\n",
    "                        'macro_ex_usd_volume_24h_cex_t',\n",
    "                        'macro_ex_volume_future_usd_t',\n",
    "                        'macro_ex_volume_t',\n",
    "                        'macro_expinf1yr_t',\n",
    "                        'macro_fedfunds_t',\n",
    "                        'macro_gs1_t',\n",
    "                        'macro_invest_t',\n",
    "                        'macro_m2sl_t',\n",
    "                        'macro_mvrv_med_t',\n",
    "                        'macro_oilpricex_t',\n",
    "                        'macro_snp500_t',\n",
    "                        'macro_snp_pe_t',\n",
    "                        'macro_stablecoin_dev_t',\n",
    "                        'macro_t10yie_t',\n",
    "                        'macro_tmu_sca_t',\n",
    "                        'macro_total_adr_bal_cnt_t',\n",
    "                        'macro_total_cap_act_1yr_usd_t',\n",
    "                        'macro_total_maker_deposits_t',\n",
    "                        'macro_total_tx_tfr_val_adj_usd_t',\n",
    "                        'macro_us_mpu_t',\n",
    "                        'macro_vixclsx_t']\n",
    "\n",
    "    # Dropping pre 2020 rows so when we create a balanced panel we have more real data than fake\n",
    "    df = df[df.date >= '2020-01-01'].copy()\n",
    "\n",
    "    # Form dataframe of all dates and assets and merge back on df to ensure obs for all assets\n",
    "    unique_dates  = df.date.unique()\n",
    "    unique_assets = df.asset.unique()\n",
    "    cross_product = list(itertools.product(unique_dates, unique_assets))\n",
    "    cross_df      = pd.DataFrame(cross_product, columns=['date', 'asset'])\n",
    "    df            = df.merge(\n",
    "        cross_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    df            = df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "    assert(len(df) == (len(np.unique(df.date))*len(np.unique(df.asset))))\n",
    "\n",
    "    # Form dataframe\n",
    "    y_df = df[['date', 'asset', lhs_col]].copy()\n",
    "    y_df = y_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "\n",
    "    # Form characteristic dataframe\n",
    "    char_df = df[['date', 'asset']+char_cols_to_keep].copy()\n",
    "    char_df = char_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "\n",
    "    # Form previous return dataframe by extracting from char_df and reshaping wide\n",
    "    r_df         = char_df[['date', 'asset', 'char_r_tm1h']].copy()\n",
    "    r_df         = r_df.pivot(index='date', columns='asset', values='char_r_tm1h').reset_index()\n",
    "    r_df.columns = (['date'] \n",
    "        + ['asset_r_' + str(col) + '_t1h' for col in r_df.columns if col != 'date'])\n",
    "\n",
    "    # Form ts_df as the macro columns plus the previous return columns\n",
    "    macro_df = df[df.asset=='btc'][['date']+macro_cols_to_keep].copy()\n",
    "    assert(set(macro_df.date)==set(r_df.date))\n",
    "    ts_df    = macro_df.merge(r_df, on='date', how='inner', validate='one_to_one')\n",
    "    ts_df    = ts_df.sort_values(by='date', ignore_index=True)\n",
    "\n",
    "    # Run final checks\n",
    "    assert(set(y_df.date)==set(char_df.date))\n",
    "    assert(set(y_df.date)==set(ts_df.date))\n",
    "    assert(0 == ((len(macro_cols_to_keep) \n",
    "                / len(char_cols_to_keep)) % 1)) # macro cols are some multiple of # char cols\n",
    "\n",
    "    return y_df, char_df, ts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e7a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeAndFillMissing(df: pd.DataFrame, \n",
    "        lhs_col: Optional[str] = None, lhs_pad: float = -2, rhs_pad: float = -2,\n",
    "        ignore_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize all numeric columns of a DataFrame to be between -1 and 1 based on rank,\n",
    "    and set missing values to given lhs and rhs pad values.\n",
    "\n",
    "    Parameters:\n",
    "    df          (pd.DataFrame):        Input DataFrame to normalize.\n",
    "    lhs_col     (str, optional):       Column name to fill missing values with lhs_pad.\n",
    "    lhs_pad     (float, optional):     Value to replace missing values in lhs_col.\n",
    "    rhs_pad     (float, optional):     Value to replace missing values in other columns.\n",
    "    ignore_cols (List[str], optional): List of column names to ignore during normalization.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Normalized DataFrame.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If any column to normalize is not numeric.\n",
    "    \"\"\"\n",
    "    # If ignore_cols is not provided, use an empty list\n",
    "    if ignore_cols is None:\n",
    "        ignore_cols = []\n",
    "\n",
    "    # Get the columns to normalize\n",
    "    cols_to_normalize = [col for col in df.columns if col not in ignore_cols]\n",
    "\n",
    "    # Check if all columns to normalize are numeric\n",
    "    if not all(dtype.kind in 'biufc' for dtype in df[cols_to_normalize].dtypes):\n",
    "        raise ValueError(\"All columns to normalize must be numeric\")\n",
    "\n",
    "    # Check for constant columns and raise a warning\n",
    "    constant_columns = df[cols_to_normalize].columns[df[cols_to_normalize].nunique() <= 1]\n",
    "    if len(constant_columns) > 0:\n",
    "        print(f\"Warning: Columns {constant_columns} have constant values and will result in NaN after normalization.\")\n",
    "\n",
    "    # Find out how many missing in the cols to normalize \n",
    "    missing_per_column = df[cols_to_normalize].isnull().sum()\n",
    "\n",
    "    # Rank the values in each column \n",
    "    #    -where we first rank normalize\n",
    "    #    -but then add noise and \n",
    "    #         re rank normalize to ensure unique values with \n",
    "    #         even coverage of RHS latent space\n",
    "    if len(cols_to_normalize) > 0:\n",
    "        df[cols_to_normalize] = 2*((df[cols_to_normalize].rank() \n",
    "                                    / (len(df) - missing_per_column)) - 0.5)\n",
    "        df[cols_to_normalize] = (df[cols_to_normalize] \n",
    "            + np.random.uniform(-1e-6, 1e-6, (len(df), len(cols_to_normalize))))\n",
    "        df[cols_to_normalize] = (2*((df[cols_to_normalize].rank() \n",
    "                                    / (len(df) - missing_per_column)) - 0.5))\n",
    "\n",
    "    # Replace missing LHS and RHS with given buffer values\n",
    "    if lhs_col is not None and lhs_col in df.columns:\n",
    "        df[lhs_col].fillna(lhs_pad, inplace=True)\n",
    "        assert np.allclose(0, df[lhs_col].isnull().sum())\n",
    "    df.fillna(rhs_pad, inplace=True)\n",
    "    assert 0 == df.isnull().sum().sum()\n",
    "\n",
    "    # Compress data, resort, and reset index\n",
    "    for col in df.select_dtypes(include=[np.float64]).columns:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    if 'asset' in df.columns:\n",
    "        df      = df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "    else:\n",
    "        df      = df.sort_values(by=['date'], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6ef756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formLhsAndRhsTensors(rel_y_df: pd.DataFrame, rel_char_df: pd.DataFrame, rel_ts_df: pd.DataFrame,\n",
    "                            datetimes_window: List[pd.Timestamp], prev_asset_ret_cols: List[str],\n",
    "                            macro_cols: List[str], lhs_col: str, num_lags: int, num_assets: int,\n",
    "                            num_chars: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Form tensors for loading input, factor input, and output for machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "        rel_y_df            (pd.DataFrame):       DataFrame containing target variable (response) data.\n",
    "        rel_char_df         (pd.DataFrame):       DataFrame containing characteristics data.\n",
    "        rel_ts_df           (pd.DataFrame):       DataFrame containing time series data (factors).\n",
    "        datetimes_window    (List[pd.Timestamp]): List of datetime values for which to form tensors.\n",
    "        prev_asset_ret_cols (List[str]):          List of column names for previous asset returns in `rel_ts_df`.\n",
    "        macro_cols          (List[str]):          List of column names for macroeconomic data in `rel_ts_df`.\n",
    "        lhs_col             (str):                The column name in `rel_y_df` representing the target variable.\n",
    "        num_lags            (int):                Number of lagged time steps to consider.\n",
    "        num_assets          (int):                Number of assets (entities).\n",
    "        num_chars           (int):                Number of characteristics.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: A tuple containing three tensors:\n",
    "            - `loading_input`: Tensor containing the concatenated characteristics and macro data.\n",
    "            - `factor_input`: Tensor containing previous asset return data.\n",
    "            - `output`: Tensor containing the target returns.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input data shapes are not as expected.\n",
    "    \"\"\"\n",
    "    # Initialize lists to return\n",
    "    loading_input_list = []\n",
    "    factor_input_list  = []\n",
    "    output_list        = []\n",
    "\n",
    "    # Loop over all datetimes to form LHS and RHS for\n",
    "    for i, datetime in enumerate(datetimes_window):\n",
    "        # Form the beginning datetime for this observation given the number of lags to step back\n",
    "        datetime_input_start = datetime - pd.Timedelta(hours=num_lags - 1)\n",
    "\n",
    "        # Obtain output data\n",
    "        output_list.append(rel_y_df.loc[rel_y_df.date == datetime, lhs_col].values)\n",
    "\n",
    "        # Obtain input data\n",
    "        rel_char_filtered = rel_char_df[\n",
    "            (rel_char_df.date >= datetime_input_start) & (rel_char_df.date <= datetime)\n",
    "            ].drop(columns=['date', 'asset'])\n",
    "\n",
    "        char_data = rel_char_filtered.values.reshape(\n",
    "                        (num_lags, num_assets, num_chars)\n",
    "                        ).transpose((0, 2, 1))\n",
    "\n",
    "        rel_ts_filtered = rel_ts_df[\n",
    "            (rel_ts_df.date >= datetime_input_start) & (rel_ts_df.date <= datetime)]\n",
    "\n",
    "        if len(rel_ts_filtered) < num_lags:\n",
    "            raise ValueError(\"Not enough data points for the given number of lags.\")\n",
    "\n",
    "        macro_data = rel_ts_filtered[macro_cols].values.reshape((num_lags, num_chars, -1))\n",
    "\n",
    "        # Append to lists of input data\n",
    "        loading_input_list.append(np.concatenate((char_data, macro_data), axis=2))\n",
    "        factor_input_list.append(rel_ts_filtered[prev_asset_ret_cols].values)\n",
    "\n",
    "    # Convert validation data lists to tensors\n",
    "    loading_input = tf.convert_to_tensor(np.array(loading_input_list), dtype=tf.float32)\n",
    "    factor_input  = tf.convert_to_tensor(np.array(factor_input_list), dtype=tf.float32)\n",
    "    output        = tf.convert_to_tensor(np.array(output_list), dtype=tf.float32)\n",
    "\n",
    "    return loading_input, factor_input, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430d5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, num_heads: int, hidden_dim: int, dropout_pct: float,\n",
    "                 dense_dim: int, l2_penalty: float, **kwargs):\n",
    "        \"\"\"\n",
    "        TransformerEncoder layer.\n",
    "\n",
    "        Parameters:\n",
    "            num_heads   (int):   Number of attention heads in the multi-head attention layer.\n",
    "            hidden_dim  (int):   Dimension of the hidden layers.\n",
    "            dropout_pct (float): Dropout rate as a percentage.\n",
    "            dense_dim   (int):   Dimension of the dense layers.\n",
    "            l2_penalty  (float): L2 regularization penalty.\n",
    "            **kwargs: Additional arguments for the base Layer class.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads   = num_heads\n",
    "        self.hidden_dim  = hidden_dim\n",
    "        self.dropout_pct = dropout_pct\n",
    "        self.dense_dim   = dense_dim\n",
    "        self.l2_penalty  = l2_penalty\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=hidden_dim,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='random_uniform',\n",
    "            dropout = dropout_pct,\n",
    "            kernel_regularizer=regularizers.l2(l2=l2_penalty))\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation='gelu',\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='random_uniform',\n",
    "                        kernel_regularizer=regularizers.l2(l2=l2_penalty)),\n",
    "                layers.Dropout(dropout_pct),\n",
    "                layers.Dense(hidden_dim, activation='linear',\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            bias_initializer='random_uniform',\n",
    "                            kernel_regularizer=regularizers.l2(l2=l2_penalty)),\n",
    "                layers.Dropout(dropout_pct),\n",
    "                ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        attention_output = self.attention(query=inputs, key=inputs, value=inputs)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\":  self.num_heads,\n",
    "            \"hidden_dim\": self.hidden_dim,\n",
    "            \"dropout_pct\": self.dropout_pct,\n",
    "            \"dense_dim\":  self.dense_dim,\n",
    "            \"l2_penalty\": self.l2_penalty,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def buildLoadingOutputs(inputs: Layer, num_assets: int, hidden_dim: int, l2_penalty: float,\n",
    "                        dropout_pct: float, num_heads: int, dense_dim: int, num_factors: int) -> Layer:\n",
    "    \"\"\"\n",
    "    Build loading outputs for the Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "        inputs      (Layer): Input layer for the encoder.\n",
    "        num_assets  (int):   Number of assets (entities).\n",
    "        hidden_dim  (int):   Dimension of the hidden layers.\n",
    "        l2_penalty  (float): L2 regularization penalty.\n",
    "        dropout_pct (float): Dropout rate as a percentage.\n",
    "        num_heads   (int):   Number of attention heads.\n",
    "        dense_dim   (int):   Dimension of the dense layers.\n",
    "        num_factors (int):   Number of factors in the model.\n",
    "\n",
    "    Returns:\n",
    "        Layer: Output layer of the loading outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    encoder_outputs = []\n",
    "    for _ in range(num_assets):\n",
    "        encoder_output = layers.Dense(hidden_dim, activation='linear',\n",
    "                                    kernel_initializer='glorot_uniform',\n",
    "                                    bias_initializer='random_uniform',\n",
    "                                    kernel_regularizer=regularizers.l2(l2=l2_penalty))(inputs)\n",
    "        encoder_output = layers.Dropout(dropout_pct)(encoder_output)\n",
    "        encoder_output = TransformerEncoder(num_heads, hidden_dim, dropout_pct, dense_dim, l2_penalty)(encoder_output)\n",
    "        encoder_output = layers.GlobalAveragePooling2D()(encoder_output)\n",
    "        encoder_output = layers.Dense(num_factors, activation='linear',\n",
    "                                    kernel_initializer='glorot_uniform',\n",
    "                                    bias_initializer='random_uniform',\n",
    "                                    kernel_regularizer=regularizers.l2(l2=l2_penalty))(encoder_output)\n",
    "        encoder_outputs.append(encoder_output)\n",
    "\n",
    "    # Stack the outputs and reshape to a matrix of dim num_assets by num_factors\n",
    "    outputs = layers.Concatenate(axis=1)(encoder_outputs)  \n",
    "    output  = layers.Reshape((num_assets, num_factors))(outputs)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def buildFactorOutputs(inputs: Layer, hidden_dim: int, l2_penalty: float, dropout_pct: float,\n",
    "                        num_heads: int, dense_dim: int, num_factors: int) -> Layer:\n",
    "    \"\"\"\n",
    "    Build factor outputs for the Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "        inputs      (Layer): Input layer for the encoder.\n",
    "        hidden_dim  (int):   Dimension of the hidden layers.\n",
    "        l2_penalty  (float): L2 regularization penalty.\n",
    "        dropout_pct (float): Dropout rate as a percentage.\n",
    "        num_heads   (int):   Number of attention heads.\n",
    "        dense_dim   (int):   Dimension of the dense layers.\n",
    "        num_factors (int):   Number of factors in the model.\n",
    "\n",
    "    Returns:\n",
    "        Layer: Output layer of the factor outputs.\n",
    "    \"\"\"\n",
    "    encoder_output = layers.Dense(hidden_dim, activation='linear',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='random_uniform',\n",
    "                                kernel_regularizer=regularizers.l2(l2=l2_penalty))(inputs)\n",
    "    encoder_output = layers.Dropout(dropout_pct)(encoder_output)\n",
    "    encoder_output = TransformerEncoder(num_heads, hidden_dim, dropout_pct, dense_dim, l2_penalty)(encoder_output)\n",
    "    encoder_output = layers.GlobalAveragePooling1D()(encoder_output)\n",
    "    output = layers.Dense(num_factors, activation='linear',\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        bias_initializer='random_uniform')(encoder_output)\n",
    "    return output\n",
    "\n",
    "def buildTransformer(num_chars: int, num_macro_vectors: int, \n",
    "    num_assets: int, num_training_obs: int, \n",
    "    hps_dict: Dict) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Build and compile a Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "        num_chars (int): Number of characteristics.\n",
    "        num_macro_vectors (int): Number of macroeconomic vectors.\n",
    "        num_assets (int): Number of assets (entities).\n",
    "        num_training_obs (ints): Number of observations in the training data for this model.\n",
    "        hps_dict (Dict): hyperparameter values.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Compiled Transformer model.\n",
    "    \"\"\"\n",
    "    # Set dense dim to be two times that of hidden dimension\n",
    "    dense_dim = int(hps_dict['hidden_dim']*2)\n",
    "    \n",
    "    # Build inputs\n",
    "    input_loadings = keras.Input(shape=(hps_dict['num_lags'], num_chars, num_assets + num_macro_vectors))\n",
    "    input_factors = keras.Input(shape=(hps_dict['num_lags'], num_assets))\n",
    "\n",
    "    # Build mappings to outputs\n",
    "    output_loadings = buildLoadingOutputs(\n",
    "        input_loadings, num_assets, hps_dict['hidden_dim'],\n",
    "        hps_dict['l2_penalty'], hps_dict['dropout_pct'], \n",
    "        hps_dict['num_heads'], dense_dim, hps_dict['number_factors'])\n",
    "    output_factors = buildFactorOutputs(\n",
    "        input_factors, hps_dict['hidden_dim'],\n",
    "        hps_dict['l2_penalty'], hps_dict['dropout_pct'], \n",
    "        hps_dict['num_heads'], dense_dim, hps_dict['number_factors'])\n",
    "    output = layers.Dot(axes=[2, 1])([output_loadings, output_factors])\n",
    "\n",
    "    # Build optimizer\n",
    "    decay_steps = int(num_training_obs / hps_dict['batch_size'])\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        hps_dict['initial_learning_rate'],\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=hps_dict['learning_decay_rate'],\n",
    "        staircase=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule,\n",
    "        beta_1=hps_dict['adam_beta_1'], beta_2=hps_dict['adam_beta_2'], \n",
    "        clipnorm=hps_dict['adam_clipnorm'])\n",
    "\n",
    "    # Build and compile model\n",
    "    model = keras.Model(inputs=[input_loadings, input_factors], outputs=output)\n",
    "    model.compile(optimizer=optimizer, \n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mse'], \n",
    "        weighted_metrics=['mse'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50baeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitTransformer(model: keras.Model,\n",
    "        train_loading_input: tf.Tensor, train_factor_input: tf.Tensor, train_output: tf.Tensor, \n",
    "        rel_y_df: pd.DataFrame, lhs_col: str, lhs_pad: int, tc_per_hour: float, num_prtfl_qntls: int,\n",
    "        train_datetimes: List[np.datetime64], num_assets: int, \n",
    "        hps_dict: Dict,\n",
    "        val_loading_input: tf.Tensor = None, val_factor_input: tf.Tensor = None, val_output: tf.Tensor = None, \n",
    "    ) -> Tuple[keras.Model, int, float, float]:\n",
    "    \"\"\"\n",
    "    Fit the Transformer model.\n",
    "\n",
    "    Parameters: \n",
    "        model (keras.Model): The Transformer model to be trained.\n",
    "        train_loading_input (tf.Tensor): Training loading input tensor.\n",
    "        train_factor_input (tf.Tensor): Training factor input tensor.\n",
    "        train_output (tf.Tensor): Training output tensor.\n",
    "        rel_y_df (pd.DataFrame):\n",
    "        lhs_col (str): Name of LHS column.\n",
    "        lhs_pad (int): The padded value for the lhs to signify missing.\n",
    "        tc_per_hour (float): Transaction costs per hour in simple return.\n",
    "        num_prtfl_qntls (int): Number of quantiles for long-short portfolio construction.\n",
    "        train_datetimes (List[np.datetime64]): date at which validation period begins.\n",
    "        num_assets (int): Number of assets (entities).\n",
    "        hps_dict (Dict): hyperparameter values.\n",
    "        val_loading_input (tf.Tensor, optional): Validation loading input tensor.\n",
    "        val_factor_input (tf.Tensor, optional): Validation factor input tensor.\n",
    "        val_output (tf.Tensor, optional): Validation output tensor.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: A tuple containing the trained model, number of epochs trained,\n",
    "                training r^2 predictive, and training geometric average return.\n",
    "    \"\"\"\n",
    "    # Build sample weights to down weight hours where lots of missing asset-hours\n",
    "    # NOTE: this will downweight past obs relevant to recent obs\n",
    "    sample_weight_mat = tf.cast(train_output != lhs_pad, dtype=tf.int16)\n",
    "    sample_weight_array = np.sum(sample_weight_mat, axis=1) / num_assets\n",
    "\n",
    "    # Fit the model\n",
    "    callbacks = []\n",
    "    if hps_dict['early_stopping']:\n",
    "        es = EarlyStopping(monitor='val_mse', mode='min', verbose=1, patience=5)\n",
    "        callbacks.append(es)\n",
    "\n",
    "    if val_loading_input is not None and val_factor_input is not None and val_output is not None:\n",
    "        validation_data = ([val_loading_input, val_factor_input], val_output)\n",
    "    else:\n",
    "        validation_data = None\n",
    "\n",
    "    model.fit(x=[train_loading_input, train_factor_input],\n",
    "                y=train_output,\n",
    "                batch_size=hps_dict['batch_size'], epochs=hps_dict['num_epochs'], verbose=1, callbacks=callbacks,\n",
    "                validation_data=validation_data,\n",
    "                sample_weight=sample_weight_array)\n",
    "\n",
    "    num_epochs_trained = es.stopped_epoch if hps_dict['early_stopping'] else hps_dict['num_epochs']\n",
    "\n",
    "    # Predict on the training data to return the training r^2_pred for obs with nonmissing return\n",
    "    train_yhats = model.predict([train_loading_input, train_factor_input])\n",
    "    train_mask = train_output != lhs_pad\n",
    "    train_r2_pred = (1 \n",
    "        - np.mean(np.square(train_output[train_mask] - train_yhats[train_mask])) \n",
    "            / np.mean(np.square(train_output[train_mask] - 0)))\n",
    "\n",
    "    # Form array of training yhats\n",
    "    train_yhats_array = tf.reshape(train_yhats, [-1]).numpy()\n",
    "\n",
    "    # Form DataFrame of y and yhat values for nonmissing returns in training window\n",
    "    train_pos_df = rel_y_df[rel_y_df.date.isin(train_datetimes)].copy()\n",
    "    train_pos_df['yhats'] = train_yhats_array\n",
    "    train_pos_df = train_pos_df[train_pos_df[lhs_col] != lhs_pad].reset_index(drop=True)\n",
    "\n",
    "    # Calculate the geom avg return of given quantile long short portfolios\n",
    "    train_pos_df = QuantTools.formPortfolioPositionsQuantileLongShort(\n",
    "            train_pos_df, num_prtfl_qntls)\n",
    "    train_pos_df['returns'] = train_pos_df.position*train_pos_df[lhs_col]\n",
    "    returns = train_pos_df.groupby('date')['returns'].sum()\n",
    "    train_geom_mean_rtrn = QuantTools.calcGeomAvg(returns-tc_per_hour)\n",
    "    \n",
    "    return model, num_epochs_trained, train_r2_pred, train_geom_mean_rtrn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291af01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(y_df: pd.DataFrame, char_df: pd.DataFrame, ts_df: pd.DataFrame, \n",
    "    asset_universe_dict: Dict[str, List],\n",
    "    val_start_date: str, val_end_date: str, test_start_date: str, lhs_col: str,\n",
    "    lhs_pad: int, rhs_pad: int, num_prtfl_qntls: int, tc_per_hour: float,\n",
    "    hp_grid: Dict[str, list], periods_in_year: int, \n",
    "    cv_out_fp: str, arch_name: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Run custom step forward cross-validation.\n",
    "\n",
    "    This function evaluates the performance of the Transformer-based factor model. \n",
    "    It uses the input data and hyperparameter grid to train multiple models with \n",
    "    different hyperparameter combinations and evaluates their performance using \n",
    "    step-forward cross-validation. The function outputs the results to a csv and\n",
    "    returns a list of dictionaries containing the evaluation results for each model.\n",
    "\n",
    "    Parameters:\n",
    "        y_df (pd.DataFrame): DataFrame containing the target variable data.\n",
    "        char_df (pd.DataFrame): DataFrame containing the characteristic data.\n",
    "        ts_df (pd.DataFrame): DataFrame containing the previous return data.\n",
    "        asset_universe_dict (Dict[str, List]): Dictionary containing the asset universe \n",
    "                                                for each month in the study period.\n",
    "        val_start_date (str): Start date of the validation period in 'YYYY-MM-DD' format.\n",
    "        val_end_date (str): End date of the validation period in 'YYYY-MM-DD' format.\n",
    "        test_start_date (str): Start date of the test period in 'YYYY-MM-DD' format.\n",
    "        lhs_col (str): The name of the target variable (lhs) column in y_df.\n",
    "        lhs_pad (int): The value to pad missing lhs values with.\n",
    "        rhs_pad (int): The value to pad missing rhs values with.\n",
    "        num_prtfl_qntls (int): Number of quantiles for long-short portfolio construction.\n",
    "        tc_per_hour (float): Transaction cost per hour for calculating returns.\n",
    "        hp_grid (Dict[str, list]): Hyperparameter grid to search for the best model.\n",
    "        periods_in_year (int): Number of periods in a year for annualization.\n",
    "        cv_out_fp (str): Filepath to save the cross-validation results in CSV format.\n",
    "        arch_name (str): Name of the architecture/model being tested.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries containing evaluation results for each model \n",
    "            in the hyperparameter grid. Each dictionary includes the model's hyperparameters, \n",
    "            evaluation metrics, and other relevant information.\n",
    "    \"\"\"\n",
    "    # Subset the LHS and RHS to remove test period\n",
    "    y_df = y_df[y_df.date < test_start_date].copy()\n",
    "    char_df = char_df[char_df.date < test_start_date].copy()\n",
    "    ts_df = ts_df[ts_df.date < test_start_date].copy()\n",
    "\n",
    "    # Initialize cv result objects\n",
    "    results_list = []\n",
    "\n",
    "    # Determine RHS column names\n",
    "    char_cols = list(char_df.columns.values)\n",
    "    char_cols.remove('date')\n",
    "    char_cols.remove('asset')\n",
    "    macro_cols = [col for col in ts_df.columns if 'macro' in col]\n",
    "    ret_lag_cols = [col for col in ts_df.columns if 'asset_r_' in col]\n",
    "\n",
    "    # Determine number of RHS values\n",
    "    num_chars  = len(char_cols)\n",
    "    num_macro_vectors = int(len(macro_cols)/len(char_cols))\n",
    "\n",
    "    # Determine validation datetimes to loop over and datetimes to refit at\n",
    "    val_dts_dict = {}\n",
    "    val_datetimes = np.unique(y_df[y_df.date>=val_start_date].date.values)\n",
    "    val_sun_midnights = np.unique(y_df[(y_df.date>=val_start_date) \n",
    "        & (y_df.date.dt.hour==0) & (y_df.date.dt.day_of_week==6)].date.values)\n",
    "\n",
    "    # Check if first val date is sunday midnight, if not then add the dates\n",
    "    first_val_date = np.min(y_df[(y_df.date==val_start_date)].date.values)\n",
    "    day_of_week_of_first_val_datetime = (first_val_date.astype('datetime64[D]').view('int64') - 4) % 7\n",
    "    if day_of_week_of_first_val_datetime != 6:\n",
    "        val_dts_dict[first_val_date] = np.unique(y_df[(y_df.date>=first_val_date) & (y_df.date<val_sun_midnights[0])].date.values)\n",
    "\n",
    "    # Complete the dictionary with all the sundays as keys as the dates until the next sunday as the values\n",
    "    for val_sun_midnight in val_sun_midnights:\n",
    "        next_sun_midnight = val_sun_midnight + np.timedelta64(7, 'D')\n",
    "        val_dts_dict[val_sun_midnight] = np.unique(y_df[(y_df.date>=val_sun_midnight) \n",
    "                                            & (y_df.date<next_sun_midnight)\n",
    "                                            & (y_df.date<test_start_date)].date.values)\n",
    "\n",
    "    # Loop over hp combinations\n",
    "    keys = hp_grid.keys()\n",
    "    values = hp_grid.values()\n",
    "    hp_combos = list(itertools.product(*values))\n",
    "    for hps in hp_combos:\n",
    "        # Start the timer\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        # Create hp dictionary and other objects for this iteration\n",
    "        hps_dict = dict(zip(keys, hps))\n",
    "        hps_results_dict = hps_dict.copy()\n",
    "        val_y_yhats_df = pd.DataFrame()\n",
    "\n",
    "        # Report on progress\n",
    "        print(hps_dict)\n",
    "\n",
    "        # Initiate lists for results and start the loop over the val dates to fit and predict\n",
    "        num_epochs_trained_list   = []\n",
    "        train_r2_pred_list        = []\n",
    "        train_geom_mean_rtrn_list = []\n",
    "        num_model_params_list     = []\n",
    "        for val_datetime_start in list(val_dts_dict.keys()): \n",
    "            print(val_datetime_start)\n",
    "            # form training and validation datetime objects\n",
    "            train_datetimes = list(ts_df[ts_df.date < val_datetime_start].date.values)[hps_dict['num_lags']-1:]\n",
    "            val_datetimes_window = val_dts_dict[val_datetime_start]\n",
    "            val_datetime_end = np.max(val_datetimes_window)\n",
    "\n",
    "            # form appropriate asset universe and update num asset parameter\n",
    "            first_day_of_month_for_current_val_dt = np.datetime_as_string(val_datetime_start, unit='M')+'-01'\n",
    "            asset_universe = asset_universe_dict[first_day_of_month_for_current_val_dt]\n",
    "            num_assets = len(asset_universe)\n",
    "\n",
    "            # figure out what assets are not included in this asset universe to drop from previous return df\n",
    "            prev_ret_cols_to_drop = [col for col in ts_df.columns \n",
    "                                    if (col != 'date') & ('asset_r_' in col) \n",
    "                                    if col.split('_')[2] not in asset_universe]\n",
    "\n",
    "            # for all dfs, cut down assets and form relevant dataframes of up to end of current val week\n",
    "            rel_y_df    = y_df[(y_df.asset.isin(asset_universe))\n",
    "                                & ((y_df.date <= val_datetime_end))].copy()\n",
    "            rel_char_df = char_df[(char_df.asset.isin(asset_universe))\n",
    "                                & (char_df.date <= val_datetime_end)].copy()\n",
    "            rel_ts_df   = ts_df[(ts_df.date <= val_datetime_end)].drop(columns=prev_ret_cols_to_drop, axis=1)\n",
    "\n",
    "            # form rel prev asset return col names\n",
    "            prev_asset_ret_cols = [col for col in rel_ts_df.columns if 'asset_r_' in col]\n",
    "\n",
    "            # normalize rhs data (note: this takes 2-15 min given big df's)\n",
    "            rel_char_df = normalizeAndFillMissing(rel_char_df, lhs_col,\n",
    "                            lhs_pad, rhs_pad, ignore_cols=['date', 'asset'])\n",
    "            rel_ts_df = normalizeAndFillMissing(rel_ts_df, lhs_col, \n",
    "                            lhs_pad, rhs_pad, ignore_cols='date')\n",
    "\n",
    "            # form training and validation data\n",
    "            train_loading_input, train_factor_input, train_output = formLhsAndRhsTensors(rel_y_df, rel_char_df, rel_ts_df,\n",
    "                                                                        train_datetimes, prev_asset_ret_cols, macro_cols,\n",
    "                                                                        lhs_col, hps_dict['num_lags'], num_assets, num_chars)\n",
    "            val_loading_input, val_factor_input, val_output = formLhsAndRhsTensors(rel_y_df, rel_char_df, rel_ts_df,\n",
    "                                                                        val_datetimes_window, prev_asset_ret_cols, macro_cols,\n",
    "                                                                        lhs_col, hps_dict['num_lags'], num_assets, num_chars)\n",
    "\n",
    "            # Check that no assets with missing returns in the validation data\n",
    "            assert(0 == np.sum(val_output==lhs_pad))\n",
    "\n",
    "            # Fit and predict\n",
    "            num_training_obs = train_output.shape[0]\n",
    "            model = buildTransformer(\n",
    "                num_chars, num_macro_vectors, num_assets, num_training_obs, hps_dict)\n",
    "            model, num_epochs_trained, train_r2_pred, train_geom_mean_rtrn = fitTransformer(\n",
    "                model, train_loading_input, train_factor_input, train_output,\n",
    "                rel_y_df, lhs_col, lhs_pad, tc_per_hour, num_prtfl_qntls, \n",
    "                train_datetimes, num_assets, hps_dict,\n",
    "                val_loading_input, val_factor_input, val_output)\n",
    "            val_yhats = model.predict([val_loading_input, val_factor_input])\n",
    "            val_yhats_array = tf.reshape(val_yhats, [-1]).numpy()\n",
    "\n",
    "            # Save this val week returns\n",
    "            num_epochs_trained_list.append(num_epochs_trained)\n",
    "            train_r2_pred_list.append(train_r2_pred)\n",
    "            train_geom_mean_rtrn_list.append(train_geom_mean_rtrn)\n",
    "            num_model_params_list.append(model.count_params())\n",
    "            temp_yhats_df = rel_y_df[rel_y_df.date >= val_datetime_start].reset_index(drop=True).copy()\n",
    "            temp_yhats_df['yhats'] = val_yhats_array\n",
    "            val_y_yhats_df = pd.concat([val_y_yhats_df, temp_yhats_df])\n",
    "\n",
    "            # Output this week's results\n",
    "            if True:\n",
    "                val_week_df = val_y_yhats_df[(val_y_yhats_df.date>=val_datetime_start) \n",
    "                                        & (val_y_yhats_df.date<=val_datetime_end)].copy()\n",
    "                val_week_df = val_week_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "                val_week_df = QuantTools.formPortfolioPositionsQuantileLongShort(val_week_df, num_prtfl_qntls)\n",
    "                val_week_df['returns'] = val_week_df.position*val_week_df[lhs_col]\n",
    "                val_week_y = val_week_df[lhs_col].values\n",
    "                val_week_yhats = val_week_df['yhats'].values\n",
    "                val_week_returns = (val_week_df[val_week_df.position !=0].groupby('date')['returns'].sum().values\n",
    "                    - tc_per_hour)\n",
    "                val_week_r_2_pred = 1-np.mean(np.square(val_week_y - val_week_yhats))/np.mean(np.square(val_week_y))\n",
    "                print(f'\\n this week r 2 pred: {val_week_r_2_pred}')\n",
    "                print(f'this week geom avg ret {QuantTools.calcGeomAvg(val_week_returns)} \\n')\n",
    "        \n",
    "        # Stop the timer after this hp grid point is completed\n",
    "        toc = time.perf_counter()\n",
    "\n",
    "        # For this hp point, add metadata to the results dict\n",
    "        hps_results_dict['arch_name'] = arch_name\n",
    "        hps_results_dict['val_start_date'] = val_start_date\n",
    "        hps_results_dict['val_end_date'] = val_end_date\n",
    "        hps_results_dict['runtime'] = round((toc - tic)/60, 0) \n",
    "\n",
    "        # Add training period statistics\n",
    "        hps_results_dict['avg_epochs_trained'] = np.mean(num_epochs_trained_list)\n",
    "        hps_results_dict['avg_num_model_params'] = np.mean(num_model_params_list)\n",
    "        hps_results_dict['train_r2_pred_min'] = np.min(train_r2_pred_list)\n",
    "        hps_results_dict['train_r2_pred_mean'] = np.mean(train_r2_pred_list)\n",
    "        hps_results_dict['train_r2_pred_max'] = np.max(train_r2_pred_list)\n",
    "        hps_results_dict['train_geom_mean_rtrn_min'] = np.min(train_geom_mean_rtrn_list)\n",
    "        hps_results_dict['train_geom_mean_rtrn_mean'] = np.mean(train_geom_mean_rtrn_list)\n",
    "        hps_results_dict['train_geom_mean_rtrn_max'] = np.max(train_geom_mean_rtrn_list)\n",
    "\n",
    "        # Obtain validation period results\n",
    "        assert(0 == val_y_yhats_df.isnull().sum().sum()), \"Missing observations in the validation period.\"\n",
    "        val_y_yhats_df = val_y_yhats_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "        val_y_yhats_pos_df = QuantTools.formPortfolioPositionsQuantileLongShort(\n",
    "            val_y_yhats_df, num_prtfl_qntls)\n",
    "        val_yhats = val_y_yhats_pos_df.yhats.values\n",
    "        val_ys    = val_y_yhats_pos_df[lhs_col].values\n",
    "        val_y_yhats_pos_df['returns'] = val_y_yhats_pos_df.position*val_y_yhats_pos_df[lhs_col]\n",
    "        returns = (val_y_yhats_pos_df.groupby('date')['returns'].sum().values \n",
    "            - tc_per_hour)\n",
    "        assert len(val_yhats) == len(val_ys)\n",
    "\n",
    "        # Form validation period statistics\n",
    "        hps_results_dict['val_mse']       = np.mean(np.square(val_ys-val_yhats))\n",
    "        hps_results_dict['val_r2_pred']   = 1-np.mean(np.square(val_ys-val_yhats))/np.mean(np.square(val_ys))\n",
    "        hps_results_dict['val_yhat_min']  = np.min(val_yhats)\n",
    "        hps_results_dict['val_yhat_q1']   = np.quantile(val_yhats, q=0.25)\n",
    "        hps_results_dict['val_yhat_q2']   = np.quantile(val_yhats, q=0.5)\n",
    "        hps_results_dict['val_yhat_mean'] = np.mean(val_yhats)\n",
    "        hps_results_dict['val_yhat_q3']   = np.quantile(val_yhats, q=0.75)\n",
    "        hps_results_dict['val_yhat_max']  = np.max(val_yhats)\n",
    "        hps_results_dict['geom_mean_1h']  = QuantTools.calcGeomAvg(returns)\n",
    "        hps_results_dict['sharpe_annual'] = QuantTools.calcSharpe(returns, periods_in_year=periods_in_year)\n",
    "        hps_results_dict['sd_annual']     = QuantTools.calcSD(returns, periods_in_year=periods_in_year)\n",
    "        hps_results_dict['max_dd']        = QuantTools.calcMaxDrawdown(returns)\n",
    "        hps_results_dict['avg_turnover']  = QuantTools.calcTSAvgTurnover(val_y_yhats_pos_df)\n",
    "\n",
    "        # Save results to return\n",
    "        results_list.append(hps_results_dict)\n",
    "\n",
    "        # For this hp, save results to csv\n",
    "        cv_df = pd.DataFrame(results_list)\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = cv_out_fp + '-' + arch_name + '-' + timestr + '.csv'\n",
    "        cv_df.to_csv(fp, index=False)\n",
    "    \n",
    "    # Return cv results\n",
    "    return results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6953a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    IN_FP           = '../data/clean/panel_train.pkl'\n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    CV_OUT_FP       = '../output/high_dim_fm/cv_results'\n",
    "    TEST_OUT_FP     = '../data/clean/test_yhats_transformer.pkl'\n",
    "    LHS_COL         = 'r_ex_tp1'\n",
    "    VAL_START_DATE  = '2021-01-01'\n",
    "    VAL_END_DATE    = '2021-01-31'\n",
    "    TEST_START_DATE = '2021-02-01' # TODO: UP NEXT IS FEB+MARCH TOGETHER\n",
    "    PERIODS_IN_YEAR = int(365.25*24)\n",
    "    ARCH_NAME       = 'transfomer'\n",
    "    LHS_PAD         = -2\n",
    "    RHS_PAD         = -2\n",
    "    NUM_PRTFL_QNTLS = 20\n",
    "    TC_PER_HOUR     = 10.5e-4\n",
    "    HP_GRID         = {'number_factors': [1],\n",
    "        'num_lags': [2],                 # NOTE: 3 or 2\n",
    "        'hidden_dim': [16],              # NOTE: 32, 16, 8\n",
    "        'num_heads': [2],                # NOTE: 4, 2, 1 (powers of 2)\n",
    "        'l2_penalty': [1e-4],            # NOTE: 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1\n",
    "        'dropout_pct': [0.1],            # NOTE: 0.1, 0.2, 0.4\n",
    "        'initial_learning_rate': [2e-3], # NOTE: 1e-3, 6e-4, 3e-4, 1e-4\n",
    "        'learning_decay_rate': [0.95],   # NOTE: 0.8, 0.9, 0.95; calc out what itll be across epochs given decay_steps\n",
    "        'adam_beta_1': [0.9],            # NOTE: 0.95, 0.9, 0.8\n",
    "        'adam_beta_2': [0.999],          # NOTE: 0.9995, 0.999, 0.99\n",
    "        'adam_clipnorm': [100],\n",
    "        'batch_size': [64],\n",
    "        'num_epochs': [100],\n",
    "        'early_stopping': [True]}\n",
    "    \n",
    "    # read in data\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    all_df = pd.read_pickle(IN_FP)\n",
    "    \n",
    "    # subset rows and columns and separate input and output data\n",
    "    y_df, char_df, ts_df = subsetRowsAndColumns(all_df, LHS_COL)\n",
    "    gc.collect()\n",
    "\n",
    "    # pad the lhs data\n",
    "    y_df = normalizeAndFillMissing(y_df, \n",
    "            lhs_col=LHS_COL, lhs_pad=LHS_PAD, rhs_pad=RHS_PAD, \n",
    "            ignore_cols=['date', 'asset', LHS_COL])\n",
    "\n",
    "    # run custom step forward cross validation\n",
    "    cv_results_list = runCV(y_df, char_df, ts_df, asset_universe_dict, \n",
    "        VAL_START_DATE, VAL_END_DATE, TEST_START_DATE,\n",
    "        LHS_COL, LHS_PAD, RHS_PAD, NUM_PRTFL_QNTLS, TC_PER_HOUR,\n",
    "        HP_GRID, PERIODS_IN_YEAR, CV_OUT_FP, ARCH_NAME)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f14a76dcff07d5b93f2c0fc65ce65c8ac7788ddb1ef5c63daa3feefa016fa519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
