{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "944cf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dot\n",
    "from joblib import Parallel, delayed\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed6bbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropRowsAndColsForCA(df: pd.DataFrame, lhs_col: str) -> pd.DataFrame:\n",
    "   # drop 2018-2019 and 2022 data\n",
    "   # - '18-'19 does not have enough assets\n",
    "   # - 2022 is oos for now\n",
    "   df = df[~df.date.dt.year.isin([2018, 2019, 2022])].reset_index(drop=True)\n",
    "\n",
    "   # Set characteristics of interest\n",
    "   selected_rhs = ['char_addr_active_tm1h', 'char_bidask_t', 'char_circulation_dormant_tm365', \n",
    "      'char_delta_flow_dist_tm1h', 'char_dex_prct_circ_supply_t', 'char_exchange_inflow_tm1h', \n",
    "      'char_exchange_outflow_tm1h', 'char_exchange_prct_circ_supply_t', 'char_mvrv_t', \n",
    "      'char_num_pairs_t', 'char_prct_supply_in_profit_t', 'char_r_ath_t', 'char_r_atl_t', 'char_r_industry_tm6h', \n",
    "      'char_r_max_tm12h', 'char_r_tm1h', 'char_sent_volume_consumed_tm1',\n",
    "      'char_size_realized_t', 'char_social_volume_tm7', 'char_trades_t', \n",
    "      'char_turnover_tm7', 'char_tx_volume_t', 'char_vol_tm1']\n",
    "\n",
    "   # Cut to characteristics columns of interest\n",
    "   df = df[['date', 'asset', lhs_col]+selected_rhs]\n",
    "\n",
    "   # Note: keep obs to RHS ratio roughly 4e4:1\n",
    "\n",
    "   # Note: for any macro, take cartesian product with characteristics to make it characteritisc level\n",
    "   # -or do the reg thing to reduce it down to same dim as number of assets\n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e12308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPortfolioReturnCovariates(df: pd.DataFrame, lhs_col: str) -> pd.DataFrame:\n",
    "    # Obtain the datetimes of the dataframe\n",
    "    df = df.sort_values(by = 'date')\n",
    "    datetimes = np.unique(df.date.values)\n",
    "\n",
    "    # Form new covariate names\n",
    "    characteristics = list(df.columns.values)\n",
    "    characteristics.remove('date')\n",
    "    characteristics.remove('asset')\n",
    "    characteristics.remove(lhs_col)\n",
    "    new_covars = ['x_' + char[5:] for char in characteristics]\n",
    "\n",
    "    # Loop over all datetimes\n",
    "    for current_dt in datetimes: \n",
    "        # Obtain the datetime's LHS \"tomorrow\" returns and the covariates\n",
    "        r_tp1 = df[df.date == current_dt].r_ex_tp1.values\n",
    "        z_t  = df[df.date == current_dt][characteristics].values\n",
    "        \n",
    "        # Calculate the characteristic managed portfolio returns\n",
    "        design = np.linalg.inv(np.matmul(np.transpose(z_t), z_t))\n",
    "        x_tp1    = np.matmul(np.matmul(design, np.transpose(z_t)), r_tp1)\n",
    "        \n",
    "        # Set the new columns to this week's vector's value\n",
    "        df.loc[df.date == current_dt, new_covars] = x_tp1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df66ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAutoencoder(b_covars, x_covars, \n",
    "    number_hidden_layer, l1_penalty, weight_initializer, bias_initializer, number_factor, learning_rate):\n",
    "    # Build the betas model from the time t covariates\n",
    "    model_b = tf.keras.models.Sequential()\n",
    "    model_b.add(tf.keras.Input(shape=(len(b_covars),)))\n",
    "    for j in range(number_hidden_layer):\n",
    "        model_b.add(Dense(16*1/(2**(j)), activation='relu',\n",
    "                        kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                        kernel_initializer=weight_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "        model_b.add(BatchNormalization())\n",
    "    model_b.add(Dense(number_factor, activation='linear',\n",
    "                    kernel_initializer=weight_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "\n",
    "    # Form the x model from time t plus 1 returns\n",
    "    model_x = tf.keras.models.Sequential()\n",
    "    model_x.add(tf.keras.Input(shape=(len(x_covars),)))\n",
    "    model_x.add(Dense(number_factor, activation='linear',\n",
    "                    kernel_initializer=weight_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "\n",
    "    # Form the dot product output for the combination of the two neurals\n",
    "    mergedOut = Dot(axes=(1,1))([model_b.output, model_x.output])\n",
    "\n",
    "    # Form the entire model\n",
    "    model = Model([model_b.input, model_x.input], mergedOut)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fitAutoencoder(train_df: pd.DataFrame, \n",
    "    hps_dict: dict, val_df: pd.DataFrame=None, lhs_col: str='r_ex_tp1', rhs_cols: list=[], num_cpus: int=-1) -> list:\n",
    "    # Obtain beta and factor side covariates\n",
    "    b_covars = [covar for covar in rhs_cols if covar[:4] == 'char']\n",
    "    x_covars = [covar for covar in rhs_cols if covar[:2] == 'x_']\n",
    "    assert set(rhs_cols) == (set(b_covars).union(set(x_covars)))\n",
    "\n",
    "    # Extract the hyperparameters\n",
    "    number_hidden_layer = hps_dict['num_hidden_layers']\n",
    "    number_factor       = hps_dict['number_factors']\n",
    "    learning_rate       = hps_dict['learning_rates']\n",
    "    l1_penalty          = hps_dict['l1_penalties']\n",
    "    batch_size          = hps_dict['batch_sizes']\n",
    "    number_ensemble     = hps_dict['num_ensemble']\n",
    "    epoch               = hps_dict['epochs']\n",
    "    early_stopping      = hps_dict['early_stopping']\n",
    "\n",
    "    # Loop over the ensembles to build models for each\n",
    "    models = []\n",
    "    assert(number_ensemble <= 10), 'whatcha think you got infinite come pew ters'\n",
    "    for i in range(0, number_ensemble):\n",
    "        # Obtain the training input and output data and, if passed, validation data\n",
    "        train_b = train_df[b_covars]\n",
    "        train_x = train_df[x_covars]  \n",
    "        train_y = train_df[[lhs_col]]\n",
    "        if val_df is not None:\n",
    "            val_b = val_df[b_covars]\n",
    "            val_x = val_df[x_covars]  \n",
    "            val_y = val_df[[lhs_col]]\n",
    "\n",
    "        # According to which model in the ensemble it is, initialize parameters.\n",
    "        random.seed(i*42)\n",
    "        initializer_list = [initializers.HeNormal(seed=i), \n",
    "                            initializers.GlorotUniform(seed=i), \n",
    "                            initializers.RandomUniform(seed=i)]\n",
    "        initializer_pair = random.sample(initializer_list, 2)\n",
    "        weight_initializer = initializer_pair[0]\n",
    "        bias_initializer   = initializer_pair[1]\n",
    "\n",
    "        # Build the model\n",
    "        model = buildAutoencoder(b_covars, x_covars, number_hidden_layer, \n",
    "            l1_penalty, weight_initializer, bias_initializer, number_factor, learning_rate)\n",
    "\n",
    "        # Prepare early stopping object \n",
    "        es = EarlyStopping(monitor='val_mse', mode='min', verbose=2, patience = 5) \n",
    "\n",
    "        # Fit the model\n",
    "        with tf.device('/CPU:0'):\n",
    "            if early_stopping == True:\n",
    "                model.fit(x=[train_b, train_x], y=train_y, \n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=([val_b, val_x], val_y), \n",
    "                            epochs=epoch, verbose=2,\n",
    "                            workers=20, callbacks=[es])  # TODO CHANGE WORKERS TO 1 WHEN I WRAP UP\n",
    "            else:\n",
    "                model.fit(x=[train_b, train_x], y=train_y, \n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epoch, verbose=2,\n",
    "                            workers=20)  # TODO CHANGE WORKERS TO 1 WHEN I WRAP UP\n",
    "                \n",
    "        models.append(model)\n",
    "\n",
    "    # build the time window for the training data to predict on\n",
    "    first_datetime = np.min(train_df.date.values)\n",
    "    month = np.datetime64(first_datetime, 'M').astype(int) % 12 + 1\n",
    "    year = np.datetime64(first_datetime, 'Y').astype(int) + 1970\n",
    "    oos_start_date = np.min(train_df[(train_df.date.dt.year==year) \n",
    "                        & (train_df.date.dt.month==month+1)].date.values)\n",
    "    oos_end_date   = np.max(train_df.date.values)\n",
    "\n",
    "    # fit on the training data for all models to report the r2_pred\n",
    "    train_yhats = genAutoencoderYhats(train_df, oos_start_date, oos_end_date, number_factor, num_cpus)\n",
    "    train_ys    = train_df[train_df.date>=oos_start_date][lhs_col].values\n",
    "    train_r2_pred = 1-(np.mean(np.square(train_ys-train_yhats)))/(np.mean(np.square(train_ys)))\n",
    "\n",
    "    return models, train_r2_pred\n",
    "\n",
    "def genAutoencoderYhats(df, oos_start_date, oos_end_date, number_factor, num_cpus) -> np.array:\n",
    "    # Obtain beta and factor side covariates\n",
    "    b_covars = [covar for covar in rhs_cols if covar[:4] == 'char']\n",
    "    x_covars = [covar for covar in rhs_cols if covar[:2] == 'x_']\n",
    "    assert set(rhs_cols) == (set(b_covars).union(set(x_covars)))\n",
    "\n",
    "    # Obtain the oos data\n",
    "    oos_df = df[(df.date >= oos_start_date) & (df.date <= oos_end_date)].copy()\n",
    "    oos_x  = oos_df[x_covars]\n",
    "    oos_b  = oos_df[b_covars]\n",
    "\n",
    "    # Form each model's results\n",
    "    b_hats = np.zeros((oos_df.shape[0], number_factor, len(models)))\n",
    "    lambda_hats = np.zeros((oos_df.shape[0], number_factor, len(models)))\n",
    "    all_dates = np.unique(df[(df.date <= oos_end_date)].date)\n",
    "    oos_dates = np.unique(oos_df.date.values)\n",
    "    for i in range(len(models)):\n",
    "        print(i)\n",
    "        # Update the model to use\n",
    "        model = models[i]\n",
    "\n",
    "        # Form the beta hats\n",
    "        layer_name = model.layers[-3]._name  \n",
    "        assert(model.layers[-3].output_shape[1] == number_factor)\n",
    "        b_hat_layer = Model(inputs=model.input[0],\n",
    "                            outputs=model.get_layer(layer_name).output)\n",
    "        b_hat = b_hat_layer.predict(oos_b, verbose=0)\n",
    "        b_hats[:,:,i] = b_hat\n",
    "\n",
    "        # Form the sample average of the estimated factors up to each oos date\n",
    "        # build this model's mapping from input to f hat\n",
    "        model = models[i]\n",
    "        layer_name = model.layers[-2]._name \n",
    "        assert(model.layers[-2].output_shape[1] == number_factor)\n",
    "        f_hat_layer = Model(inputs=model.input[1],\n",
    "                            outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "        # estimate this model's f hats for all dates before the oos dates\n",
    "        x = df[df.date < oos_start_date][x_covars]\n",
    "        f_hat = f_hat_layer.predict(x, verbose=0)\n",
    "        assert(all(np.isclose(f_hat[0,:], f_hat[1,:])))\n",
    "        f_hats = np.sum(f_hat, axis=0)\n",
    "\n",
    "        # obtain the f hats for the entire oos period\n",
    "        x = df[df.date >= oos_start_date][x_covars]\n",
    "        f_hat = f_hat_layer.predict(x, verbose=0)\n",
    "        assert(all(np.isclose(f_hat[0,:], f_hat[1,:])))\n",
    "\n",
    "        # determine the lambda hat for each oos date\n",
    "        lambda_hat_index_start = 0\n",
    "        for t in range(len(oos_dates)):\n",
    "            # update this oos date \n",
    "            oos_date = oos_dates[t]\n",
    "\n",
    "            # update the fhats with the appropriate f_hat values\n",
    "            # -update start index in here so this is skipped on first run but occurs\n",
    "            #  on every other before we update the end index two lines below\n",
    "            if t != 0:\n",
    "                f_hats += np.sum(f_hat[lambda_hat_index_start:lambda_hat_index_end,:], axis=0)\n",
    "                lambda_hat_index_start = lambda_hat_index_end\n",
    "\n",
    "            # determine how many obs are in this oos date\n",
    "            num_rows_in_oos_dt = df[df.date==oos_date].shape[0]\n",
    "            \n",
    "            # update the end index given the number of oos obs for this date\n",
    "            lambda_hat_index_end = lambda_hat_index_start + num_rows_in_oos_dt\n",
    "\n",
    "            # divide by total number of f_hats added together to figure out TS average for this oos_date\n",
    "            #     save as this time period's and this model's lambda hat\n",
    "            lambda_hats[lambda_hat_index_start:lambda_hat_index_end, :, i] = (\n",
    "                np.tile(f_hats / df[df.date<oos_date].shape[0], (num_rows_in_oos_dt, 1)))\n",
    "            \n",
    "    # Form model predictions of beta hats times lambda hats where\n",
    "    #     we take dot product between two factor length vectors for all time periods and models\n",
    "    #     and then average each model's forecast to return a vector of length of oos dataframe\n",
    "    yhats = np.mean(np.sum(b_hats * lambda_hats, axis=1), axis=1)\n",
    "\n",
    "    return yhats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f20bb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotYvsYhat(y: np.ndarray, yhats: np.ndarray, num_quantiles: int = 10) -> None:\n",
    "    \"\"\" Plot the average values of y (returns) against bins of yhat (predicted returns).\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): A vector of target variable.\n",
    "        yhats (np.ndarray): A vector of fitted values.\n",
    "        num_quantiles (int): The number of quantiles to bin yhats into. Default is 10.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with y and yhats\n",
    "    temp_df = pd.DataFrame({'y': y, 'yhat': yhats})\n",
    "\n",
    "    # Bin yhats into quantiles\n",
    "    temp_df['yhat'] = pd.qcut(temp_df['yhat'], q=num_quantiles, labels=False)\n",
    "\n",
    "    # Group y by yhat and plot the means\n",
    "    temp_df.groupby('yhat')['y'].mean().plot()\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title('Mean Return vs. Quantiles of Predicted Returns')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0acab751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenTestYhats(df, opt_hps, test_year=2021): \n",
    "    test_weeks = np.unique(df[df.index.year == test_year].index.values)\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "    for test_week in test_weeks:\n",
    "        # TODO DO SOME LOGIC TO FIGURE OUT THE WINDOWS TO FIT A MODEL FOR SO I LOOP OVER THOSE INITIAL DATES\n",
    "        # -fit teh model once\n",
    "        # -then predict on all subsequent weeks\n",
    "        # -maybe one month at time?\n",
    "        # -maybe one week at a time?\n",
    "        # -let's try once a month to see how it does\n",
    "        print(test_week, '\\n')\n",
    "        temp_df  = df[df.index <= test_week].copy()\n",
    "        temp_df  = subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week=test_week)\n",
    "        train_df = temp_df[temp_df.index < test_week].copy()\n",
    "        oos_df   = temp_df[temp_df.index == test_week].copy()\n",
    "        \n",
    "        models   = fitAutoencoder(train_df, opt_hps, val_df=None, early_stopping=False)\n",
    "        yhats    = genYhats(oos_df, models, test_week, opt_hps['number_factor'])\n",
    "       \n",
    "        oos_df = oos_df[['asset', 'r_tplus7']]\n",
    "        oos_df['yhat'] = yhats\n",
    "        test_df = pd.concat((test_df, oos_df))\n",
    "        rw_mse = np.mean(np.square(test_df.r_tplus7.values))\n",
    "        model_mse = np.mean(np.square(test_df.r_tplus7.values - test_df.yhat.values))\n",
    "        print('\\n test random walk mse: ' + str(rw_mse))\n",
    "        print('\\n test model mse: ' + str(model_mse))\n",
    "        print('winning?: ' + str(model_mse < rw_mse))\n",
    "        print('\\n')\n",
    "    \n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39c19e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTestPeriod(df: pd.DataFrame,\n",
    "    lhs_col: str,\n",
    "    test_start_date: str,\n",
    "    test_end_date: str,\n",
    "    opt_model_dict: dict,\n",
    "    num_cpus: int) -> np.array:\n",
    "    ''' predict for test period using backtested model.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): panel of training and test data with date index, LHS variable named \n",
    "                           `lhs_col`, and remaining cols are RHS features.\n",
    "        lhs_col (str): name of the lhs target column.\n",
    "        test_start_date (str): the first date for the test period.\n",
    "        test_end_date (str): the last date for the test period.\n",
    "        opt_model_dict (dict): hyperparameters for optimal backtested model.\n",
    "        num_cpus (int): number of cpus to use when parallelizing.\n",
    "    \n",
    "    Returns:\n",
    "        yhats (np.array): vector of yhat positions for entire test period.\n",
    "    '''\n",
    "    # form list of rhs col\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # form lhs for oos df\n",
    "    oos_df = df.copy()\n",
    "    ret_threshold = opt_model_dict['ret_threshold']\n",
    "    oos_df['y'] = 1\n",
    "    oos_df.loc[oos_df[lhs_col] > ret_threshold, 'y'] = 2\n",
    "    oos_df.loc[oos_df[lhs_col] < -ret_threshold, 'y'] = 0\n",
    "\n",
    "    # form test dates to loop over\n",
    "    test_dates = np.unique(oos_df[test_start_date:test_end_date].index.values)\n",
    "\n",
    "    def loopOverTestDates(test_date):\n",
    "        # form train and val data\n",
    "        temp_df   = oos_df[oos_df.index <= test_date].copy()\n",
    "        train_df  = temp_df[temp_df.index < test_date].copy()\n",
    "        train_y_btc_eth_diff = train_df[lhs_col].values\n",
    "        train_df  = train_df.drop(lhs_col, axis=1)\n",
    "        test_df   = temp_df[temp_df.index == test_date].copy()\n",
    "        test_df   = test_df.drop(lhs_col, axis=1)\n",
    "\n",
    "        # fit and predict\n",
    "        model, train_acc, train_yhats = fitRF(train_df, 'y', rhs_cols, \n",
    "                                            hps=opt_model_dict, \n",
    "                                            ys_real=train_y_btc_eth_diff)\n",
    "        yhats = genRFYhats(test_df, model, 'y', rhs_cols)\n",
    "\n",
    "        return yhats, model, train_acc, train_yhats\n",
    "\n",
    "    # predict for entire test period\n",
    "    test_results = Parallel(n_jobs=int(num_cpus/4))(delayed(loopOverTestDates)(test_date) for test_date in test_dates)\n",
    "\n",
    "    # extract test yhats to return\n",
    "    yhats_list = []\n",
    "    for t in range(len(test_results)):\n",
    "        yhats_list.append(test_results[t][0])\n",
    "    yhats = np.array(yhats_list)-1\n",
    "\n",
    "    return yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b24e4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportTestPeriodResults(yhats: np.array, y_real: np.array):\n",
    "    ''' report various portfolio statistics for the test period.\n",
    "    \n",
    "    Args:\n",
    "        yhats (np.array): vector of actual portfolio positions.\n",
    "        y_real (np.array): vector of real return difference for target instrument.\n",
    "    '''\n",
    "    # calculate returns post transaction costs\n",
    "    tc = calcTransactionCosts(yhats)\n",
    "    returns = yhats*y_real - tc\n",
    "    \n",
    "    # calc portfolio statistics\n",
    "    geom_mean = calcGeomAvg(returns)\n",
    "    sharpe    = calcSharpe(returns, periods_in_year=365*12)\n",
    "    max_dd    = calcMaxDrawdown(returns)\n",
    "    max_1week = calcMaxOneWeekLoss(returns, periods_in_week=7*12)\n",
    "\n",
    "    # report\n",
    "    print(f\"Geometric average 2 hour return: {geom_mean:.6f}\")\n",
    "    print(f\"Annualized Sharpe: {sharpe:.3f}\")\n",
    "    print(f\"Maximum drawdown: {max_dd:.4f}\")\n",
    "    print(f\"Maximum loss over any one week period: {max_1week:.4f}\")\n",
    "\n",
    "    # plot classification\n",
    "    plotYvsYhat(y=y_real, yhats=yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461f0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set settings\n",
    "    #tf.config.set_visible_devices([], 'GPU') # TODO CONFIRM THIS RUNS IT ON GPU\n",
    "\n",
    "    # set args\n",
    "    IN_FP           = '../data/clean/panel_train.pkl'\n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    CV_OUT_FP       = '../output/cv_results'\n",
    "    LHS_COL         = 'r_ex_tp1'\n",
    "    VAL_START_DATE  = '2021-01-01'\n",
    "    VAL_END_DATE    = '2021-06-30'\n",
    "    TEST_START_DATE = '2021-07-01'\n",
    "    NUM_CPUS        = 22 # TODO change to 22 when doing on desktop\n",
    "    PERIODS_IN_YEAR = 365*24\n",
    "    HP_GRID         = {'num_hidden_layers': [3, 2, 1],\n",
    "        'number_factors': [6, 5, 4, 3, 2, 1],\n",
    "        'learning_rates': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'batch_sizes': [1024],\n",
    "        'l1_penalties': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "        'num_ensemble': [10],\n",
    "        'early_stopping': [True],\n",
    "        'epochs': [100]}\n",
    "    \n",
    "    # read in data\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    df = pd.read_pickle(IN_FP)\n",
    "\n",
    "    # drop rows and columns such that data will work for conditional autoencoder (CA)\n",
    "    df = dropRowsAndColsForCA(df, LHS_COL)\n",
    "\n",
    "    # form the char-sorted portfolios for factor side of CA input\n",
    "    df = formPortfolioReturnCovariates(df, LHS_COL) # NOTE: ~7 min runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbf0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO CV FUNCTION\n",
    "# def runCV(df: pd.DataFrame, asset_universe_dict: Dict[str, List])\n",
    "\n",
    "# WHAT TO RETURN:\n",
    "\n",
    "# ARGS TO PASS IN:\n",
    "df\n",
    "asset_universe_dict\n",
    "val_start_date = VAL_START_DATE\n",
    "val_end_date   = VAL_END_DATE\n",
    "test_start_date = TEST_START_DATE\n",
    "lhs_col = LHS_COL\n",
    "hp_grid = HP_GRID\n",
    "#model_params = PARAMS\n",
    "#fitModel = FUNC\n",
    "#predictModel = FUNC\n",
    "num_cpus = NUM_CPUS\n",
    "periods_in_year = PERIODS_IN_YEAR\n",
    "cv_out_fp = CV_OUT_FP\n",
    "arch_name = 'autoencoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ef263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to relevant data\n",
    "df = df[df.date < test_start_date].copy()\n",
    "\n",
    "# Initialize cv result objects\n",
    "results_list = []\n",
    "\n",
    "# Determine RHS columns\n",
    "rhs_cols = list(df.columns.values)\n",
    "rhs_cols.remove('date')\n",
    "rhs_cols.remove('asset')\n",
    "rhs_cols.remove(lhs_col)\n",
    "\n",
    "# Determine validation datetimes to loop over and datetimes to refit at\n",
    "val_dts_dict = {}\n",
    "val_datetimes = np.unique(df[df.date>=val_start_date].date.values)\n",
    "val_sun_midnights = np.unique(df[(df.date>=val_start_date) \n",
    "    & (df.date.dt.hour==0) & (df.date.dt.day_of_week==6)].date.values)\n",
    "\n",
    "# Check if first val date is sunday midnight, if not then add the dates\n",
    "first_val_date = np.min(df[(df.date==val_start_date)].date.values)\n",
    "day_of_week_of_first_val_datetime = (first_val_date.astype('datetime64[D]').view('int64') - 4) % 7\n",
    "if day_of_week_of_first_val_datetime != 6:\n",
    "    val_dts_dict[first_val_date] = np.unique(df[(df.date>=first_val_date) & (df.date<val_sun_midnights[0])].date.values)\n",
    "\n",
    "# Complete the dictionary with all the sundays as keys as the dates until the next sunday as the values\n",
    "for val_sun_midnight in val_sun_midnights:\n",
    "    next_sun_midnight = val_sun_midnight + np.timedelta64(7, 'D')\n",
    "    val_dts_dict[val_sun_midnight] = np.unique(df[(df.date>=val_sun_midnight) \n",
    "                                        & (df.date<next_sun_midnight)\n",
    "                                        & (df.date<test_start_date)].date.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ab5b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_hidden_layers': 3, 'number_factors': 6, 'learning_rates': 0.0001, 'batch_sizes': 1024, 'l1_penalties': 0.1, 'num_ensemble': 10, 'early_stopping': True, 'epochs': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 19:23:45.738561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5354 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 19:23:47.716909: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-21-at-0x274cf450 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.716989: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-22-at-0x2764fa70 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717057: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Trying to access resource Resource-21-at-0x274cf450 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "\t [[{{node StatefulPartitionedCall_12}}]]\n",
      "2023-05-02 19:23:47.717097: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-18-at-0x274a46e0 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717162: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Trying to access resource Resource-21-at-0x274cf450 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "\t [[{{node StatefulPartitionedCall_12}}]]\n",
      "\t [[GroupCrossDeviceControlEdges_0/AssignAddVariableOp_2/_177]]\n",
      "2023-05-02 19:23:47.717187: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-17-at-0x27483a80 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717215: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-26-at-0x274d0210 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717266: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-27-at-0x274c81e0 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717652: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-16-at-0x27486b30 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717692: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-15-at-0x27483ae0 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717785: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-11-at-0x274560d0 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717865: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-10-at-0x2743ab90 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717915: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-12-at-0x274402d0 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717957: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-6-at-0x27366760 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.717981: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-9-at-0x27436e80 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.718078: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-5-at-0x27331ee0 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.718671: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-4-at-0x27326dd0 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
      "2023-05-02 19:23:47.718731: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:347 : INVALID_ARGUMENT: Trying to access resource Resource-3-at-0x2740b4b0 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n",
      " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_12' defined at (most recent call last):\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2326477/1825436028.py\", line 35, in <module>\n      models, train_r2_pred = fitAutoencoder(train_df, hps_dict, val_df, lhs_col, rhs_cols, num_cpus)\n    File \"/tmp/ipykernel_2326477/1539199236.py\", line 85, in fitAutoencoder\n      model.fit(x=[train_b, train_x], y=train_y,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_12'\nDetected at node 'StatefulPartitionedCall_12' defined at (most recent call last):\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2326477/1825436028.py\", line 35, in <module>\n      models, train_r2_pred = fitAutoencoder(train_df, hps_dict, val_df, lhs_col, rhs_cols, num_cpus)\n    File \"/tmp/ipykernel_2326477/1539199236.py\", line 85, in fitAutoencoder\n      model.fit(x=[train_b, train_x], y=train_y,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_12'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Trying to access resource Resource-21-at-0x274cf450 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall_12}}]]\n\t [[GroupCrossDeviceControlEdges_0/AssignAddVariableOp_2/_177]]\n  (1) INVALID_ARGUMENT:  Trying to access resource Resource-21-at-0x274cf450 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall_12}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2630]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m     34\u001b[0m \u001b[39m# fit and predict\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m models, train_r2_pred \u001b[39m=\u001b[39m fitAutoencoder(train_df, hps_dict, val_df, lhs_col, rhs_cols, num_cpus)\n\u001b[1;32m     36\u001b[0m yhats \u001b[39m=\u001b[39m genAutoencoderYhats(df, \n\u001b[1;32m     37\u001b[0m     val_datetime_start, val_datetime_end, hps_dict[\u001b[39m'\u001b[39m\u001b[39mnumber_factors\u001b[39m\u001b[39m'\u001b[39m], num_cpus)\n\u001b[1;32m     39\u001b[0m \u001b[39m# save the results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 85\u001b[0m, in \u001b[0;36mfitAutoencoder\u001b[0;34m(train_df, hps_dict, val_df, lhs_col, rhs_cols, num_cpus)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/CPU:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m early_stopping \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m         model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m[train_b, train_x], y\u001b[39m=\u001b[39;49mtrain_y, \n\u001b[1;32m     86\u001b[0m                     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     87\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m([val_b, val_x], val_y), \n\u001b[1;32m     88\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepoch, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     89\u001b[0m                     workers\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[es])  \u001b[39m# TODO CHANGE WORKERS TO 1 WHEN I WRAP UP\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m         model\u001b[39m.\u001b[39mfit(x\u001b[39m=\u001b[39m[train_b, train_x], y\u001b[39m=\u001b[39mtrain_y, \n\u001b[1;32m     92\u001b[0m                     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     93\u001b[0m                     epochs\u001b[39m=\u001b[39mepoch, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     94\u001b[0m                     workers\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)  \u001b[39m# TODO CHANGE WORKERS TO 1 WHEN I WRAP UP\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_12' defined at (most recent call last):\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2326477/1825436028.py\", line 35, in <module>\n      models, train_r2_pred = fitAutoencoder(train_df, hps_dict, val_df, lhs_col, rhs_cols, num_cpus)\n    File \"/tmp/ipykernel_2326477/1539199236.py\", line 85, in fitAutoencoder\n      model.fit(x=[train_b, train_x], y=train_y,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_12'\nDetected at node 'StatefulPartitionedCall_12' defined at (most recent call last):\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2326477/1825436028.py\", line 35, in <module>\n      models, train_r2_pred = fitAutoencoder(train_df, hps_dict, val_df, lhs_col, rhs_cols, num_cpus)\n    File \"/tmp/ipykernel_2326477/1539199236.py\", line 85, in fitAutoencoder\n      model.fit(x=[train_b, train_x], y=train_y,\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_12'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Trying to access resource Resource-21-at-0x274cf450 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall_12}}]]\n\t [[GroupCrossDeviceControlEdges_0/AssignAddVariableOp_2/_177]]\n  (1) INVALID_ARGUMENT:  Trying to access resource Resource-21-at-0x274cf450 (defined @ /home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer_utils.py:134) located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall_12}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2630]"
     ]
    }
   ],
   "source": [
    "# Loop over hp combinations\n",
    "keys = hp_grid.keys()\n",
    "values = hp_grid.values()\n",
    "hp_combos = list(itertools.product(*values))\n",
    "for hps in hp_combos:\n",
    "    # Start the timer\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    # Create hp dictionary and other objects for this iteration\n",
    "    hps_dict = dict(zip(keys, hps))\n",
    "    hps_results_dict = hps_dict.copy()\n",
    "    val_y_yhats_df = df[(df.date >= val_start_date) & (df.date < test_start_date)][['date', 'asset', lhs_col]].copy()\n",
    "\n",
    "    # Report on progress\n",
    "    print(hps_dict)\n",
    "\n",
    "    # Define function to loop over\n",
    "    train_r2_pred_list = []\n",
    "    for val_datetime_start in list(val_dts_dict.keys()):\n",
    "        # form end of this window\n",
    "        val_datetime_end = np.max(val_dts_dict[val_datetime_start])\n",
    "\n",
    "        # form appropriate asset universe\n",
    "        first_day_of_month_for_current_val_dt = np.datetime_as_string(val_datetime_start, unit='M')+'-01'\n",
    "        asset_universe = asset_universe_dict[first_day_of_month_for_current_val_dt]\n",
    "\n",
    "        # form train and val dataframes\n",
    "        temp_df = df[(df.date<=val_datetime_end) & (df.asset.isin(asset_universe))].copy()\n",
    "        train_df = temp_df[temp_df.date<val_datetime_start].copy()\n",
    "        val_df = temp_df[(temp_df.date>=val_datetime_start) & (temp_df.date<=val_datetime_end)].copy()\n",
    "        del temp_df\n",
    "        gc.collect()\n",
    "\n",
    "        # fit and predict\n",
    "        models, train_r2_pred = fitAutoencoder(train_df, hps_dict, val_df, lhs_col, rhs_cols, num_cpus)\n",
    "        yhats = genAutoencoderYhats(df, \n",
    "            val_datetime_start, val_datetime_end, hps_dict['number_factors'], num_cpus)\n",
    "\n",
    "        # save the results\n",
    "        train_r2_pred_list.append(train_r2_pred)\n",
    "        temp_yhats_df = val_df[['date', 'asset']].copy()\n",
    "        temp_yhats_df['yhats'] = yhats\n",
    "        val_y_yhats_df = val_y_yhats_df.merge(temp_yhats_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Stop the timer\n",
    "    toc = time.perf_counter()\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d01dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef78b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -form metadata: val_start_date, val_end_date, arch_name, runtime\n",
    "# -form training stats: some avg of the r2_pred\n",
    "# -form val period stats: r2_pred, geom avg ret, sharpe_anual, std_annual, ts_avg_annual, avg_yhat, min, q1, q2, q3, max yhat,  max dd, avg turnover\n",
    "\n",
    "# -save the cv results to csv\n",
    "\n",
    "# return the list of dictionaries of results\n",
    "# return results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffa693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hps_yhats_dict['yhats'] = np.append(hps_yhats_dict['yhats'], yhats)\n",
    "hps_yhats_dict['ys']    = np.append(hps_yhats_dict['ys'], ys)\n",
    "\n",
    "val_ys_todate = hps_yhats_dict['ys']\n",
    "rw_val_mse    = np.mean(np.square(val_ys_todate))\n",
    "model_val_mse = np.mean(np.square(val_ys_todate - hps_yhats_dict['yhats']))\n",
    "#             print('\\n val random walk mse: ' + str(rw_val_mse))\n",
    "#             print('\\n val model mse: ' + str(model_val_mse))\n",
    "#             print('\\n val model mse winning?: ' + str(model_val_mse < rw_val_mse))\n",
    "#             print('\\n\\n')\n",
    "# TODO WRITE FUNC HERE TO REPORT SIMPLE STUFF LIKE DIST OF PREDICTED RETURNS AND PREDICTIVE R^2\n",
    "# Save run time and space out result print out\n",
    "toc = time.perf_counter()\n",
    "#         print('\\n\\n\\n')\n",
    "\n",
    "# Update lists of results\n",
    "# TODO WRITE FUNC TO REPORT MORE STUFF HERE FROM OTHER NOTEBOOKS\n",
    "# -including all the results that i will have for test period\n",
    "hps_yhats_dict_list.append(hps_yhats_dict)\n",
    "cv_results_dict = hps_yhats_dict.copy()\n",
    "del cv_results_dict['yhats']\n",
    "del cv_results_dict['ys']\n",
    "cv_results_dict['runtime_mins'] = round((toc - tic)/60, 0)\n",
    "cv_results_dict['mse'] = model_val_mse\n",
    "hps_mse_df_list.append(pd.DataFrame(cv_results_dict, index=[0]))\n",
    "\n",
    "# Save CV results\n",
    "cv_df = pd.concat(hps_mse_df_list, ignore_index=True)\n",
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "fp = '../4-output/cv-results-autoencoder-' + timestr + '.csv'\n",
    "cv_df.to_csv(fp, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extract validation periods results\n",
    "yhats_list = []\n",
    "models_list = []\n",
    "train_r2_pred_list = []\n",
    "train_yhats_list = []\n",
    "for t in range(len(val_results)):\n",
    "    yhats_list.append(val_results[t][0])\n",
    "    models_list.append(val_results[t][1])\n",
    "    train_r2_pred_list.append(val_results[t][2])\n",
    "    train_yhats_list.append(val_results[t][3])\n",
    "yhats = np.array(yhats_list)\n",
    "positions = np.sign(yhats)\n",
    "ys    = cv_df[val_start_date:][lhs_col].values\n",
    "results_dict['yhats'] = yhats\n",
    "\n",
    "# save results to master result list    \n",
    "results_list.append(results_dict)\n",
    "\n",
    "# save results to csv to monitor during cv\n",
    "toc = time.perf_counter()\n",
    "csv_dict = results_dict['hps'].copy()\n",
    "del results_dict\n",
    "csv_dict['arch_name'] = arch_name\n",
    "csv_dict['val_start_date'] = val_start_date\n",
    "csv_dict['val_end_date'] = np.datetime_as_string(np.max(cv_df.index.values))[:10]\n",
    "csv_dict['runtime_mins'] = round((toc - tic)/60, 0) \n",
    "csv_dict['mean_r2_pred_train'] = np.mean(np.array(train_r2_pred_list))\n",
    "csv_dict['r2_pred_val'] = 1-np.mean(np.square(ys-yhats))/np.mean(np.square(ys))\n",
    "tc = calcTransactionCosts(positions)\n",
    "returns = positions*ys - tc\n",
    "csv_dict['geom_mean_1h'] = calcGeomAvg(returns)\n",
    "csv_dict['sharpe_ann'] = calcSharpe(returns, periods_in_year=periods_in_year)\n",
    "csv_dict['sd_ann'] = calcSD(returns, annualized=True, periods_in_year=periods_in_year)\n",
    "csv_dict['ts_mean_ann'] = calcTSAvgReturn(returns, annualized=True, periods_in_year=periods_in_year)\n",
    "for lev in [2, 3, 4]:\n",
    "    lev_pos = positions*lev\n",
    "    lev_tc = calcTransactionCosts(lev_pos)\n",
    "    lev_returns = lev_pos*ys - lev_tc \n",
    "    csv_dict['geom_mean_x'+str(lev)] = calcGeomAvg(lev_returns)\n",
    "    csv_dict['sharpe_x'+str(lev)] = calcSharpe(lev_returns, periods_in_year=periods_in_year)\n",
    "    csv_dict['sd_ann_x'+str(lev)] = calcSD(lev_returns, annualized=True, periods_in_year=periods_in_year)\n",
    "    csv_dict['ts_mean_ann_x'+str(lev)] = calcTSAvgReturn(lev_returns, annualized=True, periods_in_year=periods_in_year)\n",
    "csv_dict_list.append(csv_dict)\n",
    "results_df = pd.DataFrame(csv_dict_list)\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "fp = out_fp+'_'+arch_name+'_'+timestr+'.csv'\n",
    "results_df.to_csv(fp, index=False)\n",
    "\n",
    "# output results to track\n",
    "print(csv_dict['runtime_mins'])\n",
    "print(f\"val r2_pred: {csv_dict['r2_pred_val']:.4f}\")\n",
    "print(f\"val sharpe: {csv_dict['sharpe_ann']:.4f}\")\n",
    "\n",
    "print('\\n\\n')\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "dd1180c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write to file the beta and factor side values each time i predict?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO THIS NOTEBOOK ENDS WITH SAVING A DF OF DATE, ASSET, PREDICTED RETURN FOR A VAL OR TEST PERIOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO GO SCOPE OLD NOTEBOOK FOR LATEST NUMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ALSO NEED TO TRY A MODEL WHERE WE SET MISSING ASSETS LHS \n",
    "# AND RHS TO -2 AND THEN WE HAVE BALANCED PANEL SO WE FIT ON THE \n",
    "# MATRIX OF RHS ACROSS ASSETS AS OPPOSED OT INDIVIDUAL DATE-ASSET \n",
    "# AND THEN THE NETWORK LEARNS ASSET-SPECIFIC PARAMETERS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
