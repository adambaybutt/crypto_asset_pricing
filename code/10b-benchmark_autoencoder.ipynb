{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944cf121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 18:56:58.281497: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-21 18:56:58.342297: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 18:56:59.261355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Ti, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dot\n",
    "from joblib import Parallel, delayed\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from tensorflow import keras\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d20fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGeomAvg(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the geometric average of a vector of simple returns.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar geometric average.\n",
    "    \"\"\"\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        raise TypeError(\"Input 'returns' must be a NumPy array\")\n",
    "    if annualized and periods_in_year is None:\n",
    "        raise ValueError(\"Input 'periods_in_year' must be provided if 'annualized' is True\")\n",
    "    geom_avg_at_given_freq = np.prod(1 + returns) ** (1 / np.size(returns)) - 1\n",
    "    return (geom_avg_at_given_freq + 1) ** periods_in_year - 1 if annualized else geom_avg_at_given_freq\n",
    "\n",
    "def calcTSAvgReturn(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float:\n",
    "    \"\"\" Calculate the time series mean return of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar time series mean return.\n",
    "    \"\"\"\n",
    "    mean_ret_at_given_freq = np.mean(returns)\n",
    "    if annualized == False:\n",
    "        return mean_ret_at_given_freq\n",
    "    else:\n",
    "        mean_ret = periods_in_year*mean_ret_at_given_freq\n",
    "        if mean_ret < -1:\n",
    "            return -1.\n",
    "        else:\n",
    "            return mean_ret\n",
    "\n",
    "def calcSD(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the standard deviation of a vector of simple returns with option to annualize.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar standard deviation.\n",
    "    \"\"\"\n",
    "    sd_at_given_freq = np.std(returns)\n",
    "    if annualized==False:\n",
    "        return sd_at_given_freq\n",
    "    else:\n",
    "        return np.sqrt(periods_in_year)*sd_at_given_freq\n",
    "\n",
    "def calcSharpe(returns: np.array,\n",
    "    periods_in_year: int,\n",
    "    risk_free_returns: np.array=None) -> float:\n",
    "    \"\"\" Calculate the annual Sharpe Ratio of a vector of simple returns. \n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "        risk_free_returns (np.array): vector of simple returns of the risk free rate.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar standard deviation.\n",
    "    \"\"\"\n",
    "    if risk_free_returns is not None:\n",
    "        returns = returns - risk_free_returns\n",
    "    \n",
    "    return (calcTSAvgReturn(returns, annualized=True, periods_in_year=periods_in_year) /\n",
    "            calcSD(returns, annualized=True, periods_in_year=periods_in_year))\n",
    "\n",
    "def calcMaxDrawdown(returns: np.array) -> float:\n",
    "    ''' calculate maximum drawdown for a vector of returns of any frequency.\n",
    "    \n",
    "    Args:\n",
    "        returns (np.array): vector of simple returns.\n",
    "    \n",
    "    Returns:\n",
    "        max_drawdown (float): maximum drawdown in simple return units over this period.\n",
    "    '''\n",
    "    # calculate the cumulative return as a new vector of the same length\n",
    "    cumulative_ret=(returns+1).cumprod()\n",
    "\n",
    "    # for every period, calc the historic maximum value of the portfolio \n",
    "    roll_max=pd.Series(cumulative_ret).rolling(len(cumulative_ret), min_periods=1).max()\n",
    "\n",
    "    # calc drawdown as the current portfolio value divided by the historic max value\n",
    "    dd=np.min(cumulative_ret/roll_max)\n",
    "    \n",
    "    # return simple return of max drawdown\n",
    "    return dd-1\n",
    "\n",
    "def formPortfolioPositionsQuantileLongShort(df: pd.DataFrame, quantile: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame with columns \"date\", \"asset\", and \"yhats\" and performs\n",
    "    the specified steps to form a new column \"position\" containing portfolio allocation percentages\n",
    "    that sum to 0 within date where we long the top quantile and short the bottom quantile.\n",
    "    The function then returns the modified DataFrame with the four columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): A pandas DataFrame containing columns \"date\", \"asset\", and \"yhats\".\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with an additional \"position\" column.\n",
    "    \"\"\"\n",
    "    # Randomly sort the DataFrame\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Sort the DataFrame by date and yhats\n",
    "    df = df.sort_values(by=['date', 'yhats'])\n",
    "\n",
    "    # Add a small random noise to yhats to handle potential duplicated values\n",
    "    df['yhats_noisy'] = df['yhats'] + np.random.uniform(-1e-10, 1e-10, size=len(df))\n",
    "\n",
    "    # Calculate quantiles\n",
    "    quantiles = df.groupby('date')['yhats_noisy'].transform(lambda x: pd.qcut(x, quantile, labels=False))\n",
    "\n",
    "    # Assign positions\n",
    "    df['position'] = np.where(quantiles == 0, -1, np.where(quantiles == quantile-1, 1, 0))\n",
    "\n",
    "    # Divide the positions column by how many assets are in each date\n",
    "    df['position'] = df.groupby(['date', 'position'])['position'].transform(lambda x: x / x.count())\n",
    "\n",
    "    # Check that the position column sums to 1 within each date\n",
    "    position_sums = df.groupby('date')['position'].sum()\n",
    "    assert all(np.isclose(position_sums, 0)), f\"Position column sums do not equal 1 for all dates: {position_sums[np.isclose(position_sums, 0)]}\"\n",
    "\n",
    "    # Drop the noisy yhats column\n",
    "    df = df.drop(columns=['yhats_noisy'])\n",
    "\n",
    "    # Sort the DataFrame by date and asset\n",
    "    df = df.sort_values(by=['date', 'asset'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def calcTSAvgTurnover(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    This function takes a pandas DataFrame with columns \"date\", \"asset\", and \"position\" and\n",
    "    calculates the average turnover, which is defined as the time series average of the percentage\n",
    "    of assets each date that do not have the same position (-1, 0, 1) as the previous date for that asset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): A pandas DataFrame containing columns \"date\", \"asset\", and \"position\".\n",
    "    \n",
    "    Returns:\n",
    "        float: The average turnover.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort the DataFrame by date and asset\n",
    "    df = df.sort_values(by=['date', 'asset'])\n",
    "\n",
    "    # Shift the position column to get the previous position for each asset\n",
    "    df['prev_position'] = df.groupby('asset')['position'].shift(1)\n",
    "\n",
    "    # Calculate the percentage of assets each date that do not have the same position as the previous date\n",
    "    df['position_changed'] = np.where(df['position'] != df['prev_position'], 1, 0)\n",
    "    turnover_pct = df.groupby('date')['position_changed'].mean()\n",
    "\n",
    "    # Calculate the time series average of the percentage of assets with changed positions\n",
    "    average_turnover = turnover_pct.mean()\n",
    "\n",
    "    return average_turnover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6bbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropRowsAndColsForCA(df: pd.DataFrame, lhs_col: str) -> pd.DataFrame:\n",
    "   # drop 2018-2019 and 2022 data\n",
    "   # - '18-'19 does not have enough assets\n",
    "   # - 2022 is oos for now\n",
    "   df = df[~df.date.dt.year.isin([2018, 2019, 2022])].reset_index(drop=True)\n",
    "\n",
    "   # Set characteristics of interest\n",
    "   selected_rhs = ['char_addr_new_log_delta_tm2_tm1',\n",
    "      'char_alpha_tm7',\n",
    "      'char_beta_tm7',\n",
    "      \"char_delta_flow_dist_tm1h\",\n",
    "      \"char_exchange_prct_circ_supply_t\",\n",
    "      \"char_illiq_tm7\",\n",
    "      \"char_r_ath_t\",\n",
    "      \"char_r_tm14\",\n",
    "      \"char_r_tm60\",\n",
    "      \"char_rank_cmc_t\",\n",
    "      \"char_sent_volume_consumed_tm1\",\n",
    "      \"char_vol_tm6h\"]\n",
    "\n",
    "   # Cut to characteristics columns of interest\n",
    "   df = df[['date', 'asset', lhs_col]+selected_rhs]\n",
    "\n",
    "   # Note: keep obs to RHS ratio roughly 4e4:1\n",
    "\n",
    "   # Note: for any macro, take cartesian product with characteristics to make it characteritisc level\n",
    "   # -or do the reg thing to reduce it down to same dim as number of assets\n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e12308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPortfolioReturnCovariates(df: pd.DataFrame, lhs_col: str) -> pd.DataFrame:\n",
    "    # Obtain the datetimes of the dataframe\n",
    "    df = df.sort_values(by = 'date')\n",
    "    datetimes = np.unique(df.date.values)\n",
    "\n",
    "    # Form new covariate names\n",
    "    characteristics = list(df.columns.values)\n",
    "    characteristics.remove('date')\n",
    "    characteristics.remove('asset')\n",
    "    characteristics.remove(lhs_col)\n",
    "    new_covars = ['x_' + char[5:] for char in characteristics]\n",
    "\n",
    "    # Loop over all datetimes\n",
    "    for current_dt in datetimes: \n",
    "        # Obtain the datetime's LHS \"tomorrow\" returns and the covariates\n",
    "        r_tp1 = df[df.date == current_dt].r_ex_tp1.values\n",
    "        z_t   = df[df.date == current_dt][characteristics].values\n",
    "        \n",
    "        # Calculate the characteristic managed portfolio returns\n",
    "        design = np.linalg.inv(np.matmul(np.transpose(z_t), z_t))\n",
    "        x_tp1  = np.matmul(np.matmul(design, np.transpose(z_t)), r_tp1)\n",
    "        \n",
    "        # Set the new columns to this week's vector's value\n",
    "        df.loc[df.date == current_dt, new_covars] = x_tp1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df66ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAutoencoder(b_covars, x_covars, \n",
    "    number_hidden_layer, l1_penalty, weight_initializer, bias_initializer, number_factor, learning_rate):\n",
    "    # Build the betas model from the time t covariates\n",
    "    model_b = tf.keras.models.Sequential()\n",
    "    model_b.add(tf.keras.Input(shape=(len(b_covars),)))\n",
    "    for j in range(number_hidden_layer):\n",
    "        model_b.add(Dense(16*1/(2**(j)), activation='relu',\n",
    "                        kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                        kernel_initializer=weight_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "        model_b.add(BatchNormalization())\n",
    "    model_b.add(Dense(number_factor, activation='linear',\n",
    "                    kernel_initializer=weight_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "\n",
    "    # Form the x model from time t plus 1 returns\n",
    "    model_x = tf.keras.models.Sequential()\n",
    "    model_x.add(tf.keras.Input(shape=(len(x_covars),)))\n",
    "    model_x.add(Dense(number_factor, activation='linear',\n",
    "                    kernel_initializer=weight_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "\n",
    "    # Form the dot product output for the combination of the two neurals\n",
    "    mergedOut = Dot(axes=(1,1))([model_b.output, model_x.output])\n",
    "\n",
    "    # Form the entire model\n",
    "    model = Model([model_b.input, model_x.input], mergedOut)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fitAutoencoder(train_df: pd.DataFrame, \n",
    "    hps_dict: dict, lhs_col: str='r_ex_tp1', rhs_cols: list=[], \n",
    "    val_df: pd.DataFrame=None, return_train_r2: bool=False) -> list:\n",
    "    # Obtain beta and factor side covariates\n",
    "    b_covars = [covar for covar in rhs_cols if covar[:4] == 'char']\n",
    "    x_covars = [covar for covar in rhs_cols if covar[:2] == 'x_']\n",
    "    assert set(rhs_cols) == (set(b_covars).union(set(x_covars)))\n",
    "\n",
    "    # Extract the hyperparameters\n",
    "    number_hidden_layer = hps_dict['num_hidden_layers']\n",
    "    number_factor       = hps_dict['number_factors']\n",
    "    learning_rate       = hps_dict['learning_rates']\n",
    "    l1_penalty          = hps_dict['l1_penalties']\n",
    "    batch_size          = hps_dict['batch_sizes']\n",
    "    number_ensemble     = hps_dict['num_ensemble']\n",
    "    epoch               = hps_dict['epochs']\n",
    "    early_stopping      = hps_dict['early_stopping']\n",
    "\n",
    "    # Loop over the ensembles to build models for each\n",
    "    models = []\n",
    "    num_epochs_trained = []\n",
    "    assert(number_ensemble <= 10), 'whatcha think you got infinite come pew ters'\n",
    "    for i in range(number_ensemble):\n",
    "        # Obtain the training input and output data and, if passed, validation data\n",
    "        train_b = train_df[b_covars]\n",
    "        train_x = train_df[x_covars]  \n",
    "        train_y = train_df[[lhs_col]]\n",
    "        if val_df is not None:\n",
    "            val_b = val_df[b_covars]\n",
    "            val_x = val_df[x_covars]  \n",
    "            val_y = val_df[[lhs_col]]\n",
    "\n",
    "        # According to which model in the ensemble it is, initialize parameters.\n",
    "        random.seed(i*42)\n",
    "        initializer_list = [initializers.HeNormal(seed=i), \n",
    "                            initializers.GlorotUniform(seed=i), \n",
    "                            initializers.RandomUniform(seed=i)]\n",
    "        initializer_pair = random.sample(initializer_list, 2)\n",
    "        weight_initializer = initializer_pair[0]\n",
    "        bias_initializer   = initializer_pair[1]\n",
    "\n",
    "        # Build the model\n",
    "        model = buildAutoencoder(b_covars, x_covars, number_hidden_layer, \n",
    "            l1_penalty, weight_initializer, bias_initializer, number_factor, learning_rate)\n",
    "\n",
    "        # Fit the model\n",
    "        with tf.device('/GPU:0'):\n",
    "            if early_stopping == True:\n",
    "                es = EarlyStopping(monitor='val_mse', mode='min', verbose=0, patience = 5) \n",
    "                \n",
    "                model.fit(x=[train_b, train_x], y=train_y, \n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=([val_b, val_x], val_y), \n",
    "                            epochs=epoch, verbose=1, callbacks=[es]) \n",
    "                \n",
    "                num_epochs = es.stopped_epoch\n",
    "                num_epochs_trained.append(num_epochs)\n",
    "            else:\n",
    "                model.fit(x=[train_b, train_x], y=train_y, \n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epoch, verbose=1)\n",
    "\n",
    "                num_epochs_trained.append(epoch)\n",
    "        \n",
    "        models.append(model)\n",
    "\n",
    "    if return_train_r2:\n",
    "        # build the time window for the training data to predict on\n",
    "        first_datetime = np.min(train_df.date.values)\n",
    "        month = np.datetime64(first_datetime, 'M').astype(int) % 12 + 1\n",
    "        year = np.datetime64(first_datetime, 'Y').astype(int) + 1970\n",
    "        oos_start_date = np.min(train_df[(train_df.date.dt.year==year) \n",
    "                            & (train_df.date.dt.month==month+1)].date.values)\n",
    "        oos_end_date   = np.max(train_df.date.values)\n",
    "\n",
    "        # fit on the training data for all models to report the r2_pred\n",
    "        train_yhats = genAutoencoderYhats(models, train_df, rhs_cols, oos_start_date, oos_end_date, number_factor)\n",
    "        train_ys    = train_df[train_df.date>=oos_start_date][lhs_col].values\n",
    "        train_r2_pred = 1-(np.mean(np.square(train_ys-train_yhats)))/(np.mean(np.square(train_ys)))\n",
    "    else:\n",
    "        train_r2_pred = 0\n",
    "    \n",
    "    return models, np.mean(num_epochs_trained), train_r2_pred\n",
    "\n",
    "def genAutoencoderYhats(models, in_df, rhs_cols, oos_start_date, oos_end_date, number_factor) -> np.array:\n",
    "    # Obtain beta and factor side covariates\n",
    "    b_covars = [covar for covar in rhs_cols if covar[:4] == 'char']\n",
    "    x_covars = [covar for covar in rhs_cols if covar[:2] == 'x_']\n",
    "    assert set(rhs_cols) == (set(b_covars).union(set(x_covars)))\n",
    "\n",
    "    # Obtain the oos data\n",
    "    oos_df = in_df[(in_df.date >= oos_start_date) & (in_df.date <= oos_end_date)].copy()\n",
    "    oos_b  = oos_df[b_covars]\n",
    "\n",
    "    # Form each model's results\n",
    "    b_hats = np.zeros((oos_df.shape[0], number_factor, len(models)))\n",
    "    lambda_hats = np.zeros((oos_df.shape[0], number_factor, len(models)))\n",
    "    oos_dates = np.unique(oos_df.date.values)\n",
    "    for i in range(len(models)):\n",
    "        # Update the model to use\n",
    "        model = models[i]\n",
    "\n",
    "        # Form the beta hats\n",
    "        layer_name = model.layers[-3]._name  \n",
    "        assert(model.layers[-3].output_shape[1] == number_factor)\n",
    "        b_hat_layer = Model(inputs=model.input[0],\n",
    "                            outputs=model.get_layer(layer_name).output)\n",
    "        b_hat = b_hat_layer.predict(oos_b, verbose=0)\n",
    "        b_hats[:,:,i] = b_hat\n",
    "\n",
    "        # Form the sample average of the estimated factors up to each oos date\n",
    "        # build this model's mapping from input to f hat\n",
    "        model = models[i]\n",
    "        layer_name = model.layers[-2]._name \n",
    "        assert(model.layers[-2].output_shape[1] == number_factor)\n",
    "        f_hat_layer = Model(inputs=model.input[1],\n",
    "                            outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "        # estimate this model's f hats for dates in month before the oos dates\n",
    "        x = in_df[(in_df.date >= (oos_start_date-np.timedelta64(30, 'D'))) \n",
    "                & (in_df.date < oos_start_date)][x_covars]\n",
    "        f_hat = f_hat_layer.predict(x, verbose=0).astype('float32')\n",
    "        assert(all(np.isclose(f_hat[0,:], f_hat[1,:])))\n",
    "        f_hats = np.sum(f_hat, axis=0)\n",
    "\n",
    "        # obtain the f hats for the entire oos period\n",
    "        x = in_df[in_df.date >= oos_start_date][x_covars]\n",
    "        f_hat = f_hat_layer.predict(x, verbose=0).astype('float32')\n",
    "        assert(all(np.isclose(f_hat[0,:], f_hat[1,:])))\n",
    "\n",
    "        # determine the lambda hat for each oos date\n",
    "        lambda_hat_index_start = 0\n",
    "        for t in range(len(oos_dates)):\n",
    "            # update this oos date \n",
    "            oos_date = oos_dates[t]\n",
    "\n",
    "            # update the fhats with the appropriate f_hat values\n",
    "            # -update start index in here so this is skipped on first run but occurs\n",
    "            #  on every other before we update the end index two lines below\n",
    "            if t != 0:\n",
    "                f_hats += np.sum(f_hat[lambda_hat_index_start:lambda_hat_index_end, :], axis=0)\n",
    "                lambda_hat_index_start = lambda_hat_index_end\n",
    "\n",
    "            # determine how many obs are in this oos date\n",
    "            num_rows_in_oos_dt = in_df[in_df.date==oos_date].shape[0]\n",
    "            \n",
    "            # update the end index given the number of oos obs for this date\n",
    "            lambda_hat_index_end = lambda_hat_index_start + num_rows_in_oos_dt\n",
    "\n",
    "            # divide by total number of f_hats added together to figure out TS average for this oos_date\n",
    "            #     save as this time period's and this model's lambda hat\n",
    "            lambda_hats[lambda_hat_index_start:lambda_hat_index_end, :, i] = (\n",
    "                np.tile(f_hats / in_df[(in_df.date >= (oos_start_date-np.timedelta64(30, 'D'))) \n",
    "                                        & (in_df.date<oos_date)].shape[0], \n",
    "                        (num_rows_in_oos_dt, 1)))\n",
    "\n",
    "    # Form model predictions of beta hats times lambda hats where\n",
    "    #     we take dot product between two factor length vectors for all time periods and models\n",
    "    #     and then average each model's forecast to return a vector of length of oos dataframe\n",
    "    yhats = np.mean(np.sum(b_hats * lambda_hats, axis=1), axis=1)\n",
    "\n",
    "    return yhats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f5a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df: pd.DataFrame, asset_universe_dict: Dict[str, List],\n",
    "    val_start_date: str, val_end_date: str, test_start_date: str, lhs_col: str,\n",
    "    hp_grid: Dict[str, list], periods_in_year: int, \n",
    "    cv_out_fp: str, arch_name: str) -> List[dict]:\n",
    "    # Subset to relevant data\n",
    "    df = df[df.date < test_start_date].copy()\n",
    "\n",
    "    # Initialize cv result objects\n",
    "    results_list = []\n",
    "\n",
    "    # Determine RHS columns\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove('date')\n",
    "    rhs_cols.remove('asset')\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # Determine validation datetimes to loop over and datetimes to refit at\n",
    "    val_dts_dict = {}\n",
    "    val_datetimes = np.unique(df[df.date>=val_start_date].date.values)\n",
    "    val_sun_midnights = np.unique(df[(df.date>=val_start_date) \n",
    "        & (df.date.dt.hour==0) & (df.date.dt.day_of_week==6)].date.values)\n",
    "\n",
    "    # Check if first val date is sunday midnight, if not then add the dates\n",
    "    first_val_date = np.min(df[(df.date==val_start_date)].date.values)\n",
    "    day_of_week_of_first_val_datetime = (first_val_date.astype('datetime64[D]').view('int64') - 4) % 7\n",
    "    if day_of_week_of_first_val_datetime != 6:\n",
    "        val_dts_dict[first_val_date] = np.unique(df[(df.date>=first_val_date) & (df.date<val_sun_midnights[0])].date.values)\n",
    "\n",
    "    # Complete the dictionary with all the sundays as keys as the dates until the next sunday as the values\n",
    "    for val_sun_midnight in val_sun_midnights:\n",
    "        next_sun_midnight = val_sun_midnight + np.timedelta64(7, 'D')\n",
    "        val_dts_dict[val_sun_midnight] = np.unique(df[(df.date>=val_sun_midnight) \n",
    "                                            & (df.date<next_sun_midnight)\n",
    "                                            & (df.date<test_start_date)].date.values)\n",
    "\n",
    "    # Loop over hp combinations\n",
    "    keys = hp_grid.keys()\n",
    "    values = hp_grid.values()\n",
    "    hp_combos = list(itertools.product(*values))\n",
    "    for hps in hp_combos:\n",
    "\n",
    "        # Start the timer\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        # Create hp dictionary and other objects for this iteration\n",
    "        hps_dict = dict(zip(keys, hps))\n",
    "        hps_results_dict = hps_dict.copy()\n",
    "        val_y_yhats_df = pd.DataFrame()\n",
    "\n",
    "        # Report on progress\n",
    "        print(hps_dict)\n",
    "\n",
    "        # Define function to loop over\n",
    "        avg_num_epochs_trained_list = []\n",
    "        train_r2_pred_list = []\n",
    "\n",
    "        for val_datetime_start in list(val_dts_dict.keys()): \n",
    "            print(val_datetime_start)\n",
    "            # form end of this window\n",
    "            val_datetime_end = np.max(val_dts_dict[val_datetime_start])\n",
    "\n",
    "            # form appropriate asset universe\n",
    "            first_day_of_month_for_current_val_dt = np.datetime_as_string(val_datetime_start, unit='M')+'-01'\n",
    "            asset_universe = asset_universe_dict[first_day_of_month_for_current_val_dt]\n",
    "\n",
    "            # form relevant date-assets given asset universe and the train and val dataframes\n",
    "            rel_df = df[(df.date<=val_datetime_end) & (df.asset.isin(asset_universe))].copy()\n",
    "            train_df = rel_df[rel_df.date<val_datetime_start].copy()\n",
    "            val_df = rel_df[(rel_df.date>=val_datetime_start) & (rel_df.date<=val_datetime_end)].copy()\n",
    "\n",
    "            # fit and predict\n",
    "            models, avg_num_epochs_trained, train_r2_pred = fitAutoencoder(\n",
    "                train_df, hps_dict, lhs_col, rhs_cols, val_df, return_train_r2=False)\n",
    "            yhats = genAutoencoderYhats(models, rel_df, rhs_cols,\n",
    "                val_datetime_start, val_datetime_end, hps_dict['number_factors'])\n",
    "            del rel_df, models\n",
    "            gc.collect()\n",
    "\n",
    "            # save the results\n",
    "            avg_num_epochs_trained_list.append(avg_num_epochs_trained)\n",
    "            train_r2_pred_list.append(train_r2_pred)\n",
    "            temp_yhats_df = val_df[['date', 'asset', lhs_col]].copy()\n",
    "            temp_yhats_df['yhats'] = yhats\n",
    "            val_y_yhats_df = pd.concat([val_y_yhats_df, temp_yhats_df])\n",
    "\n",
    "            # output this week r2_pred and return\n",
    "            val_week_df = val_y_yhats_df[(val_y_yhats_df.date>=val_datetime_start) \n",
    "                                        & (val_y_yhats_df.date<=val_datetime_end)].copy()\n",
    "            val_week_df = val_week_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "            val_week_df = formPortfolioPositionsQuantileLongShort(val_week_df, 5)\n",
    "            val_week_df['returns'] = val_week_df.position*val_week_df[lhs_col]\n",
    "            val_week_y = val_week_df[lhs_col].values\n",
    "            val_week_yhats = val_week_df['yhats'].values\n",
    "            val_week_returns = val_week_df[val_week_df.position !=0].groupby('date')['returns'].sum().values\n",
    "            print('this week r 2 pred:')\n",
    "            print(1-np.mean(val_week_y - val_week_yhats)/np.mean(val_week_y))\n",
    "            print(f'this week geom avg ret {calcGeomAvg(val_week_returns)}')\n",
    "\n",
    "        # Stop the timer\n",
    "        toc = time.perf_counter()\n",
    "\n",
    "        # For this hp, add metadata to results dict\n",
    "        hps_results_dict['avg_epochs_trained'] = np.mean(avg_num_epochs_trained_list)\n",
    "        hps_results_dict['val_start_date'] = val_start_date\n",
    "        hps_results_dict['val_end_date'] = val_end_date\n",
    "        hps_results_dict['arch_name'] = arch_name\n",
    "        hps_results_dict['runtime'] = round((toc - tic)/60, 0) \n",
    "\n",
    "        # For this hp, add training period statistics\n",
    "        hps_results_dict['train_r2_pred_min'] = np.min(train_r2_pred_list)\n",
    "        hps_results_dict['train_r2_pred_mean'] = np.mean(train_r2_pred_list)\n",
    "        hps_results_dict['train_r2_pred_max'] = np.max(train_r2_pred_list)\n",
    "\n",
    "        # For this hp, obtain the yhats and ys and positions\n",
    "        val_y_yhats_df = val_y_yhats_df.dropna()\n",
    "        val_y_yhats_df = val_y_yhats_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "        val_y_yhats_pos_df = formPortfolioPositionsQuantileLongShort(val_y_yhats_df, 5)\n",
    "        val_yhats = val_y_yhats_pos_df.yhats.values\n",
    "        val_ys    = val_y_yhats_pos_df[lhs_col].values\n",
    "        val_y_yhats_pos_df['returns'] = val_y_yhats_pos_df.position*val_y_yhats_pos_df[lhs_col]\n",
    "        returns = val_y_yhats_pos_df[val_y_yhats_pos_df.position != 0].groupby('date')['returns'].sum().values\n",
    "        assert len(val_yhats) == len(val_ys)\n",
    "\n",
    "        # For this hp, form validation period statistics\n",
    "        hps_results_dict['val_mse']       = np.mean(np.square(val_ys-val_yhats))\n",
    "        hps_results_dict['val_r2_pred']   = 1-np.mean(np.square(val_ys-val_yhats))/np.mean(np.square(val_ys))\n",
    "        hps_results_dict['val_yhat_min']  = np.min(val_yhats)\n",
    "        hps_results_dict['val_yhat_q1']   = np.quantile(val_yhats, q=0.25)\n",
    "        hps_results_dict['val_yhat_q2']   = np.quantile(val_yhats, q=0.5)\n",
    "        hps_results_dict['val_yhat_mean'] = np.mean(val_yhats)\n",
    "        hps_results_dict['val_yhat_q3']   = np.quantile(val_yhats, q=0.75)\n",
    "        hps_results_dict['val_yhat_max']  = np.max(val_yhats)\n",
    "        hps_results_dict['geom_mean_1h']  = calcGeomAvg(returns)\n",
    "        hps_results_dict['sharpe_annual'] = calcSharpe(returns, periods_in_year=periods_in_year)\n",
    "        hps_results_dict['sd_annual']     = calcSD(returns, periods_in_year=periods_in_year)\n",
    "        hps_results_dict['max_dd']        = calcMaxDrawdown(returns)\n",
    "        hps_results_dict['avg_turnover']  = calcTSAvgTurnover(val_y_yhats_pos_df)\n",
    "\n",
    "        # Save results to return\n",
    "        results_list.append(hps_results_dict)\n",
    "\n",
    "        # For this hp, save results to csv\n",
    "        cv_df = pd.DataFrame(results_list)\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = cv_out_fp + '-' + arch_name + '-' + timestr + '.csv'\n",
    "        cv_df.to_csv(fp, index=False)\n",
    "\n",
    "    # Return cv results\n",
    "    return results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9f8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTestPeriod(df: pd.DataFrame, asset_universe_dict: Dict[str, list],\n",
    "    test_start_date: str, lhs_col: str, opt_hps_dict: dict) -> pd.DataFrame:\n",
    "    # Confirm separate df formed\n",
    "    df = df.copy()\n",
    "\n",
    "    # Determine RHS columns\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove('date')\n",
    "    rhs_cols.remove('asset')\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # Determine test period datetimes to loop over and datetimes to refit at\n",
    "    test_dts_dict = {}\n",
    "    test_sun_midnights = np.unique(df[(df.date>=test_start_date) \n",
    "        & (df.date.dt.hour==0) & (df.date.dt.day_of_week==6)].date.values)\n",
    "\n",
    "    # Check if first test date is sunday midnight, if not then add the dates\n",
    "    first_test_datetime = np.min(df[(df.date==test_start_date)].date.values)\n",
    "    day_of_week_of_first_test_datetime = (first_test_datetime.astype('datetime64[D]').view('int64') - 4) % 7\n",
    "    if day_of_week_of_first_test_datetime != 6:\n",
    "        test_dts_dict[first_test_datetime] = np.unique(df[(df.date>=first_test_datetime) & (df.date<test_sun_midnights[0])].date.values)\n",
    "\n",
    "    # Complete the dictionary with all the sundays as keys as the dates until the next sunday as the testues\n",
    "    for test_sun_midnight in test_sun_midnights:\n",
    "        next_sun_midnight = test_sun_midnight + np.timedelta64(7, 'D')\n",
    "        test_dts_dict[test_sun_midnight] = np.unique(df[(df.date>=test_sun_midnight) \n",
    "                                            & (df.date<next_sun_midnight)].date.values)\n",
    "        \n",
    "    # Create dataframe of results to return\n",
    "    test_y_yhats_df = pd.DataFrame()\n",
    "\n",
    "    # Loop over all the datetimes in the test period where we want to refit the model\n",
    "    for test_datetime_start in list(test_dts_dict.keys()):\n",
    "        # Monitor progress\n",
    "        print('Currently fitting and predicting for the week starting: ')\n",
    "        print(test_datetime_start)\n",
    "\n",
    "        # Form end of this window\n",
    "        test_datetime_end = np.max(test_dts_dict[test_datetime_start])\n",
    "\n",
    "        # Form appropriate asset universe\n",
    "        first_day_of_month_for_current_test_dt = np.datetime_as_string(test_datetime_start, unit='M')+'-01'\n",
    "        asset_universe = asset_universe_dict[first_day_of_month_for_current_test_dt]\n",
    "\n",
    "        # From relevant date-assets given asset universe and the train and oos dataframes\n",
    "        rel_df   = df[(df.date<=test_datetime_end) & (df.asset.isin(asset_universe))].copy()\n",
    "        train_df = rel_df[rel_df.date<test_datetime_start].copy()\n",
    "        oos_df   = rel_df[(rel_df.date>=test_datetime_start) & (rel_df.date<=test_datetime_end)].copy()\n",
    "\n",
    "        # Fit and predict\n",
    "        models, _, _ = fitAutoencoder(train_df, opt_hps_dict, lhs_col, rhs_cols)\n",
    "        yhats = genAutoencoderYhats(models, rel_df, rhs_cols, test_datetime_start, test_datetime_end, opt_hps_dict['number_factors'])\n",
    "        del rel_df, train_df\n",
    "        gc.collect()\n",
    "        temp_yhats_df = oos_df[['date', 'asset', lhs_col]].copy()\n",
    "        temp_yhats_df['yhats'] = yhats\n",
    "        test_y_yhats_df = pd.concat([test_y_yhats_df, temp_yhats_df])\n",
    "\n",
    "    return test_y_yhats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1abb100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number_factors': 1, 'num_hidden_layers': 1, 'learning_rates': 5e-05, 'batch_sizes': 2048, 'l1_penalties': 0.01, 'num_ensemble': 10, 'early_stopping': True, 'epochs': 100}\n",
      "2021-01-01T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 19:03:42.195337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5281 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 19:03:45.360857: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fe6cc0031f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-21 19:03:45.360891: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti, Compute Capability 8.6\n",
      "2023-06-21 19:03:45.381961: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-21 19:03:45.381998: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-21 19:03:45.626855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-21 19:03:45.864456: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/237 [==============================] - 6s 10ms/step - loss: 0.4243 - mse: 0.0016 - val_loss: 0.4119 - val_mse: 3.9619e-04\n",
      "Epoch 2/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.4015 - mse: 9.3915e-04 - val_loss: 0.3900 - val_mse: 4.3760e-04\n",
      "Epoch 3/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.3796 - mse: 6.8363e-04 - val_loss: 0.3687 - val_mse: 4.2832e-04\n",
      "Epoch 4/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.3587 - mse: 5.7932e-04 - val_loss: 0.3483 - val_mse: 4.0788e-04\n",
      "Epoch 5/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.3385 - mse: 5.2774e-04 - val_loss: 0.3284 - val_mse: 3.9159e-04\n",
      "Epoch 6/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.3188 - mse: 4.9305e-04 - val_loss: 0.3090 - val_mse: 3.7826e-04\n",
      "Epoch 7/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2998 - mse: 4.6371e-04 - val_loss: 0.2904 - val_mse: 3.6486e-04\n",
      "Epoch 8/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2816 - mse: 4.3840e-04 - val_loss: 0.2727 - val_mse: 3.5355e-04\n",
      "Epoch 9/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2642 - mse: 4.1612e-04 - val_loss: 0.2556 - val_mse: 3.4401e-04\n",
      "Epoch 10/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2473 - mse: 3.9770e-04 - val_loss: 0.2391 - val_mse: 3.3540e-04\n",
      "Epoch 11/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2312 - mse: 3.8141e-04 - val_loss: 0.2232 - val_mse: 3.2722e-04\n",
      "Epoch 12/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2157 - mse: 3.6691e-04 - val_loss: 0.2080 - val_mse: 3.2016e-04\n",
      "Epoch 13/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2005 - mse: 3.5422e-04 - val_loss: 0.1930 - val_mse: 3.1428e-04\n",
      "Epoch 14/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1857 - mse: 3.4358e-04 - val_loss: 0.1783 - val_mse: 3.0955e-04\n",
      "Epoch 15/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1712 - mse: 3.3366e-04 - val_loss: 0.1639 - val_mse: 3.0544e-04\n",
      "Epoch 16/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1570 - mse: 3.2515e-04 - val_loss: 0.1501 - val_mse: 3.0175e-04\n",
      "Epoch 17/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1435 - mse: 3.1758e-04 - val_loss: 0.1369 - val_mse: 2.9880e-04\n",
      "Epoch 18/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1306 - mse: 3.1103e-04 - val_loss: 0.1243 - val_mse: 2.9612e-04\n",
      "Epoch 19/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1183 - mse: 3.0506e-04 - val_loss: 0.1123 - val_mse: 2.9384e-04\n",
      "Epoch 20/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1066 - mse: 2.9971e-04 - val_loss: 0.1010 - val_mse: 2.9198e-04\n",
      "Epoch 21/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0955 - mse: 2.9475e-04 - val_loss: 0.0900 - val_mse: 2.9043e-04\n",
      "Epoch 22/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0848 - mse: 2.9024e-04 - val_loss: 0.0797 - val_mse: 2.8907e-04\n",
      "Epoch 23/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0748 - mse: 2.8679e-04 - val_loss: 0.0701 - val_mse: 2.8779e-04\n",
      "Epoch 24/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0655 - mse: 2.8426e-04 - val_loss: 0.0612 - val_mse: 2.8638e-04\n",
      "Epoch 25/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0572 - mse: 2.8268e-04 - val_loss: 0.0532 - val_mse: 2.8513e-04\n",
      "Epoch 26/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0493 - mse: 2.8189e-04 - val_loss: 0.0455 - val_mse: 2.8414e-04\n",
      "Epoch 27/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0420 - mse: 2.8083e-04 - val_loss: 0.0387 - val_mse: 2.8310e-04\n",
      "Epoch 28/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0355 - mse: 2.7902e-04 - val_loss: 0.0325 - val_mse: 2.8252e-04\n",
      "Epoch 29/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0297 - mse: 2.7736e-04 - val_loss: 0.0269 - val_mse: 2.8166e-04\n",
      "Epoch 30/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0245 - mse: 2.7528e-04 - val_loss: 0.0221 - val_mse: 2.8122e-04\n",
      "Epoch 31/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0198 - mse: 2.7401e-04 - val_loss: 0.0175 - val_mse: 2.8135e-04\n",
      "Epoch 32/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0155 - mse: 2.7322e-04 - val_loss: 0.0136 - val_mse: 2.8103e-04\n",
      "Epoch 33/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0119 - mse: 2.7246e-04 - val_loss: 0.0103 - val_mse: 2.8149e-04\n",
      "Epoch 34/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0088 - mse: 2.7187e-04 - val_loss: 0.0074 - val_mse: 2.8197e-04\n",
      "Epoch 35/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0061 - mse: 2.7173e-04 - val_loss: 0.0050 - val_mse: 2.8239e-04\n",
      "Epoch 36/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0039 - mse: 2.7163e-04 - val_loss: 0.0030 - val_mse: 2.8216e-04\n",
      "Epoch 37/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0023 - mse: 2.7138e-04 - val_loss: 0.0016 - val_mse: 2.8163e-04\n",
      "Epoch 1/100\n",
      "237/237 [==============================] - 4s 10ms/step - loss: 0.0357 - mse: 0.0018 - val_loss: 0.0259 - val_mse: 0.0014\n",
      "Epoch 2/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 0.0185 - mse: 0.0014 - val_loss: 0.0119 - val_mse: 0.0012\n",
      "Epoch 3/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 0.0074 - mse: 0.0010 - val_loss: 0.0037 - val_mse: 8.3972e-04\n",
      "Epoch 4/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 0.0018 - mse: 7.2803e-04 - val_loss: 7.2602e-04 - val_mse: 6.4749e-04\n",
      "Epoch 5/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 5.4118e-04 - mse: 5.2225e-04 - val_loss: 4.5517e-04 - val_mse: 4.4313e-04\n",
      "Epoch 6/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 4.0297e-04 - mse: 3.9179e-04 - val_loss: 3.6031e-04 - val_mse: 3.4928e-04\n",
      "Epoch 7/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 3.3144e-04 - mse: 3.2021e-04 - val_loss: 3.1208e-04 - val_mse: 3.0149e-04\n",
      "Epoch 8/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.9906e-04 - mse: 2.8789e-04 - val_loss: 2.9746e-04 - val_mse: 2.8655e-04\n",
      "Epoch 9/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.8776e-04 - mse: 2.7648e-04 - val_loss: 2.9523e-04 - val_mse: 2.8413e-04\n",
      "Epoch 10/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.8475e-04 - mse: 2.7345e-04 - val_loss: 2.9575e-04 - val_mse: 2.8374e-04\n",
      "Epoch 11/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.8420e-04 - mse: 2.7286e-04 - val_loss: 2.9599e-04 - val_mse: 2.8375e-04\n",
      "Epoch 12/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.8405e-04 - mse: 2.7278e-04 - val_loss: 2.9526e-04 - val_mse: 2.8387e-04\n",
      "Epoch 13/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 2.8388e-04 - mse: 2.7277e-04 - val_loss: 2.9525e-04 - val_mse: 2.8401e-04\n",
      "Epoch 14/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.8407e-04 - mse: 2.7277e-04 - val_loss: 2.9583e-04 - val_mse: 2.8449e-04\n",
      "Epoch 15/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 2.8401e-04 - mse: 2.7277e-04 - val_loss: 2.9593e-04 - val_mse: 2.8381e-04\n",
      "Epoch 1/100\n",
      "237/237 [==============================] - 4s 10ms/step - loss: 1.1856 - mse: 1.1532 - val_loss: 1.1808 - val_mse: 1.1574\n",
      "Epoch 2/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 1.0979 - mse: 1.0812 - val_loss: 1.0551 - val_mse: 1.0442\n",
      "Epoch 3/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.9974 - mse: 0.9912 - val_loss: 0.9366 - val_mse: 0.9336\n",
      "Epoch 4/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.8897 - mse: 0.8885 - val_loss: 0.8393 - val_mse: 0.8390\n",
      "Epoch 5/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.7800 - mse: 0.7799 - val_loss: 0.7200 - val_mse: 0.7198\n",
      "Epoch 6/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 0.6708 - mse: 0.6708 - val_loss: 0.6019 - val_mse: 0.6019\n",
      "Epoch 7/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.5652 - mse: 0.5651 - val_loss: 0.5154 - val_mse: 0.5157\n",
      "Epoch 8/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.4658 - mse: 0.4658 - val_loss: 0.4092 - val_mse: 0.4094\n",
      "Epoch 9/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.3749 - mse: 0.3749 - val_loss: 0.3295 - val_mse: 0.3295\n",
      "Epoch 10/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 0.2940 - mse: 0.2940 - val_loss: 0.2544 - val_mse: 0.2546\n",
      "Epoch 11/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2238 - mse: 0.2238 - val_loss: 0.1877 - val_mse: 0.1876\n",
      "Epoch 12/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1647 - mse: 0.1647 - val_loss: 0.1284 - val_mse: 0.1284\n",
      "Epoch 13/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 0.1165 - mse: 0.1165 - val_loss: 0.1009 - val_mse: 0.1009\n",
      "Epoch 14/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 0.0787 - mse: 0.0787 - val_loss: 0.0565 - val_mse: 0.0564\n",
      "Epoch 15/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.0322 - val_mse: 0.0321\n",
      "Epoch 16/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.0246 - val_mse: 0.0246\n",
      "Epoch 17/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 18/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0053 - val_mse: 0.0053\n",
      "Epoch 19/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 20/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0021 - val_mse: 0.0020\n",
      "Epoch 21/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 8.5664e-04 - mse: 8.4325e-04 - val_loss: 3.6805e-04 - val_mse: 3.5519e-04\n",
      "Epoch 22/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 4.6162e-04 - mse: 4.4846e-04 - val_loss: 2.9688e-04 - val_mse: 2.8362e-04\n",
      "Epoch 23/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 3.3295e-04 - mse: 3.1972e-04 - val_loss: 3.1584e-04 - val_mse: 3.0252e-04\n",
      "Epoch 24/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 2.9588e-04 - mse: 2.8271e-04 - val_loss: 3.1475e-04 - val_mse: 3.0248e-04\n",
      "Epoch 25/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 2.8825e-04 - mse: 2.7488e-04 - val_loss: 3.1268e-04 - val_mse: 2.9807e-04\n",
      "Epoch 26/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 2.8649e-04 - mse: 2.7312e-04 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 27/100\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 2.8613e-04 - mse: 2.7283e-04 - val_loss: 3.2944e-04 - val_mse: 3.1602e-04\n",
      "Epoch 1/100\n",
      "237/237 [==============================] - 5s 14ms/step - loss: 1.3600 - mse: 1.3238 - val_loss: 1.2973 - val_mse: 1.2705\n",
      "Epoch 2/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 1.2571 - mse: 1.2379 - val_loss: 1.2205 - val_mse: 1.2088\n",
      "Epoch 3/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 1.1406 - mse: 1.1332 - val_loss: 1.1080 - val_mse: 1.1050\n",
      "Epoch 4/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 1.0179 - mse: 1.0166 - val_loss: 0.9668 - val_mse: 0.9664\n",
      "Epoch 5/100\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 0.8945 - mse: 0.8945 - val_loss: 0.8284 - val_mse: 0.8284\n",
      "Epoch 6/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.7718 - mse: 0.7718 - val_loss: 0.7200 - val_mse: 0.7201\n",
      "Epoch 7/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.6529 - mse: 0.6529 - val_loss: 0.5920 - val_mse: 0.5923\n",
      "Epoch 8/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.5408 - mse: 0.5407 - val_loss: 0.4857 - val_mse: 0.4857\n",
      "Epoch 9/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.4378 - mse: 0.4378 - val_loss: 0.3916 - val_mse: 0.3916\n",
      "Epoch 10/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.3456 - mse: 0.3456 - val_loss: 0.2866 - val_mse: 0.2867\n",
      "Epoch 11/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.2652 - mse: 0.2652 - val_loss: 0.2293 - val_mse: 0.2293\n",
      "Epoch 12/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1970 - mse: 0.1970 - val_loss: 0.1616 - val_mse: 0.1616\n",
      "Epoch 13/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.1410 - mse: 0.1410 - val_loss: 0.1161 - val_mse: 0.1161\n",
      "Epoch 14/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0965 - mse: 0.0965 - val_loss: 0.0772 - val_mse: 0.0772\n",
      "Epoch 15/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0628 - mse: 0.0628 - val_loss: 0.0476 - val_mse: 0.0476\n",
      "Epoch 16/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0323 - val_mse: 0.0323\n",
      "Epoch 17/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 18/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 19/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 20/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 21/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 22/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 5.6392e-04 - mse: 5.5155e-04 - val_loss: 5.0241e-04 - val_mse: 4.9098e-04\n",
      "Epoch 23/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 3.6287e-04 - mse: 3.5070e-04 - val_loss: 2.9785e-04 - val_mse: 2.8517e-04\n",
      "Epoch 24/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 3.0378e-04 - mse: 2.9159e-04 - val_loss: 2.9793e-04 - val_mse: 2.8650e-04\n",
      "Epoch 25/100\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 2.8816e-04 - mse: 2.7592e-04 - val_loss: 3.0826e-04 - val_mse: 2.9629e-04\n",
      "Epoch 26/100\n",
      "179/237 [=====================>........] - ETA: 0s - loss: 2.8543e-04 - mse: 2.7332e-04"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    IN_FP           = '../data/clean/panel_train.pkl'\n",
    "    ASSET_IN_FP     = '../data/clean/asset_universe_dict.pickle'\n",
    "    CV_OUT_FP       = '../output/high_dim_fm/cv_results'\n",
    "    TEST_OUT_FP     = '../data/clean/test_yhats_autoencoder.pkl'\n",
    "    LHS_COL         = 'r_ex_tp1'\n",
    "    VAL_START_DATE  = '2021-01-01'\n",
    "    VAL_END_DATE    = '2021-06-30'\n",
    "    TEST_START_DATE = '2021-07-01'\n",
    "    NUM_CPUS        = 22 \n",
    "    PERIODS_IN_YEAR = 365*24\n",
    "    ARCH_NAME       = 'autoencoder-12rhs'\n",
    "    HP_GRID         = {'number_factors': [1],\n",
    "        'num_hidden_layers': [1],\n",
    "        'learning_rates': [5e-4, 1e-4, 5e-5], \n",
    "        'batch_sizes': [2048],\n",
    "        'l1_penalties': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "        'num_ensemble': [10],\n",
    "        'early_stopping': [True],\n",
    "        'epochs': [100]} \n",
    "\n",
    "    # read in data\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    all_df = pd.read_pickle(IN_FP)\n",
    "\n",
    "    # drop rows and columns such that data will work for conditional autoencoder (CA)\n",
    "    all_df = dropRowsAndColsForCA(all_df, LHS_COL)\n",
    "\n",
    "    # form the char-sorted portfolios for factor side of CA input\n",
    "    all_df = formPortfolioReturnCovariates(all_df, LHS_COL) # NOTE: ~7 min runtime\n",
    "\n",
    "    # run CV\n",
    "    cv_results_list = runCV(all_df, asset_universe_dict,\n",
    "        VAL_START_DATE, VAL_END_DATE, TEST_START_DATE,\n",
    "        LHS_COL, HP_GRID, PERIODS_IN_YEAR, CV_OUT_FP, ARCH_NAME)\n",
    "\n",
    "    # choose optimal hyperparameter combination based on validation period predictive R^2\n",
    "    opt_val_r2_pred = -1e6\n",
    "    for cv_result in cv_results_list:\n",
    "        if cv_result['val_r2_pred'] > opt_val_r2_pred:\n",
    "            opt_val_r2_pred = cv_result['val_r2_pred']\n",
    "            opt_cv_result = cv_result\n",
    "    opt_hps_dict = dict(itertools.islice(opt_cv_result.items(), len(HP_GRID)+1))\n",
    "    opt_hps_dict['epochs'] = int(opt_hps_dict['avg_epochs_trained'])\n",
    "    opt_hps_dict['early_stopping'] = False\n",
    "    del opt_hps_dict['avg_epochs_trained']\n",
    "\n",
    "    # predict in test period with the optimal model\n",
    "    test_y_yhats_df = predictTestPeriod(all_df, asset_universe_dict, TEST_START_DATE, LHS_COL, opt_hps_dict)\n",
    "    test_y_yhats_df.to_pickle(TEST_OUT_FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdaa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO CHECK THAT POSITION COLUMN IS CALCULATED PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdf65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_yhats_df = test_y_yhats_df.dropna()\n",
    "test_y_yhats_df = test_y_yhats_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "test_y_yhats_pos_df = formPortfolioPositionsQuantileLongShort(test_y_yhats_df, quantile=5)\n",
    "test_yhats = test_y_yhats_pos_df.yhats.values\n",
    "test_ys = test_y_yhats_pos_df[LHS_COL].values\n",
    "test_y_yhats_pos_df['returns'] = test_y_yhats_pos_df.position*test_y_yhats_pos_df[LHS_COL]\n",
    "returns = test_y_yhats_pos_df.groupby('date')['returns'].mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5fd3123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0434887624472069"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.mean(np.square(test_ys-test_yhats))/np.mean(np.square(test_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68fe101b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.021607466050075755"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(test_yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da7edce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003495896315294781"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(test_yhats, q=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d9362d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26090552840750597"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(test_yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0131ba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.425029307058125e-06"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcGeomAvg(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d5dd9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.900200553915258"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcSharpe(returns, periods_in_year=24*365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b910d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP GRID FOR FINAL RUN:\n",
    "\n",
    "# for 1 factor: lr 5e-5, 1e-5, 5e-6\n",
    "# for factors [2] and learning_rates [1e-4, 5e-5, 1e-5]\n",
    "# for factors [3] and learning_rates [5e-4, 5e-5, 5e-6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "\n",
    "# Linear regression with the same RHS can fit in pre 2nd half of 2021 and predit in that period to get 5 bps r2perd and 10 bps tertile diff return\n",
    "\n",
    "# Need returns above like 5 bps to beat transaction fees. Above 15 bps to beat just best single factor.\n",
    "\n",
    "# For val period in first half of 2021, should see val_mse's between 2e-4 and 5e-4 and need < 5.2e-4 to be in the green.\n",
    "\n",
    "# For the first half of 2021 val/test period, need a geom mean above 1.5e-6 to be (bootstrapped) stat diff from zero.\n",
    "# For the second half of 2021 val/test period, need a geom mean above 5e-7 to be (bootstrapped) stat diff from zero.\n",
    "\n",
    "# Based on my CV results, standard deviation of my \"legit\" mse's in first half of 2021 is something like 2e-3 so i need a mse improvement of 4e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ALSO NEED TO TRY A MODEL WHERE WE SET MISSING ASSETS LHS \n",
    "# AND RHS TO -2 AND THEN WE HAVE BALANCED PANEL SO WE FIT ON THE \n",
    "# MATRIX OF RHS ACROSS ASSETS AS OPPOSED OT INDIVIDUAL DATE-ASSET \n",
    "# AND THEN THE NETWORK LEARNS ASSET-SPECIFIC PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO GO SCOPE OLD NOTEBOOK FOR LATEST NUMBERS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
