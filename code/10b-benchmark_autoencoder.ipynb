{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279af7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO UPDATE THIS OLD MESSY CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944cf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sklearn as sk\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import initializers\n",
    "from keras.models import Model\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e77cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP TO RUN ON CPU\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6bbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropRowsAndColsForCA(df):\n",
    "    # Cut to characteristics columns of interest\n",
    "    df = df[['date', 'asset', 'r_tplus1', ]] # TODO GET ~50-100 CHARACTERISTICS THAT OWRK BEST IN UNI WORK?\n",
    "\n",
    "    # TODO NEED TO FIGURE OUT WHERE WE HAVE 2x NUMBER OF ASSETS AS CHARACTERISTICS\n",
    "    \n",
    "    # Cut panel to the first week where there are twice as many tokens as RHS vars\n",
    "    df['counts'] = 1\n",
    "    df['coins_per_week'] = df.groupby(['date'])['counts'].transform(sum)\n",
    "    df = df[df.coins_per_week >= (df.shape[1]-2)*2]\n",
    "    df = df.drop(columns = ['counts', 'coins_per_week'])\n",
    "\n",
    "    # Note: keep obs to RHS ratio roughly 4e4:1\n",
    "\n",
    "    # Note: for any macro, take cartesian product iwth characteristics to make it characteritisc level\n",
    "    # -or do teh reg thing to reduce it down to same dim as number of assets\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e12308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formPortfolioReturnCovariates(df):\n",
    "    # Obtain the datetimes of the dataframe\n",
    "    df = df.sort_values(by = 'date')\n",
    "    weeks = np.unique(df.index)\n",
    "\n",
    "    # Form new covariate names\n",
    "    column_names = list(df.columns.values)\n",
    "    column_names.remove('asset')\n",
    "    column_names.remove('r_tplus7')\n",
    "    covariates = column_names\n",
    "    new_covariates = ['x_' + cov for cov in covariates]\n",
    "\n",
    "    # TODO ADD A CONSTANT TO z_t before reg?\n",
    "\n",
    "    for current_week in weeks: \n",
    "        # Obtain the week's LHS returns and the previous week's covariates\n",
    "        r_tp1 = df[df.index == current_week].r_tplus1.values\n",
    "        z_t  = df[df.index == current_week][covariates].values\n",
    "\n",
    "        # Calculate the characteristic managed portfolio returns\n",
    "        design = np.linalg.inv(np.matmul(np.transpose(z_t), z_t))\n",
    "        x_tp1    = np.matmul(np.matmul(design, np.transpose(z_t)), r_tp1)\n",
    "\n",
    "        # Set the new columns to this week's vector's value\n",
    "        df.loc[df.index == current_week, new_covariates] = x_tp1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aeec626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processMonth(panel_df: pd.DataFrame, date_key: str, asset_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Process a single month's data and return the resulting DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert the date_key string to a datetime object to work with pandas\n",
    "    date_key_dt = pd.to_datetime(date_key)\n",
    "\n",
    "    # Create a date mask for the month\n",
    "    date_mask = (panel_df['date'].dt.year == date_key_dt.year) & (panel_df['date'].dt.month == date_key_dt.month)\n",
    "\n",
    "    # Subset the panel_df DataFrame based on the date mask and the asset list\n",
    "    subset = panel_df[date_mask & panel_df['asset'].isin(asset_list)]\n",
    "\n",
    "    return subset\n",
    "\n",
    "def subsetByMonthToAssetUniverse(panel_df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]], n_jobs=-1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset the panel data to the assets in each month (key) of asset_universe_dict.\n",
    "\n",
    "    Args:\n",
    "        panel_df: A Pandas DataFrame containing panel data at the asset-hour level\n",
    "                    with ID columns 'date' and 'asset'.\n",
    "        asset_universe_dict: A dictionary with keys as dates in the format YYYY-MM-DD \n",
    "                                and values as lists of asset strings.\n",
    "        n_jobs: Number of CPU cores to use for parallelization. Default is -1, which means using all available cores.\n",
    "    \n",
    "    Returns: A Pandas DataFrame without the rows not included in the study.\n",
    "    \"\"\"\n",
    "    # Run the process_month function in parallel using joblib\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(processMonth)(panel_df, date_key, asset_list) for date_key, asset_list in asset_universe_dict.items())\n",
    "\n",
    "    # Combine the results into a single DataFrame\n",
    "    new_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60956e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAutoencoder(train_df, hps_yhats_dict, val_df=None, early_stopping=True):\n",
    "    # Obtain the covariates\n",
    "    # TODO REPLACE WITH PASSING IN LHS AND RHS NAMES\n",
    "    column_names = list(df.columns.values)\n",
    "    column_names.remove('week_idx')\n",
    "    column_names.remove('asset')\n",
    "    column_names.remove('r_tplus7')\n",
    "    covariates = column_names\n",
    "\n",
    "    # Obtain the covariates on x_t and b_t_1 sides of the network\n",
    "    # TODO EDIT DIS\n",
    "    x_t   = [covar for covar in covariates if covar[:2] == 'x_']\n",
    "    b_t_1 = [covar for covar in covariates if covar[:2] != 'x_']\n",
    "\n",
    "    # Extract the hyperparameters\n",
    "    number_hidden_layer = hps_yhats_dict['number_hidden_layer']\n",
    "    number_factor       = hps_yhats_dict['number_factor']\n",
    "    learning_rate       = hps_yhats_dict['learning_rate']\n",
    "    l1_penalty          = hps_yhats_dict['l1_penalty']\n",
    "    batch_size          = hps_yhats_dict['batch_size']\n",
    "    number_ensemble     = hps_yhats_dict['number_ensemble']\n",
    "    bootstrap_pct       = hps_yhats_dict['bootstrap_pct']\n",
    "    epoch               = hps_yhats_dict['epoch']\n",
    "\n",
    "    # Initialize the models\n",
    "    models = []\n",
    "\n",
    "    # Loop over the ensembles to build models for each\n",
    "    assert(number_ensemble <= 10), 'whatcha think you got infinite come pew ters'\n",
    "    for i in range(0, number_ensemble):\n",
    "        # Bootstrap the rows so different models in the ensemble are less correlated\n",
    "        # TODO REMOVE?\n",
    "        train_df = sk.utils.resample(train_df, replace = True, \n",
    "            n_samples = int(train_df.shape[0]*bootstrap_pct),\n",
    "            random_state = i)\n",
    "        \n",
    "        # Obtain the training input and output data and, if passed, validation data\n",
    "        train_b_t_1 = train_df[b_t_1]\n",
    "        train_x_t   = train_df[x_t]  \n",
    "        train_y     = train_df[['r_tplus7']]\n",
    "        if val_df is not None:\n",
    "            val_b_t_1   = val_df[b_t_1]\n",
    "            val_x_t     = val_df[x_t]  \n",
    "            val_y       = val_df[['r_tplus7']]\n",
    "        \n",
    "        # According to which model in the ensemble it is, initialize parameters.\n",
    "        # TODO REPLACE WITH RANDOMLY GRABBING INIT METHOD OUT OF LIST\n",
    "        # -BUT SET SEED FOR RANDOM GRAB SO IT REPLICABLE\n",
    "        if i==0:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.GlorotUniform(seed=i)\n",
    "        elif i==1:\n",
    "            weight_initializer=initializers.GlorotUniform(seed=i)\n",
    "            bias_initializer=initializers.RandomUniform(seed=i)\n",
    "        elif i==2:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.RandomUniform(seed=i)\n",
    "        elif i==3:\n",
    "            weight_initializer=initializers.RandomUniform(seed=i)\n",
    "            bias_initializer=initializers.GlorotUniform(seed=i)\n",
    "        elif i==4:\n",
    "            weight_initializer=initializers.GlorotUniform(seed=i)\n",
    "            bias_initializer=initializers.HeNormal(seed=i)\n",
    "        else:\n",
    "            weight_initializer=initializers.HeNormal(seed=i)\n",
    "            bias_initializer=initializers.RandomUniform(seed=i)\n",
    "        \n",
    "        # Build the betas model from the time t covariates\n",
    "        # TODO ADJUST SIZE OF NETWORK IN RELATON TO NUMBER OF COVARS\n",
    "        # -THEY AHVE ~100 SO IF I HAVE LESS THEN DO PROPORTIONAL\n",
    "        # -AND MAKE THEM POWERS OF 2\n",
    "        model_b = tf.keras.models.Sequential()\n",
    "        model_b.add(tf.keras.Input(shape=(len(b_t_1),)))\n",
    "        model_b.add(Dense(32, activation='relu',\n",
    "                          kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                          kernel_initializer=weight_initializer,\n",
    "                          bias_initializer=bias_initializer))\n",
    "        model_b.add(BatchNormalization())\n",
    "        if number_hidden_layer >= 2:\n",
    "            model_b.add(Dense(16, activation='relu',\n",
    "                              kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                              kernel_initializer=weight_initializer,\n",
    "                              bias_initializer=bias_initializer))\n",
    "            model_b.add(BatchNormalization())\n",
    "        if number_hidden_layer == 3:\n",
    "            model_b.add(Dense(8, activation='relu',\n",
    "                              kernel_regularizer=regularizers.l1(l1=l1_penalty),\n",
    "                              kernel_initializer=weight_initializer,\n",
    "                              bias_initializer=bias_initializer))\n",
    "            model_b.add(BatchNormalization())\n",
    "        model_b.add(Dense(number_factor, activation='linear',\n",
    "                          kernel_initializer=weight_initializer,\n",
    "                          bias_initializer=bias_initializer))\n",
    "\n",
    "        # Form the x model from time t plus 1 returns\n",
    "        model_x = tf.keras.models.Sequential()\n",
    "        model_x.add(tf.keras.Input(shape=(len(x_t),)))\n",
    "        model_x.add(Dense(number_factor, activation='linear',\n",
    "                          kernel_initializer=weight_initializer,\n",
    "                          bias_initializer=bias_initializer))\n",
    "\n",
    "        # Form the dot product output for the combination of the two neurals\n",
    "        mergedOut = Dot(axes=(1,1))([model_b.output, model_x.output])\n",
    "\n",
    "        # Form the entire model\n",
    "        model = Model([model_b.input, model_x.input], mergedOut)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='mean_squared_error',\n",
    "                      metrics=['mse'])\n",
    "\n",
    "        # Prepare early stopping object \n",
    "        es = EarlyStopping(monitor='val_mse', mode='min', verbose=0, patience = 2) # VERBOSE 2\n",
    "        \n",
    "        # Fit the model\n",
    "        # TODO CHANGE TO GPU!!\n",
    "        with tf.device('/CPU:0'):\n",
    "            if early_stopping == True:\n",
    "                model.fit(x=[train_b_t_1, train_x_t], y=train_y, \n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=([val_b_t_1, val_x_t], val_y), \n",
    "                          epochs=epoch, verbose=0,\n",
    "                          workers=4, callbacks=[es]) # VERBOSE 1\n",
    "            else:\n",
    "                model.fit(x=[train_b_t_1, train_x_t], y=train_y, \n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epoch, verbose=1,\n",
    "                          workers=4)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df66ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genYhats(df, models, oos_week, number_factor):\n",
    "    # Obtain the covariates on x_t and b_t_1 sides of the network\n",
    "    column_names = list(df.columns.values)\n",
    "    column_names.remove('asset')\n",
    "    column_names.remove('r_tplus7')\n",
    "    column_names.remove('week_idx')\n",
    "    covariates = column_names\n",
    "    x_t   = [covar for covar in covariates if covar[:2] == 'x_']\n",
    "    b_t_1 = [covar for covar in covariates if covar[:2] != 'x_']\n",
    "\n",
    "    # Obtain the oos data\n",
    "    oos_df = df[df.index == oos_week].copy()\n",
    "    oos_x_t = oos_df[x_t]\n",
    "    oos_b_t_1 = oos_df[b_t_1]\n",
    "\n",
    "    # For each model form the beta hats\n",
    "    b_hats = np.zeros((oos_df.shape[0],number_factor))\n",
    "    for model in models:\n",
    "        layer_name = model.layers[-3]._name \n",
    "        assert(model.layers[-3].output_shape[1] == number_factor)\n",
    "        b_hat_layer = Model(inputs=model.input[0],\n",
    "                            outputs=model.get_layer(layer_name).output)\n",
    "        b_hat = b_hat_layer.predict(oos_b_t_1)\n",
    "        b_hats += b_hat\n",
    "        \n",
    "    # Obtain the average b_hat across the models\n",
    "    b_hats = b_hats/len(models)\n",
    "    \n",
    "    # output statistics- 5 stat summary of Beta Hat\n",
    "    # TODO REPLACE THIS WITH A FUNC TO DO THIS\n",
    "#     b_hats_min = b_hats.min()\n",
    "#     b_hats_max = b_hats.max()\n",
    "#     b_hats_25_percentile = np.percentile(b_hats,25)\n",
    "#     b_hats_75_percentile = np.percentile(b_hats,75)\n",
    "#     b_hats_mean = b_hats.mean()\n",
    "#     print('Min beta_hats: %.3f'% b_hats_min)\n",
    "#     print('Q1 beta_hats: %.3f'% b_hats_25_percentile)\n",
    "#     print('Mean beta_hats: %.3f'% b_hats_mean)\n",
    "#     print('Q3 beta_hats: %.3f'% b_hats_75_percentile)\n",
    "#     print('Max beta_hats: %.3f'% b_hats_max)\n",
    "#     print('\\n')\n",
    "\n",
    "    # Form the sample average of the estimated factors up to the OOS week\n",
    "    weeks = np.unique(df[df.index < oos_week].index)\n",
    "    lambda_hats = np.zeros(number_factor)\n",
    "    for prev_week in weeks:\n",
    "        prev_x_t = df[df.index == prev_week][x_t]\n",
    "        f_hats = np.zeros(number_factor)\n",
    "        for model in models:\n",
    "            layer_name = model.layers[-2]._name \n",
    "            assert(model.layers[-2].output_shape[1] == number_factor)\n",
    "            f_hat_layer = Model(inputs=model.input[1],\n",
    "                                outputs=model.get_layer(layer_name).output)\n",
    "            f_hat = f_hat_layer.predict(prev_x_t)\n",
    "            assert(all(np.isclose(f_hat[0,:], f_hat[1,:])))\n",
    "            f_hats += f_hat[0,:]\n",
    "        f_hats = f_hats / len(models)\n",
    "        lambda_hats += f_hats\n",
    "        lambda_hats = lambda_hats / len(weeks)\n",
    "        \n",
    "    # summary of lambda_hats - 5 stat summary\n",
    "    # TODO REPLACE THIS WITH A FUNC TO DO THIS\n",
    "#     lambda_hats_min = lambda_hats.min()\n",
    "#     lambda_hats_max = lambda_hats.max()\n",
    "#     lambda_hats_25_percentile = np.percentile(lambda_hats,25)\n",
    "#     lambda_hats_75_percentile = np.percentile(lambda_hats,75)\n",
    "#     lambda_hats_mean = lambda_hats.mean()\n",
    "#     print('Min lambda_hats: %.3f'% lambda_hats_min)\n",
    "#     print('Q1 lambda_hats: %.3f'% lambda_hats_25_percentile)\n",
    "#     print('Mean lambda_hats: %.3f'% lambda_hats_mean)\n",
    "#     print('Q3 lambda_hats: %.3f'% lambda_hats_75_percentile)\n",
    "#     print('Max lambda_hats: %.3f'% lambda_hats_max) \n",
    "#     print('\\n')\n",
    "        \n",
    "    # Form yhats\n",
    "    yhats = np.matmul(b_hats, lambda_hats)\n",
    "    \n",
    "    # output stats of yhats - 5 stat summar\n",
    "    # TODO REPLACE THIS WITH A FUNC TO DO THISy \n",
    "#     y_hats_min = yhats.min()\n",
    "#     y_hats_max = yhats.max()\n",
    "#     y_hats_25_percentile = np.percentile(yhats,25)\n",
    "#     y_hats_75_percentile = np.percentile(yhats,75)\n",
    "#     y_hats_mean = yhats.mean()\n",
    "#     print('Min y_hats: %.3f'% y_hats_min)\n",
    "#     print('Q1 y_hats: %.3f'% y_hats_25_percentile)\n",
    "#     print('Mean y_hats: %.3f'% y_hats_mean)\n",
    "#     print('Q3 y_hats: %.3f'% y_hats_75_percentile)\n",
    "#     print('Max y_hats: %.3f'% y_hats_max)\n",
    "#     print('\\n')\n",
    "    \n",
    "\n",
    "    return yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472f1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(df, asset_universe_dict, last_train_year=2018, val_end_year=2020):\n",
    "    # Initialize hp result objects\n",
    "    hps_yhats_dict_list = []\n",
    "    hps_mse_df_list     = []\n",
    "    \n",
    "    # Initialize the hyperparameter grid  \n",
    "    # TODO MOVE THIS OUTSIDE SO I PASS THIS IN\n",
    "    number_hidden_layers = [3, 2, 1]\n",
    "    number_factors       = [6, 5, 4, 3, 2, 1]\n",
    "    learning_rates       = [1e-5, 1e-3, 1e-1]\n",
    "    batch_sizes          = [128] # note: ensure powers of 2 for eff, [64,256]\n",
    "    l1_penalties         = [1, 1e-1, 1e-2, 1e-3]\n",
    "    number_ensembles     = [10] \n",
    "    early_stopping       = [True]\n",
    "    bootstrap_pcts       = [1]\n",
    "    epochs               = [10] \n",
    "\n",
    "    # Determine the weeks in the validation window  \n",
    "    val_weeks = np.unique(df[(df.index.year > last_train_year)  \n",
    "                             & (df.index.year <= val_end_year)].index.values)  \n",
    "\n",
    "    # Generate yhats for every hyperparameter grid point\n",
    "    # TODO ADJUST TO WHATEVER THE KEYS ARE IN THE HP DICT\n",
    "    # -use those keys to form the dict inside or maybe just get like the indices\n",
    "    for hps in itertools.product(number_hidden_layers,\n",
    "                                 number_factors,\n",
    "                                 learning_rates,\n",
    "                                 batch_sizes,\n",
    "                                 l1_penalties,\n",
    "                                 number_ensembles,\n",
    "                                 early_stopping,\n",
    "                                 bootstrap_pcts,\n",
    "                                 epochs):\n",
    "        hps_yhats_dict = {'number_hidden_layer': hps[0],\n",
    "                          'number_factor': hps[1],\n",
    "                          'learning_rate': hps[2],\n",
    "                          'batch_size': hps[3],\n",
    "                          'l1_penalty': hps[4],\n",
    "                          'number_ensemble': hps[5],\n",
    "                          'early_stopping': hps[6],\n",
    "                          'bootstrap_pct': hps[7],\n",
    "                          'epoch': hps[8],\n",
    "                          'yhats': np.array([]),\n",
    "                          'ys':    np.array([])}\n",
    "        print(hps_yhats_dict, '\\n')\n",
    "        tic = time.perf_counter()\n",
    "        # TODO DO SOME LOGIC TO FIGURE OUT THE WINDOWS TO FIT A MODEL FOR SO I LOOP OVER THOSE INITIAL DATES\n",
    "        # -fit teh model once\n",
    "        # -then predict on all subsequent weeks\n",
    "        # -maybe one month at time?\n",
    "        # -maybe one week at a time?\n",
    "        # -let's try once a month to see how it does\n",
    "        for val_week in val_weeks:\n",
    "#             print(val_week, '\\n')\n",
    "            temp_df = df[df.index <= val_week].copy()\n",
    "            temp_df = subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week=val_week)\n",
    "            train_df = temp_df[temp_df.index < val_week].copy()\n",
    "            val_df   = temp_df[temp_df.index == val_week].copy()\n",
    "            \n",
    "            models = fitAutoencoder(train_df, hps_yhats_dict, val_df=val_df, \n",
    "                                    early_stopping=hps_yhats_dict['early_stopping'])\n",
    "            yhats  = genYhats(temp_df, models, val_week, hps_yhats_dict['number_factor'])\n",
    "            ys     = val_df.r_tplus7.values\n",
    "\n",
    "            hps_yhats_dict['yhats'] = np.append(hps_yhats_dict['yhats'], yhats)\n",
    "            hps_yhats_dict['ys']    = np.append(hps_yhats_dict['ys'], ys)\n",
    "\n",
    "            val_ys_todate = hps_yhats_dict['ys']\n",
    "            rw_val_mse    = np.mean(np.square(val_ys_todate))\n",
    "            model_val_mse = np.mean(np.square(val_ys_todate - hps_yhats_dict['yhats']))\n",
    "#             print('\\n val random walk mse: ' + str(rw_val_mse))\n",
    "#             print('\\n val model mse: ' + str(model_val_mse))\n",
    "#             print('\\n val model mse winning?: ' + str(model_val_mse < rw_val_mse))\n",
    "#             print('\\n\\n')\n",
    "# TODO WRITE FUNC HERE TO REPORT SIMPLE STUFF LIKE DIST OF PREDICTED RETURNS AND PREDICTIVE R^2\n",
    "        # Save run time and space out result print out\n",
    "        toc = time.perf_counter()\n",
    "#         print('\\n\\n\\n')\n",
    "\n",
    "        # Update lists of results\n",
    "        # TODO WRITE FUNC TO REPORT MORE STUFF HERE FROM OTHER NOTEBOOKS\n",
    "        # -including all the results that i will have for test period\n",
    "        hps_yhats_dict_list.append(hps_yhats_dict)\n",
    "        cv_results_dict = hps_yhats_dict.copy()\n",
    "        del cv_results_dict['yhats']\n",
    "        del cv_results_dict['ys']\n",
    "        cv_results_dict['runtime_mins'] = round((toc - tic)/60, 0)\n",
    "        cv_results_dict['mse'] = model_val_mse\n",
    "        hps_mse_df_list.append(pd.DataFrame(cv_results_dict, index=[0]))\n",
    "\n",
    "        # Save CV results\n",
    "        cv_df = pd.concat(hps_mse_df_list, ignore_index=True)\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = '../4-output/cv-results-autoencoder-' + timestr + '.csv'\n",
    "        cv_df.to_csv(fp, index=False)\n",
    "\n",
    "    return hps_yhats_dict_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(cv_df: pd.DataFrame, \n",
    "    lhs_col: str,\n",
    "    val_start_date: str,\n",
    "    num_cpus: int,\n",
    "    arch_name: str, \n",
    "    out_fp: str,\n",
    "    periods_in_year: int) -> list:\n",
    "    ''' run step-forward cross validation to select optimal hyperparameters for target model \n",
    "        returning fitting yhats and models as well as outputting results to csv.\n",
    "\n",
    "    Args:\n",
    "        cv_df (pd.DataFrame): panel of training and val data with date index, LHS variable named \n",
    "                              `lhs_col`, and remaining cols are RHS features.\n",
    "        lhs_col (str): name of the lhs target column.\n",
    "        val_start_date (str): the first date for the validation period.\n",
    "        num_cpus (int): number of cpus to use when parallelizing.\n",
    "        arch_name (str): name of architecture to use when saving intermitent results.\n",
    "        out_fp (str): filepath to output the csv file without `.csv' on end.\n",
    "        periods_in_year (int): how many rows of the data constitute a year.\n",
    "          \n",
    "    Returns:\n",
    "        (list): list of dictionaries for each hyperparameter fit where each list contains keys of:\n",
    "                    `hps` is dict of hyperparameter combination,\n",
    "                    `yhats` is an array of validation period yhats, and,\n",
    "                    `models` is list of fitted models.  \n",
    "    '''\n",
    "    # initialize args\n",
    "    val_dates = np.unique(cv_df[val_start_date:].index.values)\n",
    "    results_list = []\n",
    "    csv_dict_list = []\n",
    "    rhs_cols = list(cv_df.columns.values)\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # initialize hp grid\n",
    "    # GO SCOPE GU EMP AP VIA ML HP'S AND JUST DO THAT\n",
    "    # SCOPE GU AUTOENCODER PAPER TO ENSURE NO OTHER HP'S MENTIONED\n",
    "    num_estimators  = [10, 20, 40] # 20 seems good but do fine grid 10, 20, 30, 40, 50, 100 later\n",
    "    subsamples      = [0.7, 0.8, 0.9] # 0.8 seems good but can do fine grid 0.75 to 0.95 later\n",
    "    min_samp_leafs  = [64, 128, 256, 512, 1024]  # 1024 seems good can but can do fine grid 512 to 1536 later\n",
    "    max_depths      = [1, 2] # keep trying 1,2 \n",
    "    max_features    = [0.005, 0.01, 0.02, 0.05, 0.1, 0.2] # kinda unclear; do fine grid 0.005 to 0.3 later\n",
    "    loss_funcs      = ['squared_error']\n",
    "    weight_funcs    = ['exp', 'linear', 'y']  # y seems epic    \n",
    "\n",
    "    # loop over all hp combos\n",
    "    for hps in itertools.product(num_estimators,\n",
    "                                 subsamples,\n",
    "                                 min_samp_leafs,\n",
    "                                 max_depths, \n",
    "                                 max_features,\n",
    "                                 loss_funcs,\n",
    "                                 weight_funcs):\n",
    "        # initialize this hp iter args\n",
    "        results_dict = {}\n",
    "        results_dict['hps'] = {'num_estimator': hps[0],\n",
    "            'subsample': hps[1],\n",
    "            'min_samp_leaf': hps[2],\n",
    "            'max_depth': hps[3],\n",
    "            'max_feature': hps[4],\n",
    "            'loss_func': hps[5],\n",
    "            'weight_func': hps[6]}\n",
    "\n",
    "        # monitor progress\n",
    "        print(results_dict['hps'], '\\n') \n",
    "\n",
    "        # fit model on all val dates\n",
    "        tic = time.perf_counter()\n",
    "        def loopOverValDates(val_date): # set up as a func to loop over\n",
    "            # form train and val data\n",
    "            train_df = cv_df[cv_df.index < val_date].copy()\n",
    "            val_df   = cv_df[cv_df.index == val_date].copy()\n",
    "\n",
    "            # fit model and generate yhats for val week\n",
    "            model, train_r2_pred, train_yhats = fitRF(train_df, lhs_col, rhs_cols, results_dict['hps'])\n",
    "            yhat = genRFYhats(val_df, model, rhs_cols)\n",
    "\n",
    "            return yhat, model, train_r2_pred, train_yhats\n",
    "\n",
    "        val_results = Parallel(n_jobs=int(num_cpus/2))(delayed(loopOverValDates)(val_date) for val_date in tqdm(val_dates))\n",
    "\n",
    "        # extract validation periods results\n",
    "        yhats_list = []\n",
    "        models_list = []\n",
    "        train_r2_pred_list = []\n",
    "        train_yhats_list = []\n",
    "        for t in range(len(val_results)):\n",
    "            yhats_list.append(val_results[t][0])\n",
    "            models_list.append(val_results[t][1])\n",
    "            train_r2_pred_list.append(val_results[t][2])\n",
    "            train_yhats_list.append(val_results[t][3])\n",
    "        yhats = np.array(yhats_list)\n",
    "        positions = np.sign(yhats)\n",
    "        ys    = cv_df[val_start_date:][lhs_col].values\n",
    "        results_dict['yhats'] = yhats\n",
    "\n",
    "        # save results to master result list    \n",
    "        results_list.append(results_dict)\n",
    "\n",
    "        # save results to csv to monitor during cv\n",
    "        toc = time.perf_counter()\n",
    "        csv_dict = results_dict['hps'].copy()\n",
    "        del results_dict\n",
    "        csv_dict['arch_name'] = arch_name\n",
    "        csv_dict['val_start_date'] = val_start_date\n",
    "        csv_dict['val_end_date'] = np.datetime_as_string(np.max(cv_df.index.values))[:10]\n",
    "        csv_dict['runtime_mins'] = round((toc - tic)/60, 0) \n",
    "        csv_dict['mean_r2_pred_train'] = np.mean(np.array(train_r2_pred_list))\n",
    "        csv_dict['r2_pred_val'] = 1-np.mean(np.square(ys-yhats))/np.mean(np.square(ys))\n",
    "        tc = calcTransactionCosts(positions)\n",
    "        returns = positions*ys - tc\n",
    "        csv_dict['geom_mean_1h'] = calcGeomAvg(returns)\n",
    "        csv_dict['sharpe_ann'] = calcSharpe(returns, periods_in_year=periods_in_year)\n",
    "        csv_dict['sd_ann'] = calcSD(returns, annualized=True, periods_in_year=periods_in_year)\n",
    "        csv_dict['ts_mean_ann'] = calcTSAvgReturn(returns, annualized=True, periods_in_year=periods_in_year)\n",
    "        for lev in [2, 3, 4]:\n",
    "            lev_pos = positions*lev\n",
    "            lev_tc = calcTransactionCosts(lev_pos)\n",
    "            lev_returns = lev_pos*ys - lev_tc \n",
    "            csv_dict['geom_mean_x'+str(lev)] = calcGeomAvg(lev_returns)\n",
    "            csv_dict['sharpe_x'+str(lev)] = calcSharpe(lev_returns, periods_in_year=periods_in_year)\n",
    "            csv_dict['sd_ann_x'+str(lev)] = calcSD(lev_returns, annualized=True, periods_in_year=periods_in_year)\n",
    "            csv_dict['ts_mean_ann_x'+str(lev)] = calcTSAvgReturn(lev_returns, annualized=True, periods_in_year=periods_in_year)\n",
    "        csv_dict_list.append(csv_dict)\n",
    "        results_df = pd.DataFrame(csv_dict_list)\n",
    "\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fp = out_fp+'_'+arch_name+'_'+timestr+'.csv'\n",
    "        results_df.to_csv(fp, index=False)\n",
    "\n",
    "        # output results to track\n",
    "        print(csv_dict['runtime_mins'])\n",
    "        print(f\"val r2_pred: {csv_dict['r2_pred_val']:.4f}\")\n",
    "        print(f\"val sharpe: {csv_dict['sharpe_ann']:.4f}\")\n",
    "\n",
    "        print('\\n\\n')\n",
    "        gc.collect()\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotYvsYhat(y: np.ndarray, yhats: np.ndarray, num_quantiles: int = 10) -> None:\n",
    "    \"\"\" Plot the average values of y (returns) against bins of yhat (predicted returns).\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): A vector of target variable.\n",
    "        yhats (np.ndarray): A vector of fitted values.\n",
    "        num_quantiles (int): The number of quantiles to bin yhats into. Default is 10.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with y and yhats\n",
    "    temp_df = pd.DataFrame({'y': y, 'yhat': yhats})\n",
    "\n",
    "    # Bin yhats into quantiles\n",
    "    temp_df['yhat'] = pd.qcut(temp_df['yhat'], q=num_quantiles, labels=False)\n",
    "\n",
    "    # Group y by yhat and plot the means\n",
    "    temp_df.groupby('yhat')['y'].mean().plot()\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title('Mean Return vs. Quantiles of Predicted Returns')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0acab751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenTestYhats(df, opt_hps, test_year=2021): \n",
    "    test_weeks = np.unique(df[df.index.year == test_year].index.values)\n",
    "    \n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "    for test_week in test_weeks:\n",
    "        # TODO DO SOME LOGIC TO FIGURE OUT THE WINDOWS TO FIT A MODEL FOR SO I LOOP OVER THOSE INITIAL DATES\n",
    "        # -fit teh model once\n",
    "        # -then predict on all subsequent weeks\n",
    "        # -maybe one month at time?\n",
    "        # -maybe one week at a time?\n",
    "        # -let's try once a month to see how it does\n",
    "        print(test_week, '\\n')\n",
    "        temp_df  = df[df.index <= test_week].copy()\n",
    "        temp_df  = subsetToAssetUniversePerWeek(temp_df, asset_universe_dict, oos_week=test_week)\n",
    "        train_df = temp_df[temp_df.index < test_week].copy()\n",
    "        oos_df   = temp_df[temp_df.index == test_week].copy()\n",
    "        \n",
    "        models   = fitAutoencoder(train_df, opt_hps, val_df=None, early_stopping=False)\n",
    "        yhats    = genYhats(oos_df, models, test_week, opt_hps['number_factor'])\n",
    "       \n",
    "        oos_df = oos_df[['asset', 'r_tplus7']]\n",
    "        oos_df['yhat'] = yhats\n",
    "        test_df = pd.concat((test_df, oos_df))\n",
    "        rw_mse = np.mean(np.square(test_df.r_tplus7.values))\n",
    "        model_mse = np.mean(np.square(test_df.r_tplus7.values - test_df.yhat.values))\n",
    "        print('\\n test random walk mse: ' + str(rw_mse))\n",
    "        print('\\n test model mse: ' + str(model_mse))\n",
    "        print('winning?: ' + str(model_mse < rw_mse))\n",
    "        print('\\n')\n",
    "    \n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c19e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTestPeriod(df: pd.DataFrame,\n",
    "    lhs_col: str,\n",
    "    test_start_date: str,\n",
    "    test_end_date: str,\n",
    "    opt_model_dict: dict,\n",
    "    num_cpus: int) -> np.array:\n",
    "    ''' predict for test period using backtested model.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): panel of training and test data with date index, LHS variable named \n",
    "                           `lhs_col`, and remaining cols are RHS features.\n",
    "        lhs_col (str): name of the lhs target column.\n",
    "        test_start_date (str): the first date for the test period.\n",
    "        test_end_date (str): the last date for the test period.\n",
    "        opt_model_dict (dict): hyperparameters for optimal backtested model.\n",
    "        num_cpus (int): number of cpus to use when parallelizing.\n",
    "    \n",
    "    Returns:\n",
    "        yhats (np.array): vector of yhat positions for entire test period.\n",
    "    '''\n",
    "    # form list of rhs col\n",
    "    rhs_cols = list(df.columns.values)\n",
    "    rhs_cols.remove(lhs_col)\n",
    "\n",
    "    # form lhs for oos df\n",
    "    oos_df = df.copy()\n",
    "    ret_threshold = opt_model_dict['ret_threshold']\n",
    "    oos_df['y'] = 1\n",
    "    oos_df.loc[oos_df[lhs_col] > ret_threshold, 'y'] = 2\n",
    "    oos_df.loc[oos_df[lhs_col] < -ret_threshold, 'y'] = 0\n",
    "\n",
    "    # form test dates to loop over\n",
    "    test_dates = np.unique(oos_df[test_start_date:test_end_date].index.values)\n",
    "\n",
    "    def loopOverTestDates(test_date):\n",
    "        # form train and val data\n",
    "        temp_df   = oos_df[oos_df.index <= test_date].copy()\n",
    "        train_df  = temp_df[temp_df.index < test_date].copy()\n",
    "        train_y_btc_eth_diff = train_df[lhs_col].values\n",
    "        train_df  = train_df.drop(lhs_col, axis=1)\n",
    "        test_df   = temp_df[temp_df.index == test_date].copy()\n",
    "        test_df   = test_df.drop(lhs_col, axis=1)\n",
    "\n",
    "        # fit and predict\n",
    "        model, train_acc, train_yhats = fitRF(train_df, 'y', rhs_cols, \n",
    "                                            hps=opt_model_dict, \n",
    "                                            ys_real=train_y_btc_eth_diff)\n",
    "        yhats = genRFYhats(test_df, model, 'y', rhs_cols)\n",
    "\n",
    "        return yhats, model, train_acc, train_yhats\n",
    "\n",
    "    # predict for entire test period\n",
    "    test_results = Parallel(n_jobs=int(num_cpus/4))(delayed(loopOverTestDates)(test_date) for test_date in tqdm(test_dates))\n",
    "\n",
    "    # extract test yhats to return\n",
    "    yhats_list = []\n",
    "    for t in range(len(test_results)):\n",
    "        yhats_list.append(test_results[t][0])\n",
    "    yhats = np.array(yhats_list)-1\n",
    "\n",
    "    return yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportTestPeriodResults(yhats: np.array, y_real: np.array):\n",
    "    ''' report various portfolio statistics for the test period.\n",
    "    \n",
    "    Args:\n",
    "        yhats (np.array): vector of actual portfolio positions.\n",
    "        y_real (np.array): vector of real return difference for target instrument.\n",
    "    '''\n",
    "    # calculate returns post transaction costs\n",
    "    tc = calcTransactionCosts(yhats)\n",
    "    returns = yhats*y_real - tc\n",
    "    \n",
    "    # calc portfolio statistics\n",
    "    geom_mean = calcGeomAvg(returns)\n",
    "    sharpe    = calcSharpe(returns, periods_in_year=365*12)\n",
    "    max_dd    = calcMaxDrawdown(returns)\n",
    "    max_1week = calcMaxOneWeekLoss(returns, periods_in_week=7*12)\n",
    "\n",
    "    # report\n",
    "    print(f\"Geometric average 2 hour return: {geom_mean:.6f}\")\n",
    "    print(f\"Annualized Sharpe: {sharpe:.3f}\")\n",
    "    print(f\"Maximum drawdown: {max_dd:.4f}\")\n",
    "    print(f\"Maximum loss over any one week period: {max_1week:.4f}\")\n",
    "\n",
    "    # plot classification\n",
    "    plotYvsYhat(y=y_real, yhats=yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e383f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO RUN UNIFACTOR IMPORTANCE ON CHARACTERISTICS TO CHOOSE 20-100 PENDING WHEN I GET ENOUGH ASSETS IN THE PANEL.\n",
    "# -FOCUS ON RAW STUFF FOR NOW AS I WANT TO SHOW TRANSFORMER LEARNS TEMPORAL RELATIONSHIPS\n",
    "# --CAN GIVE IT THE FANCY CHARACTERISTICS IN SECOND RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    IN_FP           = '../../data/clean/btcusdt-1h-eng_feats.pkl'\n",
    "    CV_OUT_FP       = '../output/cv_results'\n",
    "    LHS_COL         = 'r_tplus1'\n",
    "    VAL_START_DATE  = '2023-01-01'\n",
    "    VAL_END_DATE    = '2023-02-28'\n",
    "    TEST_START_DATE = '2023-03-01'\n",
    "    NUM_CPUS        = 2 # TODO change to 22 when doing on desktop\n",
    "    PERIODS_IN_YEAR = 365*24\n",
    "\n",
    "    # read in data\n",
    "    df = pd.read_pickle(IN_FP)\n",
    "    df = df.set_index('timestamp')\n",
    "    df = df.astype('float32')\n",
    "\n",
    "    # cv rf\n",
    "    VAL_START_DATE = '2023-01-01'\n",
    "    VAL_END_DATE = '2023-01-14'\n",
    "    results_rf_list = runCV(df[:VAL_END_DATE], LHS_COL, VAL_START_DATE, \n",
    "                            NUM_CPUS, 'rf_reg_1hour', CV_OUT_FP, PERIODS_IN_YEAR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Update CV method to strip out everything that will be common across CV's to make them functions: fitting on all val dates, gen CV results, others?\n",
    "# TODO ADD function to report max dd and add it to the CV results func to have in the csv\n",
    "# TODO add to csv to report 5 stat summary of yhats\n",
    "# TODO add to csv to report num of position changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeadabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pull helper functions out of first cell in backtest notebook to create a basic class with GPT's help of quant tools that i put in some folder on my computer of my python methods\n",
    "# maybe move into a folder where i keep my python tools\n",
    "# make it a class so i import as needed\n",
    "# create a repo for that\n",
    "# ask chatgpt how to structure it in various python files\n",
    "# maybe ask for a template folder and file structure for the library\n",
    "# add a pointer to it for the automa capital stuff\n",
    "# TODO Move helper functions to a call in utils file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f703b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "# load in the data\n",
    "input_fp = '../3-data/clean/panel_train.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "df = df.sort_values(by=['date', 'asset'])\n",
    "with open('../3-data/clean/asset_universe_dates_and_lists.pkl', 'rb') as handle:\n",
    "    asset_universe_dict = pickle.load(handle)\n",
    "\n",
    "# drop rows and columns such that data will work for conditional autoencoder (CA)\n",
    "df = dropRowsAndColsForCA(df)\n",
    "\n",
    "# form the char-sorted portfolios for factor side of CA input\n",
    "df = formPortfolioReturnCovariates(df)\n",
    "\n",
    "# sort the data\n",
    "df = df.sort_values(by=['date', 'asset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO PASS IN FUNC FOR FITTING MODEL\n",
    "# TODO PASS IN FUNC FOR PREDICTING WITH MODEL\n",
    "# TODO PASS IN PARM DIC FOR BOTH FIT AND PREDICT SO I CAN JUST UPDATE TEH DYNAMIC PARAMS WHILE REST ARE STATIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run CV\n",
    "hps_yhats_list = runCV(df, asset_universe_dict, last_train_year = 2018, val_end_year=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add code to select the best hps combo from the above list\n",
    "#       i just ran manually with setting the best CV point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form validation period results\n",
    "df = df[['asset', 'week_idx', 'r_tplus7']]\n",
    "df = subsetToAssetUniverseFull(df, asset_universe_dict, train_or_test='train')\n",
    "df = df[df.index.year >= 2019]\n",
    "df['yhat'] = hps_yhats_list[0]['yhats']\n",
    "df      = labelPortfolioWeights(df)\n",
    "annl_tc = calcAnnualTransactionCosts(df)\n",
    "df['r'] = df.prtfl_wght*df.r_tplus7\n",
    "r_df    = df.groupby(['date'])[['r']].sum()\n",
    "annl_ret = calcPortfolioReturn(r_df)\n",
    "annl_sharpe = calcPortfolioSharpe(r_df)\n",
    "max_dd = max_draw_down(r_df)\n",
    "max_1mo_loss = max_1_month_loss(r_df)\n",
    "print('annual transaction costs in simple return terms: ' + str(np.round(annl_tc, 4)))\n",
    "print('annual simple return before trans costs: ' + str(np.round(annl_ret, 4)))\n",
    "print('annual sharpe: '+str(np.round(annl_sharpe, 2)))\n",
    "print('max drawdown : '+str(np.round(max_dd, 2)))\n",
    "print('max one month loss : '+str(np.round(max_1mo_loss, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO GO SCOPE OLD NOTEBOOK FOR LATEST NUMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cdeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean up data for test fitting\n",
    "input_fp = '../3-data/clean/panel_oos.pkl'\n",
    "df = pd.read_pickle(input_fp)\n",
    "df = df.set_index('date')\n",
    "df = df.sort_values(by=['date', 'asset'])\n",
    "opt_hps = hps_yhats_list[0].copy()\n",
    "del opt_hps['yhats']\n",
    "del opt_hps['ys']\n",
    "test_df = GenTestYhats(df, opt_hps) \n",
    "test_df = test_df.merge(df[['asset', 'week_idx']],\n",
    "                        on=['date', 'asset'],\n",
    "                        how='inner', \n",
    "                        validate='one_to_one')\n",
    "\n",
    "# form test period results\n",
    "test_df = labelPortfolioWeights(test_df)\n",
    "annl_tc = calcAnnualTransactionCosts(test_df)\n",
    "test_df['r'] = test_df.prtfl_wght*test_df.r_tplus7\n",
    "r_df    = test_df.groupby(['date'])[['r']].sum()\n",
    "annl_ret = calcPortfolioReturn(r_df)\n",
    "annl_sharpe = calcPortfolioSharpe(r_df)\n",
    "max_dd = max_draw_down(r_df)\n",
    "max_1mo_loss = max_1_month_loss(r_df)\n",
    "print('annual transaction costs in simple return terms: ' + str(np.round(annl_tc, 4)))\n",
    "print('annual simple return before trans costs: ' + str(np.round(annl_ret, 4)))\n",
    "print('annual sharpe: '+str(np.round(annl_sharpe, 2)))\n",
    "print('max drawdown : '+str(np.round(max_dd, 2)))\n",
    "print('max one month loss : '+str(np.round(max_1mo_loss, 2)))\n",
    "\n",
    "# TODO GO SCOPE OLD NOTEBOOK FOR LATEST NUMBERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ALSO NEED TO TRY A MODEL WHERE WE SET MISSING ASSETS LHS \n",
    "# AND RHS TO -2 AND THEN WE HAVE BALANCED PANEL SO WE FIT ON THE \n",
    "# MATRIX OF RHS ACROSS ASSETS AS OPPOSED OT INDIVIDUAL DATE-ASSET \n",
    "# AND THEN THE NETWORK LEARNS ASSET-SPECIFIC PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO THIS NOTEBOOK ENDS WITH SAVING A DF OF DATE, ASSET, PREDICTED RETURN FOR A VAL OR TEST PERIOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Figure out why all the test period yhats are close to zero\n",
    "#       use the comments to report out the distribution \n",
    "#       to figure out if factor side or beta side that is turning all to zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
