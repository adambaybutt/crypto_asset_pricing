{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a28048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import san\n",
    "import pickle\n",
    "from typing import Dict, List, Any, Callable\n",
    "from helper_functions import Helper\n",
    "import retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5663e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retrying.retry(\n",
    "    wait_exponential_multiplier=1000,\n",
    "    wait_exponential_max=20000,\n",
    "    stop_max_attempt_number=3\n",
    ")\n",
    "def callSanFunction(san_function: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:\n",
    "    \"\"\"Calls a function in the 'san' library with retrying.\n",
    "\n",
    "    Args:\n",
    "        san_function: The function to call from the 'san' library.\n",
    "        *args: Positional arguments to pass to the function.\n",
    "        **kwargs: Keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        The return value of the 'san_function' call.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = san_function(*args, **kwargs)\n",
    "        return result\n",
    "    except retrying.RetryError as e:\n",
    "        print(f\"Request failed after {e.last_attempt_time}s of retries.\")\n",
    "        raise\n",
    "    except retrying.Retrying as r:\n",
    "        remaining_time = round((r.wait_time - r.elapsed), 2)\n",
    "        print(f\"Request failed. Retrying in {remaining_time}s...\")\n",
    "        raise\n",
    "    except Exception:\n",
    "        print(\"An error occurred while calling the 'san_function'.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a47195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formSantimentAssetUniverse(asset_universe: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" form the crosswalk for santiment to coinmetrics assets.\n",
    "\n",
    "    Args:\n",
    "        asset_universe (List[str]): Coinmetrics asset IDs.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): crosswalk between asset_san and asset_cm with column for category_san.    \n",
    "    \"\"\"\n",
    "    # Set slugs to drop that duplciates\n",
    "    slugs_to_drop = ['o-balancer', 'p-balancer', 'farmatrust', 'planet', 'plair', 'invacio', 'nftx-hashmasks-index', \n",
    "                    'truebit', 'game', 'bonded-finance', 'o-aave', 'bnb-aave', 'p-aave', 'p-chainlink', \n",
    "                    'arb-chainlink', 'bnb-chainlink', 'o-chainlink', 'mir-coin', 'p-matic-network', 'bnb-ankr',\n",
    "                    'bnb-cardano', 'o-perpetual-protocol', 'bnb-mines-of-dalarnia', 'p-quickswap', \n",
    "                    'bnb-synthetix-network-token', 'o-synthetix-network-token', 'arb-curve', 'p-uniswap',\n",
    "                    'bnb-uniswap', 'arb-stargate-finance', 'arb-sushi', 'bnb-sushi', 'gomining-token', \n",
    "                    'bnb-swipe', 'bnb-1inch', 'bnb-chromia', 'bnb-myneighboralice', 'bnb-alpha-finance-lab', \n",
    "                    'bnb-avalanche', 'bnb-axie-infinity']\n",
    "    \n",
    "    # TODO FIND CRO/CRONOS, GMT/STEPN, ORCA\n",
    "\n",
    "    # specify san endpoint\n",
    "    san_function = san.get\n",
    "    san_args = (\"projects/all\",) \n",
    "\n",
    "    # make the call\n",
    "    result = callSanFunction(san_function, *san_args)\n",
    "\n",
    "    # convert to dataframe object\n",
    "    assert type(result) == pd.DataFrame\n",
    "    df = result.copy()\n",
    "\n",
    "    # manually add asset that we are missig\n",
    "    manual_df = df[df.slug.isin(['nano'])][['marketSegment', 'slug']]\n",
    "    manual_df['symbol_lower'] = 'nano'\n",
    "\n",
    "    # subset down to matched assets\n",
    "    df['symbol_lower'] = df.ticker.str.lower()\n",
    "    df = df[df.symbol_lower.isin(asset_universe)]\n",
    "\n",
    "    # remove duplicated assets\n",
    "    df = df[~df.slug.isin(slugs_to_drop)]\n",
    "\n",
    "    # subset to relevant columns and append other asset(s)\n",
    "    df = df[['marketSegment', 'slug', 'symbol_lower']]\n",
    "    df = pd.concat([manual_df, df])\n",
    "\n",
    "    # rename\n",
    "    df = df.rename(columns={'marketSegment': 'category_san', 'slug': 'asset_san', 'symbol_lower': 'asset_cm'})\n",
    "\n",
    "    # confirm full one to one mapping, knowing we are missing orca\n",
    "    asset_universe.remove('orca')\n",
    "    assert len(asset_universe) == np.sum(np.unique(df.asset_cm.values) == np.unique(asset_universe))\n",
    "\n",
    "    # return\n",
    "    return df[['asset_san', 'asset_cm', 'category_san']].sort_values(by='asset_cm', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59e55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formAssetMetricsDicts(san_slug_universe: List[str]) -> tuple:\n",
    "    \"\"\" form dictionaries of asset slugs and associated metrics that we could pull and that we will pull.\n",
    "\n",
    "    Args:\n",
    "        san_slug_universe (List[str]): list of strings of Santiment slugs in study universe.\n",
    "    \n",
    "    Returns:\n",
    "        (tuple): dictionaries of asset slugs and associated metrics that we could pull and that we will pull.\n",
    "    \"\"\"\n",
    "    # define metrics of interest at asset level\n",
    "    asset_metrics_to_include = ['active_addresses_1h', 'active_deposits', 'active_deposits_per_exchange',\n",
    "        'active_holders_distribution_combined_balance_over_1', 'active_holders_distribution_combined_balance_over_10',\n",
    "        'active_holders_distribution_combined_balance_over_100', 'active_holders_distribution_combined_balance_over_100k',\n",
    "        'active_holders_distribution_combined_balance_over_10k', 'active_holders_distribution_combined_balance_over_1M',\n",
    "        'active_holders_distribution_combined_balance_over_1k', 'active_holders_distribution_combined_balance_total',\n",
    "        'active_holders_distribution_over_1', 'active_holders_distribution_over_10', 'active_holders_distribution_over_100',\n",
    "        'active_holders_distribution_over_100k', 'active_holders_distribution_over_10k', 'active_holders_distribution_over_1M',\n",
    "        'active_holders_distribution_over_1k', 'active_holders_distribution_total', 'active_withdrawals',\n",
    "        'active_withdrawals_per_exchange', 'age_consumed', 'age_destroyed', 'all_known_balance',\n",
    "        'amount_in_exchange_top_holders', 'amount_in_non_exchange_top_holders', 'amount_in_top_holders',\n",
    "        'cex_balance', 'cexes_to_defi_flow', 'cexes_to_dex_flow', 'cexes_to_dex_traders_flow', 'cexes_to_traders_flow',\n",
    "        'cexes_to_whale_flow', 'circulation', 'circulation_1d', 'circulation_2y', 'circulation_30d', 'circulation_365d',\n",
    "        'circulation_3y', 'circulation_5y', 'circulation_7d', 'circulation_90d', \n",
    "        'daily_active_addresses', 'defi_balance', 'defi_cex_balance', 'defi_dex_balance', 'defi_exchange_balance', \n",
    "        'defi_to_cexes_flow', 'defi_to_dex_traders_flow', 'defi_to_dexes_flow', 'defi_to_exchanges_flow', 'defi_to_traders_flow',\n",
    "        'defi_to_whale_flow', 'deposit_balance', 'deposit_transactions', 'deposit_transactions_per_exchange', 'dev_activity',\n",
    "        'dev_activity_contributors_count', 'dex_balance', 'dex_cex_balance', 'dex_to_cexes_flow', 'dex_trader_balance',\n",
    "        'dex_traders_cex_balance', 'dex_traders_defi_balance', 'dex_traders_dex_balance',\n",
    "        'dex_traders_exchange_balance', 'dex_traders_to_cexes_flow', 'dex_traders_to_defi_flow', 'dex_traders_to_dexes_flow', \n",
    "        'dex_traders_to_exchanges_flow', 'dex_traders_to_whale_flow', 'dex_traders_whale_balance', 'dexes_to_defi_flow',\n",
    "        'dexes_to_dex_traders_flow', 'dexes_to_traders_flow', 'dexes_to_whale_flow', 'dormant_circulation_180d', \n",
    "        'dormant_circulation_365d', 'dormant_circulation_90d', 'exchange_balance', 'exchange_inflow', 'exchange_inflow_usd',\n",
    "        'exchange_outflow', 'exchange_outflow_usd', 'exchanges_to_defi_flow', 'exchanges_to_dex_traders_flow',\n",
    "        'exchanges_to_genesis_flow', 'exchanges_to_traders_flow', 'exchanges_to_whales_flow',\n",
    "        'github_activity', 'github_activity_contributors_count',\n",
    "        'holders_distribution_combined_balance_over_1', 'holders_distribution_combined_balance_over_10',\n",
    "        'holders_distribution_combined_balance_over_100', 'holders_distribution_combined_balance_over_100k',\n",
    "        'holders_distribution_combined_balance_over_10k', 'holders_distribution_combined_balance_over_1M',\n",
    "        'holders_distribution_combined_balance_over_1k', 'holders_distribution_combined_balance_total',\n",
    "        'holders_distribution_over_1', 'holders_distribution_over_10', 'holders_distribution_over_100',\n",
    "        'holders_distribution_over_100k', 'holders_distribution_over_10k', 'holders_distribution_over_1M', \n",
    "        'holders_distribution_over_1k', 'holders_distribution_total',\n",
    "        'marketcap_usd', 'mean_age', 'mean_dollar_invested_age', 'mean_realized_price_usd',\n",
    "        'mvrv_long_short_diff_usd', 'mvrv_usd',\n",
    "        'network_growth', 'nvt', 'nvt_transaction_volume', 'payments_count', 'percent_of_total_supply_in_profit',\n",
    "        'percent_of_total_supply_on_exchanges', 'price_usd', 'realized_value_usd', 'sentiment_balance_reddit', 'sentiment_balance_total',\n",
    "        'sentiment_balance_twitter', 'sentiment_balance_twitter_crypto', 'sentiment_negative_reddit', 'sentiment_negative_total',\n",
    "        'sentiment_negative_twitter', 'sentiment_negative_twitter_crypto', 'sentiment_positive_reddit', 'sentiment_positive_total',\n",
    "        'sentiment_positive_twitter', 'sentiment_positive_twitter_crypto', 'sentiment_volume_consumed_reddit',\n",
    "        'sentiment_volume_consumed_total','sentiment_volume_consumed_twitter', 'sentiment_volume_consumed_twitter_crypto', \n",
    "        'social_dominance_reddit', 'social_dominance_total', 'social_dominance_twitter', 'social_dominance_twitter_crypto',\n",
    "        'social_volume_reddit', 'social_volume_total', 'social_volume_twitter', 'social_volume_twitter_crypto', 'stock_to_flow',\n",
    "        'supply_on_exchanges', 'supply_outside_exchanges', 'total_supply', 'total_supply_in_profit', 'trader_balance',\n",
    "        'traders_cex_balance', 'traders_defi_balance', 'traders_dex_balance', 'traders_exchange_balance', 'traders_to_cexes_flow',\n",
    "        'traders_to_defi_flow', 'traders_to_dexes_flow', 'traders_to_exchanges_flow', 'traders_to_whale_flow',\n",
    "        'traders_whale_balance', 'transaction_volume', 'transactions_count', 'unique_social_volume_total_1h',\n",
    "        'volume_usd', 'whale_balance', 'whale_cex_balance', 'whale_defi_balance', 'whale_dex_balance', 'whale_to_cexes_flow',\n",
    "        'whale_to_defi_flow', 'whale_to_dex_traders_flow', 'whale_to_dexes_flow', 'whale_to_traders_flow', 'whales_exchange_balance',\n",
    "        'whales_to_exchanges_flow', 'withdrawal_balance', 'withdrawal_transactions']\n",
    "\n",
    "    # specify san endpoint\n",
    "    san_function = san.available_metrics_for_slug\n",
    "\n",
    "    # initialize list for the metrics\n",
    "    assets_metrics_dict = {}\n",
    "\n",
    "    # loop over all assets to build list of all available metrics\n",
    "    for i, san_slug in enumerate(san_slug_universe):\n",
    "        # update progress\n",
    "        progress = (i + 1) / len(san_slug_universe) * 100\n",
    "        print(f\"Processing asset number #{i+1} ({progress:.2f}%): {san_slug}\")\n",
    "\n",
    "        # make the call\n",
    "        san_args = (san_slug,) \n",
    "        result = callSanFunction(san_function, *san_args)\n",
    "        if type(result) == list:\n",
    "            assets_metrics_dict[san_slug] = result\n",
    "        else:\n",
    "            print(f\"Did not obtain data for asset {san_slug}\")\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # form dictionary of assets and metrics to pull\n",
    "    assets_metrics_to_pull_dict = {}\n",
    "    for key in assets_metrics_dict:\n",
    "        assets_metrics_to_pull_dict[key] = [value for value in assets_metrics_dict[key] if value in asset_metrics_to_include]\n",
    "\n",
    "    # return both what to pull and a master list\n",
    "    return assets_metrics_to_pull_dict, assets_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e0651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullAssetMetrics(study_start: str, study_end: str, \n",
    "    assets_metrics_dict_to_pull: Dict[str, List[str]], san_df: pd.DataFrame, daily_panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pulls asset metrics from the Sanbase API for a specified date range and set of assets.\n",
    "\n",
    "    Args:\n",
    "        study_start (str): The start date of the study in string format ('YYYY-MM-DD').\n",
    "        study_end (str): The end date of the study in string format ('YYYY-MM-DD').\n",
    "        assets_metrics_dict_to_pull (Dict[str, List[str]]): A dictionary mapping asset names to lists of metric names to pull for each asset.\n",
    "        san_df (pd.DataFrame): A dataframe containing mappings between asset names used by Sanbase and Coinmetrics.\n",
    "        daily_panel_df (pd.DataFrame): A dataframe containing daily panel data for all assets.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing the pulled asset metrics for each asset and datetime.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert strings to datetimes\n",
    "    study_start_dt = np.datetime64(study_start)\n",
    "    study_end_dt = np.datetime64(study_end)\n",
    "\n",
    "    # Form list of assets\n",
    "    assets = list(assets_metrics_dict_to_pull.keys())\n",
    "\n",
    "    # Initialize a dataframe for the results\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Loop over assets\n",
    "    for i, asset in enumerate(assets):\n",
    "        # Obtain CoinMetrics name for obtaining dates\n",
    "        asset_cm = san_df.loc[san_df['asset_san'] == asset, 'asset_cm'].values[0]\n",
    "\n",
    "        # Monitor progress\n",
    "        print(f\"Processing asset #{i+1} ({(i+1)/len(assets)*100:.2f}%): {asset}\")\n",
    "\n",
    "        # Obtain metrics for this asset\n",
    "        metrics = assets_metrics_dict_to_pull[asset]\n",
    "        \n",
    "        # Determine date range for this asset\n",
    "        asset_dates = daily_panel_df.loc[daily_panel_df['asset'] == asset_cm, 'date'].values\n",
    "        asset_min_date = np.min(asset_dates)\n",
    "        asset_max_date = np.max(asset_dates)\n",
    "        if study_start_dt >= asset_min_date:\n",
    "            start_date = np.datetime_as_string(study_start_dt, 'D')\n",
    "        else:\n",
    "            start_date = np.datetime_as_string(asset_min_date, 'D')\n",
    "        if study_end_dt <= asset_max_date:\n",
    "            end_date = np.datetime_as_string(study_end_dt, 'D') \n",
    "        else:\n",
    "            end_date = np.datetime_as_string(asset_max_date, 'D') \n",
    "\n",
    "        # Loop over metrics to pull\n",
    "        asset_df = pd.DataFrame(data={'datetime': [], 'asset': []})\n",
    "        for metric in metrics:\n",
    "            # Monitor\n",
    "            print(metric)\n",
    "            \n",
    "            # Pull at hourly level\n",
    "            san_args = (metric, ) \n",
    "            san_kwargs = {'slug': asset,\n",
    "                        'from_date': start_date,\n",
    "                        'to_date': end_date,\n",
    "                        'interval': '1h'}\n",
    "            temp_df = callSanFunction(san.get, *san_args, **san_kwargs)\n",
    "\n",
    "            # If it returns nothing, then try to pull at daily level\n",
    "            if (not isinstance(temp_df, pd.DataFrame)) | (len(temp_df) == 0):\n",
    "                san_kwargs['interval'] = '1d'\n",
    "                temp_df = callSanFunction(san.get, *san_args, **san_kwargs)\n",
    "\n",
    "            # If still no results then report and carry on\n",
    "            if (not isinstance(temp_df, pd.DataFrame)) | (len(temp_df) == 0):\n",
    "                print(f\"No data for {asset} and {metric}.\")\n",
    "                continue\n",
    "                \n",
    "            # quick cleaning\n",
    "            temp_df = temp_df.rename(columns={'value': metric})\n",
    "            temp_df['asset'] = asset\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df[['datetime', 'asset', metric]]\n",
    "\n",
    "            # Merge onto master df\n",
    "            asset_df = asset_df.merge(temp_df, on=['datetime', 'asset'], how='outer', validate='one_to_one')\n",
    "\n",
    "            # Space out the calls\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # Append the asset's results to the master df to return\n",
    "        df = pd.concat([df, asset_df])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5adc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullMacroMetrics(study_start: str, study_end: str, san_slug_universe: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Pull the panel data for specified macro metrics for the relevant study time period.\n",
    "\n",
    "    Args:\n",
    "        study_start (str): The start date of the study in string format ('YYYY-MM-DD').\n",
    "        study_end (str): The end date of the study in string format ('YYYY-MM-DD').\n",
    "        san_slug_universe (List[str]): A list of strings of Santimnet unique slug IDs in the study universe.\n",
    "    \n",
    "    Returns:\n",
    "        (pd.DataFrame): A panel dataframe containing the pulled macro metrics for all assets and datetimes.\n",
    "    \"\"\"\n",
    "    # Form dictionary of macro metrics with empty lists for associated assets.\n",
    "    macro_metrics = ['aave_v2_stable_borrow_apy', 'aave_v2_supply_apy', 'aave_v2_total_borrowed_usd',\n",
    "        'aave_v2_total_deposits_usd', 'aave_v2_total_liquidations_usd', 'aave_v2_total_new_debt_usd',\n",
    "        'aave_v2_total_supplied_usd', 'aave_v2_variable_borrow_apy', 'average_fees_usd',\n",
    "        'compound_total_borrowed_usd', 'compound_total_deposits_usd', 'compound_total_liquidations_usd',\n",
    "        'compound_total_new_debt_usd', 'compound_total_supplied_usd', 'dai_created', 'dai_repaid',\n",
    "        'eth2_roi', 'eth2_stakers_count', 'fees_usd',\n",
    "        'makerdao_total_borrowed_usd', 'makerdao_total_deposits_usd', 'makerdao_total_supplied_usd',\n",
    "        'mcd_collat_ratio', 'mcd_liquidation', 'mcd_locked_token', 'median_fees_usd',\n",
    "        'mvrv_usd_intraday', 'nft_retail_trade_volume_usd', 'nft_retail_trades_count',\n",
    "        'nft_trade_volume_usd', 'nft_trades_count', 'nft_whale_trade_volume_usd',\n",
    "        'nft_whale_trades_count', 'total_assets_issued', 'total_trade_volume_by_dex',\n",
    "        'uniswap_total_claims_amount', 'uniswap_total_lp_claims_amount', 'uniswap_total_user_claims_amount',\n",
    "        'usdt_binance_funding_rate', 'usdt_binance_open_interest', 'usdt_binance_open_value']\n",
    "    macro_metrics_assets_dict = {key: [] for key in macro_metrics}\n",
    "\n",
    "    # Obtain assets for each metric\n",
    "    for metric in macro_metrics_assets_dict.keys():\n",
    "        san_args = (metric, )\n",
    "        san_kwargs = {'arr': [\"availableSlugs\", \"isAccessible\"]}\n",
    "        metric_metadata_dict = callSanFunction(san.metadata, *san_args, **san_kwargs)\n",
    "        if metric_metadata_dict['isAccessible']:\n",
    "            macro_metrics_assets_dict[metric] = metric_metadata_dict['availableSlugs']\n",
    "        else:\n",
    "            macro_metrics_assets_dict.pop(metric)\n",
    "        if metric[:5] == 'usdt_':\n",
    "            macro_metrics_assets_dict[metric] = set(metric_metadata_dict['availableSlugs']).intersection(set(san_slug_universe))\n",
    "\n",
    "    # Form list of metrics\n",
    "    metrics = list(macro_metrics_assets_dict.keys())\n",
    "\n",
    "    # Initialize a dataframe for the results\n",
    "    df = pd.DataFrame(data={'datetime': [], 'asset': []})\n",
    "\n",
    "    # Loop over macro metrics\n",
    "    for i, metric in enumerate(metrics):\n",
    "        # Monitor progress\n",
    "        print(f\"Processing metric #{i+1} ({(i+1)/len(metrics)*100:.2f}%): {metric}\")\n",
    "\n",
    "        # Obtain assets for this metric\n",
    "        assets = macro_metrics_assets_dict[metric]\n",
    "\n",
    "        # Check if no assets then just pull without slug name\n",
    "        if len(assets) == 0:\n",
    "            san_args = (metric, ) \n",
    "            san_kwargs = {'from_date': study_start,\n",
    "                          'to_date': study_end,\n",
    "                          'interval': '1h'}\n",
    "            temp_df = callSanFunction(san.get, *san_args, **san_kwargs)\n",
    "\n",
    "            temp_df = temp_df.rename(columns={'value': metric})\n",
    "            temp_df['asset'] = 'all'\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df[['datetime', 'asset', metric]]\n",
    "\n",
    "            metric_df = temp_df.copy()\n",
    "            continue # skip to next metric\n",
    "        \n",
    "        # Loop over assets to pull\n",
    "        metric_df = pd.DataFrame(data={'datetime': [], 'asset': []})\n",
    "        for asset in assets:\n",
    "            # Pull at hourly level\n",
    "            san_args = (metric, ) \n",
    "            san_kwargs = {'slug': asset,\n",
    "                          'from_date': study_start,\n",
    "                          'to_date': study_end,\n",
    "                          'interval': '1h'}\n",
    "            temp_df = callSanFunction(san.get, *san_args, **san_kwargs)\n",
    "\n",
    "            # If it returns nothing, then try to pull at daily level\n",
    "            if (not isinstance(temp_df, pd.DataFrame)) | (len(temp_df) == 0):\n",
    "                san_kwargs['interval'] = '1d'\n",
    "                temp_df = callSanFunction(san.get, *san_args, **san_kwargs)\n",
    "\n",
    "            # If still no results then report and carry on\n",
    "            if (not isinstance(temp_df, pd.DataFrame)) | (len(temp_df) == 0):\n",
    "                print(f\"No data for {asset} and {metric}.\")\n",
    "                continue\n",
    "                \n",
    "            # quick cleaning\n",
    "            temp_df = temp_df.rename(columns={'value': metric})\n",
    "            temp_df['asset'] = asset\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df[['datetime', 'asset', metric]]\n",
    "\n",
    "            # Append on the new asset's metrics\n",
    "            metric_df = pd.concat([metric_df, temp_df])\n",
    "\n",
    "            # Space out the calls\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # Merge together metrics for given assets-datetimes\n",
    "        df = df.merge(metric_df, on=['datetime', 'asset'], how='outer', validate='one_to_one')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set args\n",
    "    CW_IN_FP = '../data/derived/cm_to_coinapi_cw.pkl'\n",
    "    ASSET_IN_FP = '../data/derived/asset_universe_dict.pickle'\n",
    "    PANEL_DAILY_IN_FP = '../data/derived/basic_panel.pkl'\n",
    "    API_FP = '../../admin/santiment.txt'\n",
    "    STUDY_START = '2016-07-01'\n",
    "    STUDY_END = '2023-01-02'\n",
    "    PANEL_OUT_FP = \"../data/raw/san_panel.pkl\"\n",
    "    CW_OUT_FP = '../data/raw/san_coinmetrics_cw.pkl'\n",
    "    MACRO_OUT_FP = '../data/raw/san_macro.pkl'\n",
    "\n",
    "    # Import asset universe and cw\n",
    "    cw_df = pd.read_pickle(CW_IN_FP)\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    asset_universe = Helper.findUniqueAssets(asset_universe_dict)\n",
    "    daily_panel_df = pd.read_pickle(PANEL_DAILY_IN_FP)\n",
    "\n",
    "    # import api key and set\n",
    "    with open(API_FP) as f:\n",
    "        API_KEY = f.readlines()\n",
    "        API_KEY = API_KEY[0].strip()\n",
    "    san.ApiConfig.api_key = API_KEY\n",
    "    \n",
    "    # monitor progress\n",
    "    print(san.api_calls_remaining())\n",
    "\n",
    "    # Form crosswalk\n",
    "    san_df = formSantimentAssetUniverse(asset_universe)\n",
    "    san_df.to_pickle(CW_OUT_FP)\n",
    "    san_slug_universe = list(san_df.asset_san.values)\n",
    "\n",
    "    # Form asset metrics dictionary to pull\n",
    "    assets_metrics_dict_to_pull, assets_metrics_dict = formAssetMetricsDicts(san_slug_universe)\n",
    "\n",
    "    # Pull macro metrics\n",
    "    macro_df = pullMacroMetrics(STUDY_START, STUDY_END, san_slug_universe)\n",
    "    macro_df.to_pickle(MACRO_OUT_FP)\n",
    "\n",
    "    # Pull asset metrics\n",
    "    panel_df = pullAssetMetrics(STUDY_START, STUDY_END, \n",
    "                                assets_metrics_dict_to_pull, \n",
    "                                san_df, daily_panel_df)\n",
    "    panel_df.to_pickle(PANEL_OUT_FP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
