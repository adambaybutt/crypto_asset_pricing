{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO IMPORT COINMETRICS AND COINAPI PANELS\n",
    "# TODO FORM CROSSWALK\n",
    "# TODO MERGE PANELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets args\n",
    "cmc_asset_universe_fp = \"../data/raw/cmc_asset_universe.pkl\"\n",
    "cmc_cw_fp = \"../data/raw/cmc_cw.pkl\"\n",
    "cmc_panel_fp = \"../data/raw/cmc_price_volume_mcap_panel.pkl\"\n",
    "cg_cw_fp = \"../data/raw/coingecko_cmc_cw.pkl\"\n",
    "cg_panel_fp = \"../data/raw/coingecko_price_volume_mcap_panel.pkl\"\n",
    "cm_cw_fp = \"../data/raw/coinmetrics_cmc_cw.pkl\"\n",
    "cm_asset_info_fp = '../data/raw/coinmetrics_assets_first_tradable.pkl'\n",
    "cm_panel_fp = \"../data/raw/coinmetrics_initial_panel.pkl\"\n",
    "coinapi_panel_fp = '../data/raw/coinapi_panel.pkl'\n",
    "\n",
    "# import data\n",
    "with open(cmc_asset_universe_fp, 'rb') as f:\n",
    "    cmc_asset_universe_dict = pickle.load(f)\n",
    "cmc_cw_df =  pd.read_pickle(cmc_cw_fp)\n",
    "cmc_panel_df = pd.read_pickle(cmc_panel_fp)\n",
    "cg_cw_df =  pd.read_pickle(cg_cw_fp)\n",
    "cg_panel_df = pd.read_pickle(cg_panel_fp)\n",
    "cm_cw_df = pd.read_pickle(cm_cw_fp)\n",
    "cm_asset_df = pd.read_pickle(cm_asset_info_fp)\n",
    "cm_panel_df = pd.read_pickle(cm_panel_fp)\n",
    "ca_panel_df = pd.read_pickle(coinapi_panel_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formCoinmetricsAssetUniverse(client: CoinMetricsClient, cmc_assets_fp: str) -> pd.DataFrame:\n",
    "    \"\"\" map cmc universe to coinmetrics universe. \n",
    "    (1) pull all cm assets and open my universe of cmc assets.\n",
    "    (2) adjust cm asset names so they match to my cmc assets.\n",
    "    (3) merge asset ids together on both the cm asset id and the full name.\n",
    "    (4) clean the merged data.\n",
    "    (5) add assets from cm that should be in the universe but aren't in cmc.\n",
    "    (6) remove stablecoins and derivatives.\n",
    "\n",
    "    Args:\n",
    "        client (CoinMetricsClient): cm client object for pinging api.\n",
    "        cmc_assets_fp (str): filepath to cmc asset universe pickle.\n",
    "    \n",
    "    Returns:\n",
    "        merged_df (pd.DataFrame): dataframe of crosswalk between cmc id and cm id.    \n",
    "    \"\"\"\n",
    "    # import cmc token universe\n",
    "    with open(cmc_assets_fp, 'rb') as f:\n",
    "        cmc_asset_universe_dict = pickle.load(f)\n",
    "\n",
    "    # form unique cmc asset df\n",
    "    cmc_assets = []\n",
    "    for k, v in cmc_asset_universe_dict.items():\n",
    "        cmc_assets.extend(v)\n",
    "    cmc_assets = list(np.unique(np.array(cmc_assets)))\n",
    "    cmc_assets_df = pd.DataFrame(data={'asset_cmc': cmc_assets})\n",
    "\n",
    "    # pull all cm assets\n",
    "    full_asset_catalog = client.catalog_full_assets()\n",
    "    cm_assets_df = pd.DataFrame(full_asset_catalog)\n",
    "\n",
    "    # Check that the \"asset\" column is unique in both dataframes\n",
    "    assert (cmc_assets_df[\"asset_cmc\"].is_unique \n",
    "            and cm_assets_df[\"full_name\"].is_unique \n",
    "            and cm_assets_df['asset'].is_unique)\n",
    "\n",
    "    # remove duplicated cm asset; they have a data error\n",
    "    cm_assets_df = cm_assets_df[~cm_assets_df.asset.isin(['seed', 'tree', 'aurora'])]\n",
    "\n",
    "    # change cm full names before merge so they match cmc for known nonmatches\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='aave', 'full_name'] = 'aave-old'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='alpha', 'full_name'] = 'alpha-finance-lab'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='mco', 'full_name'] = 'crypto-com'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='fet', 'full_name'] = 'fetch'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='clv', 'full_name'] = 'clover'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='gno', 'full_name'] = 'gnosis-gno'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='glm', 'full_name'] = 'golem-network-tokens'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='hive', 'full_name'] = 'hive-blockchain'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='rook', 'full_name'] = 'keeperdao'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='yffii', 'full_name'] = 'yearn-finance-ii'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='btt', 'full_name'] = 'bittorrent'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='idex', 'full_name'] = 'aurora'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='egld', 'full_name'] = 'multiversx-egld'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='cfx', 'full_name'] = 'confluxnetwork'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='xch', 'full_name'] = 'chia-network'\n",
    "    cm_assets_df.loc[cm_assets_df.asset=='syn', 'full_name'] = 'synapse2'\n",
    "\n",
    "    # clean the asset names to just low case letters and numbers and merge\n",
    "    cmc_assets_df[\"asset_clean\"] = cmc_assets_df[\"asset_cmc\"].str.lower().str.replace(r\"[^a-zA-Z0-9]\", \"\")\n",
    "    cm_assets_df[\"asset_clean\"] = cm_assets_df[\"full_name\"].str.lower().str.replace(r\"[^a-zA-Z0-9]\", \"\")\n",
    "    merged_df = pd.merge(cmc_assets_df, cm_assets_df, \n",
    "                        on=\"asset_clean\", how='inner',\n",
    "                        validate='one_to_one')\n",
    "\n",
    "    # repeat but use the unique asset abbreviation id from cm\n",
    "    cm_assets_df[\"asset_clean\"] = cm_assets_df[\"asset\"].str.lower().str.replace(r\"[^a-zA-Z0-9]\", \"\")\n",
    "    merged_df2 = pd.merge(cmc_assets_df, cm_assets_df, \n",
    "                        on=\"asset_clean\", how='inner',\n",
    "                        validate='one_to_one')\n",
    "\n",
    "    # remove duplicated assets from the two merged dataframes and put them together\n",
    "    merged_df2 = merged_df2[~merged_df2.asset.isin(list(merged_df.asset.values))]\n",
    "    merged_df = pd.concat((merged_df, merged_df2))\n",
    "    assert merged_df.asset.is_unique\n",
    "\n",
    "    # clean up the merged data\n",
    "    merged_df = merged_df[['asset_cmc', 'asset']]\n",
    "    merged_df = merged_df.rename(columns={'asset': 'asset_cm'})\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "    # manually add to my universe of cm assets these assets to consider\n",
    "    assets_to_add = ['ape', 'apt', 'arpa', 'badger', 'bal', 'cake', 'cel', 'comp', 'cvx',\n",
    "        'dot', 'etc', 'fil', 'flr', 'flux', 'ftt', 'fun', 'gmx', 'grin', 'hnt',\n",
    "        'inv', 'knc', 'krl', 'luna', 'luna2', 'mir', 'multi', 'nft', 'nu', 'ocean', 'ohm',\n",
    "        'op', 'poly', 'qi', 'rndr', 'rpl', 'skl', 'snt', 'theta', 'tru', 'xdc', 'zrx']\n",
    "    merged_df = pd.concat((merged_df, pd.DataFrame(data={'asset_cmc': np.repeat(np.nan, len(assets_to_add)),\n",
    "                                                        'asset_cm': assets_to_add})))\n",
    "\n",
    "    # manually remove stables and derivatives\n",
    "    merged_df = merged_df[~merged_df.asset_cm.isin(['steth', 'wbtc', 'tusd', 'gusd', 'usdd', 'btcb'])]\n",
    "\n",
    "    # manually add in both aave old and new\n",
    "    merged_df = pd.concat((merged_df, pd.DataFrame(data={'asset_cmc': ['aave'],\n",
    "                                                        'asset_cm': ['aave']}))).reset_index(drop=True)\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# clean cws and panels before merge\n",
    "cm_cw_df = cm_cw_df[~cm_cw_df.asset_cm.isin(['xno'])]\n",
    "cm_cw_df = cm_cw_df[~cm_cw_df.asset_cmc.isin(['aave-old'])]\n",
    "cm_cw_df = cm_cw_df.rename(columns = {'asset_cmc': 'slug_cmc'})\n",
    "cg_cw_df = cg_cw_df[~cg_cw_df.asset_cmc.isin(['cronos', 'aave-old', 'yearn-finance-ii'])]\n",
    "cg_cw_df = cg_cw_df.rename(columns={'asset_cmc': 'slug_cmc'})\n",
    "cmc_panel_df['date'] = cmc_panel_df['date'].dt.date\n",
    "assert type(cg_panel_df.date.values[0]) == datetime.date\n",
    "cm_panel_df['time'] = pd.to_datetime(cm_panel_df['time']).dt.date\n",
    "cm_panel_df = cm_panel_df.rename(columns={'asset': 'asset_cm', 'time': 'date'})\n",
    "cm_asset_df = cm_asset_df.rename(columns={'asset': 'asset_cm'})\n",
    "\n",
    "# merge panels togethers\n",
    "panel_df = cmc_panel_df.merge(cg_cw_df,\n",
    "                              on='slug_cmc',\n",
    "                              how='left',\n",
    "                              validate='many_to_one')\n",
    "assert cmc_panel_df.shape[0]==panel_df.shape[0]\n",
    "panel_df = panel_df.merge(cg_panel_df, on=['date', 'asset_gecko'], how='outer', validate='many_to_one')\n",
    "panel_df = panel_df.merge(cm_cw_df[~cm_cw_df.slug_cmc.isnull()], on='slug_cmc', how='left', validate='many_to_one')\n",
    "panel_df = panel_df.merge(cm_panel_df, on=['date', 'asset_cm'], how='outer', validate='many_to_one')\n",
    "\n",
    "# cut down to window of interest\n",
    "panel_df = panel_df[panel_df.date.apply(lambda x: x.year) >=2015]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO CLEAN PRICE COLUMN\n",
    "# TODO CLEAN VOLUME COLUMNS\n",
    "# TODO CLEAN MCAP COLUMN; USE CM SUPPLY WHERE DONT HAVE MCAP\n",
    "# TODO subset down to date, asset, price, mcap, and volume and other useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN PRICE COLUMN\n",
    "\n",
    "# drop rows where we have no price data\n",
    "panel_df = panel_df[~(panel_df.ReferenceRateUSD.isnull() \n",
    "                    & panel_df.usd_per_token_cmc.isnull() \n",
    "                    & panel_df.usd_per_token_cg.isnull())]\n",
    "\n",
    "# form the price column\n",
    "panel_df['usd_per_token'] = np.nan\n",
    "panel_df.loc[~panel_df.ReferenceRateUSD.isnull(), 'usd_per_token'] = panel_df['ReferenceRateUSD']\n",
    "panel_df.loc[panel_df.usd_per_token.isnull(), 'usd_per_token'] = panel_df[['usd_per_token_cmc', 'usd_per_token_cg']].mean(axis=1, skipna=True)\n",
    "\n",
    "# remove rows where the price between cmc and cg is different by more than 50%\n",
    "panel_df = panel_df[~(panel_df.ReferenceRateUSD.isnull() \n",
    "                    & ~panel_df.usd_per_token_cmc.isnull() \n",
    "                    & ~panel_df.usd_per_token_cg.isnull()\n",
    "                    & (np.abs((panel_df.usd_per_token_cmc-panel_df.usd_per_token_cg)/panel_df.usd_per_token_cmc) > 0.5))]\n",
    "\n",
    "# keep just the final price\n",
    "panel_df = panel_df.drop(columns=['usd_per_token_cmc', 'usd_per_token_cg', \n",
    "                     'PriceUSD', 'ReferenceRate', 'ReferenceRateUSD'], axis=1)\n",
    "\n",
    "# convert dtype\n",
    "panel_df['usd_per_token'] = panel_df.usd_per_token.astype(float)\n",
    "\n",
    "# CLEAN MCAP COLUMN\n",
    "\n",
    "# drop if there is no mcap data\n",
    "panel_df = panel_df[~(panel_df.usd_mcap_cmc.isnull()\n",
    "                    & panel_df.usd_mcap_cg.isnull()\n",
    "                    & panel_df.CapMrktEstUSD.isnull())]\n",
    "\n",
    "# set any zeros to missing\n",
    "panel_df.loc[panel_df.CapMrktEstUSD==0, 'CapMrktEstUSD'] = np.nan\n",
    "panel_df.loc[panel_df.usd_mcap_cg==0, 'usd_mcap_cg'] = np.nan\n",
    "panel_df.loc[panel_df.usd_mcap_cmc==0, 'usd_mcap_cmc'] = np.nan\n",
    "\n",
    "# form the mcap column\n",
    "panel_df['CapMrktEstUSD'] = panel_df.CapMrktEstUSD.astype(float)\n",
    "panel_df['usd_mcap'] = panel_df[['CapMrktEstUSD', 'usd_mcap_cg', 'usd_mcap_cmc']].mean(axis=1, skipna=True)\n",
    "assert 0 == panel_df.usd_mcap.isnull().sum()\n",
    "\n",
    "# drop rows where mcaps between cg and cmc are more than order of magnitude off when we are missing CM values\n",
    "panel_df = panel_df[~(panel_df.CapMrktEstUSD.isnull() & ~panel_df.usd_mcap_cg.isnull() & ~panel_df.usd_mcap_cmc.isnull()\n",
    "                      & (np.abs((panel_df.usd_mcap_cg - panel_df.usd_mcap_cmc)/panel_df.usd_mcap_cmc) > 10))]\n",
    "\n",
    "# keep just the final price\n",
    "panel_df = panel_df.drop(columns=['usd_mcap_cmc', 'usd_mcap_cg', \n",
    "                                  'CapMrktCurUSD', 'CapMrktEstUSD', 'CapMrktFFUSD', 'CapRealUSD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO USE THIS FOR CLEANING PANELS\n",
    "\n",
    "  # drop rows\n",
    "    panel_df = panel_df[(panel_df.date.dt.year >= 2015) & (panel_df.date <= '2023-02-02')]\n",
    "    panel_df = panel_df.dropna(how='any', subset=['date', 'cmc_id'])\n",
    "    panel_df = panel_df.dropna(how='all', subset=['usd_per_token_cmc', 'usd_mcap_cmc', 'usd_volume_24h_cmc'])\n",
    "\n",
    "    # form list of data columns to work with\n",
    "    data_cols = list(panel_df.columns.values)\n",
    "    data_cols.remove('date')\n",
    "    data_cols.remove('cmc_id')\n",
    "\n",
    "    # set negative values to missing and too large values to missing\n",
    "    for col in data_cols:\n",
    "        panel_df.loc[panel_df[col] < 0, col] = np.nan\n",
    "        panel_df.loc[panel_df[col] > 2e12, col] = np.nan\n",
    "\n",
    "    # drop duplicated rows across id columns\n",
    "    panel_df = panel_df.drop_duplicates(subset=['date', 'cmc_id'])\n",
    "\n",
    "    # sort values and reset index\n",
    "    panel_df = panel_df.sort_values(by=['date', 'cmc_id'], \n",
    "                                    ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO look for continuity within asset. look at returns to see if anything crazy. look if mcap jump is way diff than price jump.\n",
    "# TODO make sure ranges of values looks good\n",
    "# TODO go scope old cleaning scripts to make sure i do all of that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO apply the inclusion criteria on the first on each month; use code below for it\n",
    "# TODO write down what i am doing in the JMP\n",
    "# TODO go scope five best factor model papers to ensure mine is best in class. add how i am better to my write up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGeomAvg(returns: np.array,\n",
    "    annualized: bool=False,\n",
    "    periods_in_year: int=None) -> float: \n",
    "    \"\"\" Calculate the geometric average of a vector of simple returns.\n",
    "\n",
    "    Args:\n",
    "        returns (np.array): vector of a simple returns at any frequency.\n",
    "        annualized (bool): whether to annualize the statistic.\n",
    "        periods_in_year (int): how many periods of the given frequency are in a year.\n",
    "\n",
    "    Returns:\n",
    "        (float): scalar geometric average.\n",
    "    \"\"\"\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        raise TypeError(\"Input 'returns' must be a NumPy array\")\n",
    "    if annualized and periods_in_year is None:\n",
    "        raise ValueError(\"Input 'periods_in_year' must be provided if 'annualized' is True\")\n",
    "    geom_avg_at_given_freq = np.prod(1 + returns) ** (1 / np.size(returns)) - 1\n",
    "    return (geom_avg_at_given_freq + 1) ** periods_in_year - 1 if annualized else geom_avg_at_given_freq\n",
    "\n",
    "def prepPanelForInitialInclusiveCriteria(panel_df: pd.DataFrame, cw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" performs various ad hoc cleaning to prep the panel further for applying inclusion criteria.\n",
    "    \n",
    "    Args:\n",
    "        panel_df (pd.DataFrame): panel of asset prices, trading volumes, and mcaps from cmc.\n",
    "        cw_df (pd.DataFrame): identifying variables for the assets.\n",
    "        \n",
    "    Returns:\n",
    "        panel_df (pd.DataFrame): cleaned panel.\n",
    "    \"\"\"\n",
    "    # manually remove tokens from panel\n",
    "    tokens_to_remove = [770, 776, 3787, 8644, 9103]\n",
    "    panel_df = panel_df[~panel_df.cmc_id.isin(tokens_to_remove)]\n",
    "\n",
    "    # merge on cmc slug and drop the cmc id\n",
    "    panel_df = panel_df.merge(cw_df[['cmc_id', 'slug_cmc']],\n",
    "                              on='cmc_id',\n",
    "                              how='inner',\n",
    "                              validate='many_to_one')\n",
    "    panel_df = panel_df.drop('cmc_id', axis=1)\n",
    "    panel_df = panel_df[['date', 'slug_cmc', 'usd_per_token_cmc', 'usd_mcap_cmc', 'usd_volume_24h_cmc']]\n",
    "    panel_df = panel_df.sort_values(by=['date', 'slug_cmc'], ignore_index=True)\n",
    "\n",
    "    # adjust particular values\n",
    "    panel_df.loc[(panel_df.slug_cmc=='uquid-coin') & \n",
    "                  panel_df.usd_volume_24h_cmc.isnull(), 'usd_volume_24h_cmc'] = 0\n",
    "\n",
    "    # ensure no missing in the df\n",
    "    assert(0==panel_df.isnull().sum().sum())\n",
    "\n",
    "    # ensure unique on key columns\n",
    "    dups = panel_df.duplicated(subset=['date', 'slug_cmc'])\n",
    "    assert(~dups.any()),('there are duplicates in the data on keys date and slug_cmc')\n",
    "\n",
    "    # drop more tokens manually\n",
    "    # NOTES: ampleforth is a stablecoin, pax gold is a gold stablecoin, index, and wrapped tokens\n",
    "    wrapped_tokens_to_drop = ['ampleforth', 'cryptoindex-com-100', 'pax-gold',\n",
    "                            'wrapped-centrifuge', 'wrapped-luna-token', 'wrapped-ncg', 'wrapped-nxm']\n",
    "    panel_df = panel_df[~panel_df.slug_cmc.isin(wrapped_tokens_to_drop)]\n",
    "\n",
    "    return panel_df\n",
    "def buildInitialAssetUniverse(panel_df: pd.DataFrame, start_date: date, end_date: date) -> dict:\n",
    "    \"\"\" build an initial universe of assets to pull data for.\n",
    "    \n",
    "    Args:\n",
    "        panel_df (pd.DataFrame): panel of asset prices, trading volumes, and mcaps from cmc.\n",
    "        start_date (date): A datetime.date object representing the start date for the study period.\n",
    "        end_date (date): A datetime.date object representing the end date for the study period.\n",
    "    \n",
    "    Returns:\n",
    "        asset_universe (dict): keys of start of each month in study period with associated value\n",
    "                               of list of asset names to include.\n",
    "    \"\"\"\n",
    "    # specify the dates to obtain\n",
    "    dates = [start_date.strftime('%Y-%m-%d')]\n",
    "    current_date = start_date+relativedelta(months=1)\n",
    "    while current_date <= end_date:\n",
    "        dates.append(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date += relativedelta(months=1)\n",
    "\n",
    "    # apply suff data, volume, and mcap filters\n",
    "    asset_universe_per_month = []\n",
    "    for i in range(len(dates)-1):\n",
    "        # determine start and end dates for window\n",
    "        start_window = dates[i]\n",
    "        end_window   = dates[i+1]\n",
    "\n",
    "        # build temporary dataframe for this time period\n",
    "        temp_df = panel_df[(panel_df.date >= start_window) & (panel_df.date <= end_window)].copy()\n",
    "\n",
    "        # obtain list of tokens to consider\n",
    "        assets_included = list(np.unique(temp_df[temp_df.date == end_window].slug_cmc.values))\n",
    "\n",
    "        # figure out tokens removed due to insuff data\n",
    "        # note: 28 days ensures at least 4 weeks of data \n",
    "        asset_ns_df = temp_df.groupby('slug_cmc').size()\n",
    "        assets_lost_given_insuff_data = list(asset_ns_df[asset_ns_df < 28].index.values)\n",
    "        for asset in assets_lost_given_insuff_data:\n",
    "            if asset in assets_included:\n",
    "                assets_included.remove(asset)\n",
    "\n",
    "        # Figure out tokens removed due to volume threshold\n",
    "        temp_vol_df = temp_df.groupby('slug_cmc').usd_volume_24h_cmc.min()\n",
    "        assets_lost_given_insuff_vol = list(temp_vol_df[temp_vol_df < 10000].index.values)\n",
    "        for asset in assets_lost_given_insuff_vol:\n",
    "            if asset in assets_included:\n",
    "                assets_included.remove(asset)\n",
    "\n",
    "        # Figure out assets removed due to mcap threshold\n",
    "        current_year = int(end_window[:4]) \n",
    "        if current_year <= 2016:\n",
    "            mcap_threshold = 750000\n",
    "        elif current_year == 2017:\n",
    "            mcap_threshold = 2e6\n",
    "        elif current_year == 2018:\n",
    "            mcap_threshold = 30e6\n",
    "        elif current_year in [2019, 2020]:\n",
    "            mcap_threshold = 15e6\n",
    "        elif current_year >= 2021:\n",
    "            mcap_threshold = 75e6\n",
    "        temp_mcap_df = temp_df.groupby('slug_cmc').usd_mcap_cmc.min()\n",
    "        assets_lost_given_mcap_threshold = list(temp_mcap_df[temp_mcap_df < mcap_threshold].index.values)\n",
    "        for asset in assets_lost_given_mcap_threshold:\n",
    "            if asset in assets_included:\n",
    "                assets_included.remove(asset)\n",
    "\n",
    "        # Report out new asset ever\n",
    "        print('New assets that we have never had are ')\n",
    "        if i != 0:\n",
    "            all_assets = []\n",
    "            for j in range(i-1,-1,-1):\n",
    "                all_assets += asset_universe_per_month[j]\n",
    "            print(np.unique(set(assets_included).difference(set(all_assets))))\n",
    "        else:\n",
    "            print(np.unique(assets_included))\n",
    "        print('\\n')\n",
    "\n",
    "        # Report out assets for this month\n",
    "        print(f'This month\\'s ({end_window}) {len(assets_included)} assets are:')\n",
    "        print(np.unique(assets_included))\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # Add assets to list\n",
    "        asset_universe_per_month.append(list(np.unique(assets_included)))\n",
    "\n",
    "    # build asset universe\n",
    "    asset_universe_dict = {}\n",
    "    for i in range(len(dates)-1):\n",
    "        asset_universe_dict[dates[i+1]] = asset_universe_per_month[i]\n",
    "\n",
    "    return asset_universe_dict\n",
    "\n",
    "def determineUniqueAssets(asset_universe_dict) -> list:\n",
    "    \"\"\" determine the unique assets in the universe to return as a list. \"\"\"\n",
    "    assets = []\n",
    "    for k, v in asset_universe_dict.items():\n",
    "        assets.extend(v)\n",
    "    assets = list(np.unique(np.array(assets)))\n",
    "    assets.sort()\n",
    "    return assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO manually check that the universe makes sense for maybe 6-10 of the random sampling of hte first years and \n",
    "# the last 3-6 random sampling over 2-3 years?\n",
    "\n",
    "# TODO output the coinapi to cm crosswalk for this universe as well as a dictionary of the cmc ids at the start of each month\n",
    "\n",
    "# TODO convert all the code to functions with professional documentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e428bc405edc59f3352e9792cab27c5e28560f7efb4b47308a6c6ea38cd15df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
