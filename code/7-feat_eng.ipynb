{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pickle\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processMonth(panel_df: pd.DataFrame, date_key: str, asset_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Process a single month's data and return the resulting DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert the date_key string to a datetime object to work with pandas\n",
    "    date_key_dt = pd.to_datetime(date_key)\n",
    "\n",
    "    # Create a date mask for the month\n",
    "    date_mask = (panel_df['date'].dt.year == date_key_dt.year) & (panel_df['date'].dt.month == date_key_dt.month)\n",
    "\n",
    "    # Subset the panel_df DataFrame based on the date mask and the asset list\n",
    "    subset = panel_df[date_mask & panel_df['asset'].isin(asset_list)]\n",
    "\n",
    "    return subset\n",
    "\n",
    "def subsetByMonthToAssetUniverse(panel_df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]], n_jobs=-1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset the panel data to the assets in each month (key) of asset_universe_dict.\n",
    "\n",
    "    Args:\n",
    "        panel_df: A Pandas DataFrame containing panel data at the asset-hour level\n",
    "                    with ID columns 'date' and 'asset'.\n",
    "        asset_universe_dict: A dictionary with keys as dates in the format YYYY-MM-DD \n",
    "                                and values as lists of asset strings.\n",
    "        n_jobs: Number of CPU cores to use for parallelization. Default is -1, which means using all available cores.\n",
    "    \n",
    "    Returns: A Pandas DataFrame without the rows not included in the study.\n",
    "    \"\"\"\n",
    "    # Run the process_month function in parallel using joblib\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(processMonth)(panel_df, date_key, asset_list) for date_key, asset_list in asset_universe_dict.items())\n",
    "\n",
    "    # Combine the results into a single DataFrame\n",
    "    new_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def subsetToWeeklyFreq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Takes DataFrame with datetime column \"date\" to subset\n",
    "        it to observations on Sunday at midnight. \"\"\"\n",
    "    return df[(df.date.dt.day_name() == 'Sunday') \n",
    "            & (df.date.dt.time == pd.Timestamp('00:00:00').time())]\n",
    "\n",
    "def calcReturn(x: pd.Series) -> float:\n",
    "    return (x.iloc[-1] - x.iloc[0]) / x.iloc[0]\n",
    "            \n",
    "def setMissingIfIncomplete(panel_df: pd.DataFrame, return_col: str, hours_to_check: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Set the specified return column to missing (np.nan) if the DataFrame is missing any of the previous hours\n",
    "    specified by hours_to_check for each asset.\n",
    "\n",
    "    Args:\n",
    "        panel_df (pd.DataFrame): The input DataFrame\n",
    "        return_col (str): The name of the column to set to missing (np.nan) if any of the previous hours are missing\n",
    "        hours_to_check (int): The number of previous hours to check for.\n",
    "\n",
    "    Returns: The DataFrame with the return column set to missing if any of the previous hours are missing\n",
    "    \"\"\"\n",
    "    # Shift the date column by the specified hours_to_check\n",
    "    panel_df['prev_date'] = panel_df.groupby('asset')['date'].shift(hours_to_check)\n",
    "    \n",
    "    # Calculate the rolling sum of hour differences over a window of size hours_to_check\n",
    "    panel_df['hours_present'] = panel_df.groupby('asset')['date'].transform(\n",
    "        lambda x: x.diff().dt.total_seconds().rolling(window=hours_to_check).sum() / 3600\n",
    "    )\n",
    "\n",
    "    # Set the return column value to missing (None) if the total number of hours present is not equal to hours_to_check\n",
    "    panel_df.loc[panel_df['hours_present'] != hours_to_check, return_col] = np.nan\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    panel_df.drop(columns=['prev_date', 'hours_present'], inplace=True)\n",
    "\n",
    "    return panel_df\n",
    "\n",
    "def process_asset(asset_data: pd.DataFrame, target_col: str, new_col: str, range_hours: int, func) -> pd.DataFrame:\n",
    "    \"\"\" Process a single asset's data and return the resulting DataFrame.\n",
    "    \"\"\"\n",
    "    # Apply the function to each asset's previous values and store the result in a new Series\n",
    "    # note: we add one to range hours so we get the appropriate window\n",
    "    asset_data[new_col] = asset_data[target_col].rolling(range_hours+1).apply(func)\n",
    "\n",
    "    # Reset missing values\n",
    "    asset_data = setMissingIfIncomplete(asset_data, new_col, range_hours)\n",
    "\n",
    "    return asset_data[['date', 'asset', new_col]]\n",
    "\n",
    "def formNewColumnByAsset(panel_df: pd.DataFrame, target_col: str, new_col: str, range_hours: int, func, n_jobs=-1) -> pd.DataFrame:\n",
    "    \"\"\" Adds a new column to a Pandas DataFrame containing panel data at the asset-hour level.\n",
    "    The new column is calculated by applying a function to a range of previous values for each asset.\n",
    "    Any values that do not have the previous range_hours are reset to missing (np.nan).\n",
    "\n",
    "    Args:\n",
    "        panel_df: Pandas DataFrame containing the panel data.\n",
    "        target_col: Name of the column to apply the given function to.\n",
    "        new_col: Name of the new column to add.\n",
    "        range_hours: Number of previous hours to consider for each asset.\n",
    "        func: Function to apply to the range of values for each asset.\n",
    "        n_jobs: Number of CPU cores to use for parallelization. Default is -1, which means using all available cores.\n",
    "    \n",
    "    Returns: A new DataFrame with the columns \"date\", \"asset\", and new_col for each asset.\n",
    "    \"\"\"\n",
    "    # Group the DataFrame by asset\n",
    "    grouped = panel_df.groupby('asset')\n",
    "    \n",
    "    # Run the process_asset function in parallel using joblib\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_asset)(asset_data, target_col, new_col, range_hours, func) for _, asset_data in grouped)\n",
    "\n",
    "    # Combine the results into a single DataFrame\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formStaticCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # set columns to keep\n",
    "    static_cols = ['char_industry_asset_mgmt',\n",
    "        'char_industry_cex',\n",
    "        'char_industry_cloud_compute',\n",
    "        'char_industry_currency',\n",
    "        'char_industry_data_mgmt',\n",
    "        'char_industry_dex',\n",
    "        'char_industry_gaming',\n",
    "        'char_industry_infra',\n",
    "        'char_industry_interop',\n",
    "        'char_industry_lending',\n",
    "        'char_industry_media',\n",
    "        'char_industry_other_defi',\n",
    "        'char_industry_smart_contract',\n",
    "        'char_asset_usage_access',\n",
    "        'char_asset_usage_discount',\n",
    "        'char_asset_usage_dividends',\n",
    "        'char_asset_usage_payments',\n",
    "        'char_asset_usage_vote',\n",
    "        'char_asset_usage_work',\n",
    "        'char_pow',\n",
    "        'char_pos',\n",
    "        'char_ico_price',\n",
    "        'char_ico']\n",
    "    \n",
    "    # form column subset\n",
    "    static_df = panel_df[['date', 'asset']+static_cols]\n",
    "\n",
    "    return static_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formDescStatCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # identify cols\n",
    "    cols = ['char_price_global_t', 'char_volume_24h_global_t', \n",
    "            'char_ico_days_since_t', 'char_vc_t',\n",
    "            'char_rank_cmc_t', 'char_num_pairs_t']\n",
    "\n",
    "    # subset to cols\n",
    "    desc_stat_df = panel_df[['date', 'asset']+cols]\n",
    "\n",
    "    return desc_stat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formMomentumCols(panel_df: pd.DataFrame, static_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Form relevant data\n",
    "    temp_df = panel_df[['date', 'asset', 'char_price_t', 'char_mcap_t']].copy()\n",
    "\n",
    "    # Form momentums \n",
    "    mom1h_df    = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm1h', range_hours=1, func=calcReturn)\n",
    "    mom2h_df    = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm2h', range_hours=2, func=calcReturn)\n",
    "    mom6h_df    = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm6h', range_hours=6, func=calcReturn)\n",
    "    mom12h_df   = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm12h', range_hours=12, func=calcReturn)\n",
    "    mom1_df     = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm1', range_hours=24, func=calcReturn)\n",
    "    mom7_df     = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm7', range_hours=168, func=calcReturn)\n",
    "    mom14_df    = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm14', range_hours=336, func=calcReturn)\n",
    "    mom30_14_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm30_tm14', range_hours=384, func=calcReturn)\n",
    "    mom30_df    = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm30', range_hours=720, func=calcReturn)\n",
    "    mom60_df    = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm60', range_hours=1440, func=calcReturn)\n",
    "    mom90_df    = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm90', range_hours=2160, func=calcReturn)\n",
    "    del temp_df\n",
    "\n",
    "    # Prep industry column for forming industry momentums\n",
    "    industry_cols = [col for col in static_df.columns if 'industry' in col]\n",
    "    temp_df = static_df[['date', 'asset']+industry_cols].copy()\n",
    "    def collapse_industries(df):\n",
    "        # Extract the columns containing industry information\n",
    "        industry_columns = [col for col in df.columns if col.startswith('char_industry_')]\n",
    "\n",
    "        # Verify that each date-asset combination has only one industry with a value of 1\n",
    "        for _, row in df[industry_columns].sum(axis=1).iteritems():\n",
    "            assert row == 1, f\"Error: {row.name[0]} and asset {row.name[1]} has {row} industries with a value of 1.\"\n",
    "        \n",
    "        # Create a new column with the industry name, removing the 'char_industry_' prefix\n",
    "        df['industry'] = df[industry_columns].idxmax(axis=1).str.replace('char_industry_', '')\n",
    "        \n",
    "        # Drop the original industry columns\n",
    "        df_result = df.drop(columns=industry_columns)\n",
    "        \n",
    "        return df_result\n",
    "\n",
    "    industry_df = collapse_industries(temp_df)\n",
    "\n",
    "    assert not industry_df.duplicated(subset=['date', 'asset']).any()\n",
    "\n",
    "    # Form industry momentums\n",
    "    temp_df  = panel_df[['date', 'asset', 'char_mcap_t']].copy()\n",
    "    mom6h_df = mom6h_df.dropna()\n",
    "    mom6h_df = mom6h_df.merge(industry_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    mom6h_df = mom6h_df.merge(temp_df[['date', 'asset', 'char_mcap_t']], on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    weighted_averages = mom6h_df.groupby(['date', 'industry']).apply(lambda x: (x['char_r_tm6h'] * x['char_mcap_t']).sum() / x['char_mcap_t'].sum())\n",
    "    weighted_averages = weighted_averages.reset_index(name='char_r_industry_tm6h')\n",
    "    mom6h_df = mom6h_df.merge(weighted_averages, on=['date', 'industry'])\n",
    "    mom6h_df = mom6h_df.drop(columns=['industry', 'char_mcap_t'])\n",
    "    mom30_df = mom30_df.dropna()\n",
    "    mom30_df = mom30_df.merge(industry_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    mom30_df = mom30_df.merge(temp_df[['date', 'asset', 'char_mcap_t']], on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    weighted_averages = mom30_df.groupby(['date', 'industry']).apply(lambda x: (x['char_r_tm30'] * x['char_mcap_t']).sum() / x['char_mcap_t'].sum())\n",
    "    weighted_averages = weighted_averages.reset_index(name='char_r_industry_tm30')\n",
    "    mom30_df = mom30_df.merge(weighted_averages, on=['date', 'industry'])\n",
    "    mom30_df = mom30_df.drop(columns=['industry', 'char_mcap_t'])\n",
    "    mom60_df = mom60_df.dropna()\n",
    "    mom60_df = mom60_df.merge(industry_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    mom60_df = mom60_df.merge(temp_df[['date', 'asset', 'char_mcap_t']], on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    weighted_averages = mom60_df.groupby(['date', 'industry']).apply(lambda x: (x['char_r_tm60'] * x['char_mcap_t']).sum() / x['char_mcap_t'].sum())\n",
    "    weighted_averages = weighted_averages.reset_index(name='char_r_industry_tm60')\n",
    "    mom60_df = mom60_df.merge(weighted_averages, on=['date', 'industry'])\n",
    "    mom60_df = mom60_df.drop(columns=['industry', 'char_mcap_t'])\n",
    "    del temp_df\n",
    "\n",
    "    # Form reversals\n",
    "    mom7_df['char_r_tm14_tm7'] = mom7_df.groupby('asset')['char_r_tm7'].shift(168)\n",
    "    mom30_14_df['char_r_tm30_tm14'] = mom30_14_df.groupby('asset')['char_r_tm30_tm14'].shift(336)\n",
    "    mom60_df['char_r_tm90_tm30'] = mom60_df.groupby('asset')['char_r_tm60'].shift(720)\n",
    "\n",
    "    # Form single momentum df\n",
    "    mom_df = mom1h_df.copy()\n",
    "    for df in [mom2h_df, mom6h_df, mom12h_df, mom1_df, mom7_df, mom14_df, mom30_df, mom60_df, mom90_df, mom30_14_df]:\n",
    "        mom_df = mom_df.merge(df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "\n",
    "    # Take out risk free rate from all returns\n",
    "    temp_df = panel_df[['date', 'macro_dgs1mo_t']].drop_duplicates().copy()\n",
    "    temp_df['r_rf_tm1h'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365*24))-1\n",
    "    temp_df['r_rf_tm2h'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365*12))-1\n",
    "    temp_df['r_rf_tm6h'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365*4))-1\n",
    "    temp_df['r_rf_tm12h'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365*2))-1\n",
    "    temp_df['r_rf_tm1'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365))-1\n",
    "    temp_df['r_rf_tm7'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365/7))-1\n",
    "    temp_df['r_rf_tm14'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365/14))-1\n",
    "    temp_df['r_rf_tm16'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365/16))-1\n",
    "    temp_df['r_rf_tm30'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365/30))-1\n",
    "    temp_df['r_rf_tm60'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365/60))-1\n",
    "    temp_df['r_rf_tm90'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365/90))-1\n",
    "    temp_df = temp_df.drop('macro_dgs1mo_t', axis=1)\n",
    "    temp_df['r_rf_tm1h'] = temp_df['r_rf_tm1h'].shift(1)\n",
    "    temp_df['r_rf_tm2h'] = temp_df['r_rf_tm2h'].shift(2)\n",
    "    temp_df['r_rf_tm6h'] = temp_df['r_rf_tm6h'].shift(6)\n",
    "    temp_df['r_rf_tm12h'] = temp_df['r_rf_tm12h'].shift(12)\n",
    "    temp_df['r_rf_tm1'] = temp_df['r_rf_tm1'].shift(24)\n",
    "    temp_df['r_rf_tm7'] = temp_df['r_rf_tm7'].shift(24*7)\n",
    "    temp_df['r_rf_tm14'] = temp_df['r_rf_tm14'].shift(24*14)\n",
    "    temp_df['r_rf_tm16'] = temp_df['r_rf_tm16'].shift(24*16) \n",
    "    temp_df['r_rf_tm30'] = temp_df['r_rf_tm30'].shift(24*30)\n",
    "    temp_df['r_rf_tm60'] = temp_df['r_rf_tm60'].shift(24*60) \n",
    "    temp_df['r_rf_tm90'] = temp_df['r_rf_tm90'].shift(24*90) \n",
    "    temp_df['r_rf_tm14_tm7'] = temp_df['r_rf_tm7'].shift(24*7) \n",
    "    temp_df['r_rf_tm30_tm14'] = temp_df['r_rf_tm16'].shift(24*14) \n",
    "    temp_df = temp_df.drop('r_rf_tm16', axis=1)\n",
    "    temp_df['r_rf_tm90_tm30'] = temp_df['r_rf_tm60'].shift(24*30)\n",
    "    mom_df = mom_df.merge(temp_df, on=['date'], how='inner', validate='many_to_one')\n",
    "    mom_df['char_r_tm1h']          = mom_df['char_r_tm1h'] - mom_df['r_rf_tm1h']\n",
    "    mom_df['char_r_tm2h']          = mom_df['char_r_tm2h'] - mom_df['r_rf_tm2h']\n",
    "    mom_df['char_r_tm6h']          = mom_df['char_r_tm6h'] - mom_df['r_rf_tm6h']\n",
    "    mom_df['char_r_industry_tm6h'] = mom_df['char_r_industry_tm6h'] - mom_df['r_rf_tm6h']\n",
    "    mom_df['char_r_tm12h']         = mom_df['char_r_tm12h'] - mom_df['r_rf_tm12h']\n",
    "    mom_df['char_r_tm1']           = mom_df['char_r_tm1'] - mom_df['r_rf_tm1']\n",
    "    mom_df['char_r_tm7']           = mom_df['char_r_tm7'] - mom_df['r_rf_tm7']\n",
    "    mom_df['char_r_tm14_tm7']      = mom_df['char_r_tm14_tm7'] - mom_df['r_rf_tm14_tm7']\n",
    "    mom_df['char_r_tm14']          = mom_df['char_r_tm14'] - mom_df['r_rf_tm14']\n",
    "    mom_df['char_r_tm30']          = mom_df['char_r_tm30'] - mom_df['r_rf_tm30']\n",
    "    mom_df['char_r_industry_tm30'] = mom_df['char_r_industry_tm30'] - mom_df['r_rf_tm30']\n",
    "    mom_df['char_r_tm60']          = mom_df['char_r_tm60'] - mom_df['r_rf_tm60']\n",
    "    mom_df['char_r_industry_tm60'] = mom_df['char_r_industry_tm60'] - mom_df['r_rf_tm60']\n",
    "    mom_df['char_r_tm90_tm30']     = mom_df['char_r_tm90_tm30'] - mom_df['r_rf_tm90_tm30']\n",
    "    mom_df['char_r_tm90']          = mom_df['char_r_tm90'] - mom_df['r_rf_tm90']\n",
    "    mom_df['char_r_tm30_tm14']     = mom_df['char_r_tm30_tm14'] - mom_df['r_rf_tm30_tm14']\n",
    "    rf_cols = [col for col in mom_df.columns if '_rf_' in col]\n",
    "    mom_df = mom_df.drop(rf_cols, axis=1)\n",
    "\n",
    "    # Fix outliers\n",
    "    ret_cols = [col for col in mom_df.columns if col not in ['date', 'asset']]\n",
    "    for col in ret_cols:\n",
    "        mom_df.loc[mom_df[col] < -.95, col] = -0.95\n",
    "        mom_df.loc[mom_df[col] > 100, col] = 100\n",
    "\n",
    "    return mom_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formCmktCol(panel_df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    # form mom cols, subset to relevant assets, and merge together for temp data to use to form cmkt return\n",
    "    mom7_df = formNewColumnByAsset(panel_df[['date', 'asset', 'char_price_t']], target_col='char_price_t', new_col='char_r_tm7', range_hours=168, func=calcReturn)\n",
    "    mom1h_df = formNewColumnByAsset(panel_df[['date', 'asset', 'char_price_t']], target_col='char_price_t', new_col='char_r_tm1h', range_hours=1, func=calcReturn)\n",
    "    rel_assets_dt_df = subsetByMonthToAssetUniverse(panel_df[['date', 'asset', 'char_mcap_t', 'char_price_t']], \n",
    "                                                    asset_universe_dict)\n",
    "    rel_assets_dt_df = rel_assets_dt_df.drop('char_price_t', axis=1)\n",
    "    temp_df = rel_assets_dt_df.merge(mom7_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    temp_1h_df = rel_assets_dt_df.merge(mom1h_df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "    temp_1h_df.loc[temp_1h_df['char_r_tm1h'] < -.95, 'char_r_tm1h'] = -0.95\n",
    "    temp_1h_df.loc[temp_1h_df['char_r_tm1h'] > 100, 'char_r_tm1h'] = 100\n",
    "    temp_df.loc[temp_df['char_r_tm7'] < -.95, 'char_r_tm7'] = -0.95\n",
    "    temp_df.loc[temp_df['char_r_tm7'] > 100, 'char_r_tm7'] = 100\n",
    "\n",
    "    # form cmkt-weighted average return by week and by day\n",
    "    cmkt_df = temp_df.groupby('date').apply(lambda x: (x['char_r_tm7']*x['char_mcap_t']).sum() / x['char_mcap_t'].sum())\n",
    "    cmkt_1h_df = temp_1h_df.groupby('date').apply(lambda x: (x['char_r_tm1h']*x['char_mcap_t']).sum() / x['char_mcap_t'].sum())\n",
    "\n",
    "    # clean it up\n",
    "    cmkt_df = pd.DataFrame(cmkt_df)\n",
    "    cmkt_df.columns = ['macro_cmkt_tm7']\n",
    "    cmkt_df = cmkt_df.reset_index()\n",
    "    cmkt_1h_df = pd.DataFrame(cmkt_1h_df)\n",
    "    cmkt_1h_df.columns = ['macro_cmkt_tm1h']\n",
    "    cmkt_1h_df = cmkt_1h_df.reset_index()\n",
    "\n",
    "    # Take out risk free rate\n",
    "    temp_df = panel_df[['date', 'macro_dgs1mo_t']].drop_duplicates().copy()\n",
    "    temp_df['r_rf_tm1h'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365*24))-1\n",
    "    temp_df['r_rf_tm7'] = (1+temp_df.macro_dgs1mo_t.values/100)**(1/(365/7))-1\n",
    "    temp_df = temp_df.drop('macro_dgs1mo_t', axis=1)\n",
    "    temp_df['r_rf_tm1h'] = temp_df['r_rf_tm1h'].shift(1)\n",
    "    temp_df['r_rf_tm7'] = temp_df['r_rf_tm7'].shift(24*7)\n",
    "    cmkt_df = cmkt_df.merge(temp_df[['date', 'r_rf_tm7']], on=['date'], how='inner', validate='one_to_one')\n",
    "    cmkt_1h_df = cmkt_1h_df.merge(temp_df[['date', 'r_rf_tm1h']], on=['date'], how='inner', validate='one_to_one')\n",
    "    cmkt_df['macro_cmkt_tm7'] = cmkt_df['macro_cmkt_tm7'] - cmkt_df['r_rf_tm7']\n",
    "    cmkt_df = cmkt_df.drop('r_rf_tm7', axis=1)\n",
    "    cmkt_1h_df['macro_cmkt_tm1h'] = cmkt_1h_df['macro_cmkt_tm1h'] - cmkt_1h_df['r_rf_tm1h']\n",
    "    cmkt_1h_df = cmkt_1h_df.drop('r_rf_tm1h', axis=1)\n",
    "\n",
    "    # Fix outliers\n",
    "    cmkt_1h_df.loc[cmkt_1h_df.macro_cmkt_tm1h>0.5, 'macro_cmkt_tm1h'] = 0.1\n",
    "    cmkt_df.loc[cmkt_df.macro_cmkt_tm7>0.65, 'macro_cmkt_tm7'] = 0.65\n",
    "\n",
    "    return cmkt_df, cmkt_1h_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formCumRetCols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds two new columns to the input DataFrame containing panel data at the asset-hour level.\n",
    "    The new columns are 'char_r_ath_t' and 'char_r_atl_t', representing the cumulative return since\n",
    "    each asset's historical all-time high price and all-time low price, respectively.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing panel data at the asset-hour level.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with the new columns added.\n",
    "    \"\"\"\n",
    "    # Form group mask\n",
    "    grouped = df.groupby('asset')\n",
    "\n",
    "    # Calculate the cumulative maximum for the 'char_price_t' column within each group\n",
    "    df['cummax_price'] = grouped['char_price_t'].cummax()\n",
    "\n",
    "    # Calculate the return since the all-time high price\n",
    "    df['char_r_ath_t'] = df['char_price_t'] / df['cummax_price'] - 1\n",
    "\n",
    "    # Calculate the cumulative minimum for the 'char_price_t' column within each group\n",
    "    df['cummin_price'] = grouped['char_price_t'].cummin()\n",
    "\n",
    "    # Calculate the return since the all-time low price\n",
    "    df['char_r_atl_t'] = df['char_price_t'] / df['cummin_price'] - 1\n",
    "\n",
    "    # Drop the temporary 'cummax_price' and 'cummin_price' columns\n",
    "    df.drop(columns=['cummax_price', 'cummin_price'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def perform_regression(group, window_size, include_squared_term, lhs_col, rhs_col):\n",
    "    group = group.dropna(subset=[lhs_col, rhs_col]).reset_index(drop=True).copy()\n",
    "\n",
    "    group['intercept'] = np.nan\n",
    "    group['slope'] = np.nan\n",
    "    group['residuals'] = np.nan\n",
    "\n",
    "    if include_squared_term:\n",
    "        group['coskew'] = np.nan\n",
    "        group[f\"{rhs_col}_sq\"] = group[rhs_col]**2\n",
    "\n",
    "    for i in range(window_size, len(group)):\n",
    "        X = group.iloc[i-window_size:i][rhs_col].reset_index(drop=True)\n",
    "\n",
    "        if include_squared_term:\n",
    "            X = pd.concat([X, X**2], axis=1)\n",
    "            X.columns = [rhs_col, f'{rhs_col}_sq']\n",
    "\n",
    "        y = group.iloc[i-window_size:i][lhs_col].reset_index(drop=True)\n",
    "\n",
    "        X = pd.concat([pd.DataFrame(data={'const': np.ones(window_size)}), pd.DataFrame(X)], axis=1) \n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "        group.loc[group.index[i], 'intercept'] = model.params['const']\n",
    "        group.loc[group.index[i], 'slope'] = model.params[rhs_col]\n",
    "\n",
    "        constant = np.array([1])\n",
    "        selected_row = group.loc[group.index[i]][list(X.columns[1:])].values\n",
    "        with_constant = np.concatenate([constant, selected_row])\n",
    "\n",
    "        y_pred = model.predict(with_constant)\n",
    "        group.loc[group.index[i], 'residuals'] = group.loc[group.index[i], lhs_col] - y_pred\n",
    "\n",
    "        if include_squared_term:\n",
    "            group.loc[group.index[i], 'coskew'] = model.params[f'{rhs_col}_sq']\n",
    "\n",
    "    return group\n",
    "\n",
    "def formFinancialCols(panel_df: pd.DataFrame, cmkt_df: pd.DataFrame, cmkt_1h_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Form all financial columns. \"\"\"\n",
    "    # subset to needed columns\n",
    "    fin_df = panel_df[['date', 'asset', 'char_price_t', 'char_mcap_t']].copy()\n",
    "\n",
    "    # merge on new data\n",
    "    fin_df = fin_df.merge(cmkt_df, on='date', how='left', validate='many_to_one') \n",
    "    fin_df = fin_df.merge(cmkt_1h_df, on='date', how='left', validate='many_to_one') \n",
    "\n",
    "    # form characteristics\n",
    "    fin_df   = fin_df.rename(columns={'char_mcap_t': 'char_size_t'})\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_price_t', new_col='char_r_tm1h', range_hours=1, func=calcReturn)\n",
    "    temp_df.loc[temp_df['char_r_tm1h'] < -.95, 'char_r_tm1h'] = -0.95\n",
    "    temp_df.loc[temp_df['char_r_tm1h'] > 100, 'char_r_tm1h'] = 100\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_r_max_tm12h', range_hours=12, func=np.max)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_r_max_tm1', range_hours=24, func=np.max)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_r_max_tm7', range_hours=168, func=np.max)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_r_max_tm30', range_hours=720, func=np.max)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm6h', range_hours=6, func=np.std)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm12h', range_hours=12, func=np.std)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm1', range_hours=24, func=np.std)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm7', range_hours=168, func=np.std)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm30', range_hours=720, func=np.std)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm90', range_hours=2160, func=np.std)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    fin_df['char_tradable_t'] = (fin_df['date'] - fin_df.groupby('asset')['date'].transform('min')).dt.total_seconds() / 3600\n",
    "\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_var5_tm1', range_hours=24, func=lambda x: x.quantile(0.05))\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_var5_tm7', range_hours=168, func=lambda x: x.quantile(0.05))\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_var5_tm90', range_hours=2160, func=lambda x: x.quantile(0.05))\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_shortfall5_tm1', range_hours=24, func=lambda x: x[x < x.quantile(0.05)].mean())\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_shortfall5_tm7', range_hours=168, func=lambda x: x[x < x.quantile(0.05)].mean())\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_shortfall5_tm90', range_hours=2160, func=lambda x: x[x < x.quantile(0.05)].mean())\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    fin_df = formCumRetCols(fin_df)\n",
    "\n",
    "\n",
    "    # Form regression characteristics\n",
    "\n",
    "    # Extract unique asset names\n",
    "    asset_names = fin_df['asset'].unique()\n",
    "\n",
    "    # Perform the 7 day reg\n",
    "    params = {\n",
    "        'window_size': 168,\n",
    "        'include_squared_term': False,\n",
    "        'lhs_col': 'char_r_tm1h',\n",
    "        'rhs_col': 'macro_cmkt_tm7'\n",
    "    }\n",
    "    reg_df = fin_df[['date', 'asset', 'char_r_tm1h', 'macro_cmkt_tm7']].copy()\n",
    "    reg_df = reg_df.dropna()\n",
    "    results = Parallel(n_jobs=-1)(delayed(perform_regression)(reg_df[reg_df['asset'] == asset], **params) for asset in asset_names)\n",
    "    reg_df = pd.concat(results).sort_values(by=['date', 'asset'])\n",
    "    reg_df = reg_df[['date', 'asset', 'residuals', 'intercept', 'slope']]\n",
    "    temp_df = formNewColumnByAsset(\n",
    "        reg_df,\n",
    "        'residuals',\n",
    "        'char_ivol_tm7',\n",
    "        168,\n",
    "        lambda x: x.std()\n",
    "    )\n",
    "    reg_df = reg_df.drop('residuals', axis=1)\n",
    "    reg_df = reg_df.rename(columns={'intercept': 'char_alpha_tm7',\n",
    "                                    'slope': 'char_beta_tm7'})\n",
    "    reg_df = setMissingIfIncomplete(reg_df, 'char_alpha_tm7', 168)\n",
    "    reg_df = setMissingIfIncomplete(reg_df, 'char_beta_tm7', 168)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    fin_df = fin_df.merge(reg_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Perform the 30 day reg \n",
    "    params = {\n",
    "        'window_size': 720,\n",
    "        'include_squared_term': False,\n",
    "        'lhs_col': 'char_r_tm1h',\n",
    "        'rhs_col': 'macro_cmkt_tm7'\n",
    "    }\n",
    "    reg_df = fin_df[['date', 'asset', 'char_r_tm1h', 'macro_cmkt_tm7']].copy()\n",
    "    reg_df = reg_df.dropna()\n",
    "    results = Parallel(n_jobs=-1)(delayed(perform_regression)(reg_df[reg_df['asset'] == asset], **params) for asset in asset_names)\n",
    "    reg_df = pd.concat(results).sort_values(by=['date', 'asset'])\n",
    "    reg_df = reg_df[['date', 'asset', 'residuals', 'intercept', 'slope']]\n",
    "    temp_df = formNewColumnByAsset(\n",
    "        reg_df[['date', 'asset', 'residuals']],\n",
    "        'residuals',\n",
    "        'char_ivol_tm30',\n",
    "        720,\n",
    "        lambda x: x.std()\n",
    "    )\n",
    "    reg_df = reg_df.drop('residuals', axis=1)\n",
    "    reg_df = reg_df.rename(columns={'intercept': 'char_alpha_tm30',\n",
    "                                    'slope': 'char_beta_tm30'})\n",
    "    reg_df = setMissingIfIncomplete(reg_df, 'char_alpha_tm30', 720)\n",
    "    reg_df = setMissingIfIncomplete(reg_df, 'char_beta_tm30', 720)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    fin_df = fin_df.merge(reg_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Perform the 90 day reg\n",
    "    params = {\n",
    "        'window_size': 2160,\n",
    "        'include_squared_term': False,\n",
    "        'lhs_col': 'char_r_tm1h',\n",
    "        'rhs_col': 'macro_cmkt_tm7'\n",
    "    }\n",
    "    reg_df = fin_df[['date', 'asset', 'char_r_tm1h', 'macro_cmkt_tm7']].copy()\n",
    "    reg_df = reg_df.dropna()\n",
    "    results = Parallel(n_jobs=-1)(delayed(perform_regression)(reg_df[reg_df['asset'] == asset], **params) for asset in asset_names)\n",
    "    reg_df = pd.concat(results).sort_values(by=['date', 'asset'])\n",
    "    reg_df = reg_df[['date', 'asset', 'residuals']]\n",
    "    temp_df = formNewColumnByAsset(\n",
    "        reg_df,\n",
    "        'residuals',\n",
    "        'char_ivol_tm90',\n",
    "        2160,\n",
    "        lambda x: x.std()\n",
    "    )\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Perform the 30 day reg with negative returns to extract slope\n",
    "    fin_df['char_r_neg_tm1h'] = 0\n",
    "    fin_df['macro_cmkt_neg_t'] = 0\n",
    "    fin_df.loc[fin_df.char_r_tm1h<0, 'char_r_neg_tm1h'] = fin_df.loc[fin_df.char_r_tm1h<0, 'char_r_tm1h']\n",
    "    fin_df.loc[fin_df.macro_cmkt_tm7<0, 'macro_cmkt_neg_t'] = fin_df.loc[fin_df.macro_cmkt_tm7<0, 'macro_cmkt_tm7']\n",
    "    params = {\n",
    "        'window_size': 720,\n",
    "        'include_squared_term': False,\n",
    "        'lhs_col': 'char_r_neg_tm1h',\n",
    "        'rhs_col': 'macro_cmkt_neg_t'\n",
    "    }\n",
    "    reg_df = fin_df[['date', 'asset', 'char_r_neg_tm1h', 'macro_cmkt_neg_t']].copy()\n",
    "    reg_df = reg_df.dropna()\n",
    "    results = Parallel(n_jobs=-1)(delayed(perform_regression)(reg_df[reg_df['asset'] == asset], **params) for asset in asset_names)\n",
    "    reg_df = pd.concat(results).sort_values(by=['date', 'asset'])\n",
    "    reg_df = reg_df[['date', 'asset', 'slope']]\n",
    "    reg_df = reg_df.rename(columns={'slope': 'char_beta_downside_tm30'})\n",
    "    reg_df = setMissingIfIncomplete(reg_df, 'char_beta_downside_tm30', 720)\n",
    "    fin_df = fin_df.merge(reg_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    fin_df = fin_df.drop(['char_r_neg_tm1h', 'macro_cmkt_neg_t'], axis=1)\n",
    "\n",
    "    # Perform the 30 day reg with two RHS including cmkt^2\n",
    "    params = {\n",
    "        'window_size': 720,\n",
    "        'include_squared_term': True,\n",
    "        'lhs_col': 'char_r_tm1h',\n",
    "        'rhs_col': 'macro_cmkt_tm7'\n",
    "    }\n",
    "    reg_df = fin_df[['date', 'asset', 'char_r_tm1h', 'macro_cmkt_tm7']].copy()\n",
    "    reg_df = reg_df.dropna()\n",
    "    results = Parallel(n_jobs=-1)(delayed(perform_regression)(reg_df[reg_df['asset'] == asset], **params) for asset in asset_names)\n",
    "    reg_df = pd.concat(results).sort_values(by=['date', 'asset'])\n",
    "    reg_df = reg_df[['date', 'asset', 'residuals', 'coskew']]\n",
    "    temp_df = formNewColumnByAsset(\n",
    "        reg_df[['date', 'asset', 'residuals']],\n",
    "        'residuals',\n",
    "        'char_iskew_tm30',\n",
    "        720,\n",
    "        lambda x: scipy.stats.skew(x)\n",
    "    )\n",
    "    reg_df = reg_df.drop('residuals', axis=1)\n",
    "    reg_df = reg_df.rename(columns={'coskew': 'char_coskew_tm30'})\n",
    "    reg_df = setMissingIfIncomplete(reg_df, 'char_coskew_tm30', 720)\n",
    "    fin_df = fin_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    fin_df = fin_df.merge(reg_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # drop cols we dont need\n",
    "    fin_df = fin_df.drop(columns=['macro_cmkt_tm7', 'macro_cmkt_tm1h', 'char_r_tm1h'])\n",
    "\n",
    "    return fin_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formMicrostructureCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Subset to needed columns\n",
    "    mic_df = panel_df[['date', 'asset', 'char_price_t', 'char_volume_t', 'char_trades_t',\n",
    "        'char_bidask_t', 'char_bid_t', 'char_ask_t', 'char_bid_size_t', 'char_ask_size_t', \n",
    "        'char_supply_circ_t']].copy()\n",
    "\n",
    "    # Form bidask in bps\n",
    "    mic_df['char_spread_bps_t'] = mic_df.char_bidask_t / mic_df.char_price_t\n",
    "\n",
    "    # Add on needed momentum col\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_price_t', new_col='char_r_tm1h', range_hours=1, func=calcReturn)\n",
    "    temp_df.loc[temp_df['char_r_tm1h'] < -.95, 'char_r_tm1h'] = -0.95\n",
    "    temp_df.loc[temp_df['char_r_tm1h'] > 100, 'char_r_tm1h'] = 100\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Add temporary columns\n",
    "    mic_df['temp_volume_price_t'] = mic_df.char_volume_t * mic_df.char_price_t\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_r_tm1h', new_col='temp_r_1m1h_abs_avg_tm7', range_hours=168, func=lambda x: np.mean(np.abs(x)))\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_r_tm1h', new_col='temp_r_1m1h_abs_avg_tm1', range_hours=24, func=lambda x: np.mean(np.abs(x)))\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Form characteristics\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_volume_t', new_col='char_volume_sum_tm12h', range_hours=12, func=np.sum)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_volume_t', new_col='char_volume_sum_tm1', range_hours=24, func=np.sum)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_volume_t', new_col='char_volume_sum_tm7', range_hours=168, func=np.sum)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_volume_t', new_col='char_volume_std_tm12h', range_hours=12, func=np.std)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_volume_t', new_col='char_volume_std_tm7', range_hours=168, func=np.std)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_trades_t', new_col='char_trades_sum_tm7', range_hours=168, func=np.sum)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_trades_t', new_col='char_trades_std_tm7', range_hours=168, func=np.std)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='temp_volume_price_t', new_col='char_volume_price_avg_tm7', range_hours=168, func=np.mean)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='temp_volume_price_t', new_col='char_volume_price_std_tm7', range_hours=168, func=np.std)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    mic_df['char_turnover_tm1'] = mic_df.char_volume_sum_tm1 / mic_df.char_supply_circ_t\n",
    "    mic_df['char_turnover_tm7'] = mic_df.char_volume_sum_tm7 / mic_df.char_supply_circ_t\n",
    "    mic_df['char_illiq_tm1'] = mic_df.temp_r_1m1h_abs_avg_tm1 / (mic_df.char_volume_sum_tm1/24)\n",
    "    mic_df['char_illiq_tm7'] = mic_df.temp_r_1m1h_abs_avg_tm7 / (mic_df.char_volume_sum_tm7/168)\n",
    "\n",
    "    # drop unneeded columns\n",
    "    mic_df = mic_df.drop(['char_price_t', 'temp_volume_price_t', \n",
    "        'char_supply_circ_t', 'char_r_tm1h', \n",
    "        'temp_r_1m1h_abs_avg_tm1', 'temp_r_1m1h_abs_avg_tm7'], axis=1)\n",
    "\n",
    "    # Form std of residuals from regressing hourly turnover on a constant over last thirty days\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='char_turnover_tm7', new_col='char_turnover_avg_tm30', range_hours=720, func=np.mean)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    mic_df['temp_diff'] = mic_df.char_turnover_tm7 - mic_df.char_turnover_avg_tm30\n",
    "    temp_df = formNewColumnByAsset(mic_df, target_col='temp_diff', new_col='char_turnover_res_vol_tm30', range_hours=720, func=np.std)\n",
    "    mic_df = mic_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    mic_df = mic_df.drop(columns=['char_turnover_avg_tm30', 'temp_diff'], axis=1)\n",
    "\n",
    "    return mic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formOnchainCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Subset to the needed columns\n",
    "    oc_df = panel_df[['date', 'asset', 'char_network_growth_t',\n",
    "        'char_holders_distribution_total_t', 'char_active_addr_t', \n",
    "        'char_tx_volume_t',\n",
    "        'char_circulation_7d_t', 'char_circulation_30d_t', \n",
    "        'char_circulation_90d_t', 'char_circulation_365d_t', \n",
    "        'char_circulation_3y_t',  'char_dormant_circulation_365d_t', \n",
    "        'char_supply_circ_t', 'char_supply_max_t', \n",
    "        'char_age_mean_dollar_t', 'char_age_destroyed_t']].copy()\n",
    "\n",
    "    # Rename columns\n",
    "    oc_df = oc_df.rename(columns={'char_holders_distribution_total_t': 'char_addr_total_t',\n",
    "                                'char_active_addr_t': 'char_addr_active_t',\n",
    "                                'char_circulation_7d_t': 'char_circulation_tm7',\n",
    "                                'char_circulation_30d_t': 'char_circulation_tm30',\n",
    "                                'char_circulation_90d_t': 'char_circulation_tm90',\n",
    "                                'char_circulation_365d_t': 'char_circulation_tm365',\n",
    "                                'char_circulation_3y_t': 'char_circulation_tm3y',\n",
    "                                'char_dormant_circulation_365d_t': 'char_circulation_dormant_tm365'})\n",
    "\n",
    "    # Form new characteristics\n",
    "    temp_df = formNewColumnByAsset(oc_df, target_col='char_network_growth_t', new_col='char_addr_new_tm1', range_hours=24, func=np.sum)\n",
    "    oc_df = oc_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(oc_df, target_col='char_network_growth_t', new_col='char_addr_new_tm7', range_hours=168, func=np.sum)\n",
    "    oc_df = oc_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(oc_df, target_col='char_addr_active_t', new_col='char_addr_active_tm1', range_hours=24, func=np.sum)\n",
    "    oc_df = oc_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(oc_df, target_col='char_addr_active_t', new_col='char_addr_active_tm7', range_hours=168, func=np.sum)\n",
    "    oc_df = oc_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    oc_df = oc_df.rename(columns={'char_network_growth_t': 'char_addr_new_tm1h',\n",
    "                                'char_addr_active_t': 'char_addr_active_tm1h'})\n",
    "    temp_df = formNewColumnByAsset(oc_df, target_col='char_tx_volume_t', new_col='char_tx_volume_tm1', range_hours=24, func=np.sum)\n",
    "    oc_df = oc_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(oc_df, target_col='char_tx_volume_t', new_col='char_tx_volume_tm7', range_hours=168, func=np.sum)\n",
    "    oc_df = oc_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(oc_df, target_col='char_age_destroyed_t', new_col='char_age_destroyed_tm1', range_hours=24, func=np.mean)\n",
    "    oc_df = oc_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(oc_df, target_col='char_age_destroyed_t', new_col='char_age_destroyed_tm7', range_hours=168, func=np.mean)\n",
    "    oc_df = oc_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Drop columns we do not need\n",
    "    oc_df = oc_df.drop(columns=['char_age_destroyed_t'], axis=1)\n",
    "\n",
    "    # Form change in week over week of char_addr_new_tm7\n",
    "    oc_df['char_addr_new_log_delta_tm2_tm1'] = oc_df.groupby('asset')['char_addr_new_tm1'].transform(lambda x: np.log(x).diff(24))\n",
    "    oc_df['char_addr_new_log_delta_tm14_tm7'] = oc_df.groupby('asset')['char_addr_new_tm7'].transform(lambda x: np.log(x).diff(168))\n",
    "\n",
    "    return oc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formDevCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Subset to the needed columns\n",
    "    dev_df = panel_df[['date', 'asset', 'char_dev_activity_t']].copy()\n",
    "\n",
    "    # Form dev column\n",
    "    temp_df = formNewColumnByAsset(dev_df, target_col='char_dev_activity_t', new_col='char_dev_activity_tm1', range_hours=24, func=np.mean)\n",
    "    dev_df = dev_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(dev_df, target_col='char_dev_activity_t', new_col='char_dev_activity_tm7', range_hours=168, func=np.mean)\n",
    "    dev_df = dev_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Drop nonnecssary columns\n",
    "    dev_df = dev_df.drop(['char_dev_activity_t'], axis=1)\n",
    "    \n",
    "    return dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formSocialCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Subset to the needed columns\n",
    "    s_df = panel_df[['date', 'asset', \n",
    "        'char_sentiment_negative_reddit_t', 'char_sentiment_negative_twitter_t',\n",
    "        'char_sentiment_positive_reddit_t', 'char_sentiment_positive_twitter_t',\n",
    "        'char_sentiment_volume_consumed_total_t', 'char_social_dominance_total_t',\n",
    "        'char_social_volume_reddit_t', 'char_social_volume_twitter_t', \n",
    "        'char_unique_social_volume_total_1h_t']].copy()\n",
    "\n",
    "    # Form columns\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_negative_reddit_t', new_col='char_sent_neg_reddit_tm1', range_hours=24, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_negative_twitter_t', new_col='char_sent_neg_twitter_tm1', range_hours=24, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_positive_reddit_t', new_col='char_sent_pos_reddit_tm1', range_hours=24, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_positive_twitter_t', new_col='char_sent_pos_twitter_tm1', range_hours=24, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_social_volume_reddit_t', new_col='char_social_volume_reddit_tm1', range_hours=24, func=np.sum)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_social_volume_twitter_t', new_col='char_social_volume_twitter_tm1', range_hours=24, func=np.sum)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_volume_consumed_total_t', new_col='char_sent_volume_consumed_tm1', range_hours=24, func=np.sum)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_social_dominance_total_t', new_col='char_social_dom_avg_tm1', range_hours=24, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_unique_social_volume_total_1h_t', new_col='char_social_volume_tm1', range_hours=24, func=np.sum)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_negative_reddit_t', new_col='char_sent_neg_reddit_tm7', range_hours=168, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_negative_twitter_t', new_col='char_sent_neg_twitter_tm7', range_hours=168, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_positive_reddit_t', new_col='char_sent_pos_reddit_tm7', range_hours=168, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_positive_twitter_t', new_col='char_sent_pos_twitter_tm7', range_hours=168, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_social_volume_reddit_t', new_col='char_social_volume_reddit_tm7', range_hours=168, func=np.sum)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_social_volume_twitter_t', new_col='char_social_volume_twitter_tm7', range_hours=168, func=np.sum)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_sentiment_volume_consumed_total_t', new_col='char_sent_volume_consumed_tm7', range_hours=168, func=np.sum)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_social_dominance_total_t', new_col='char_social_dom_avg_tm7', range_hours=168, func=np.mean)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(s_df, target_col='char_unique_social_volume_total_1h_t', new_col='char_social_volume_tm7', range_hours=168, func=np.sum)\n",
    "    s_df = s_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "    # Drop unncessary columns\n",
    "    s_df = s_df.drop(columns=['char_sentiment_negative_reddit_t', 'char_sentiment_negative_twitter_t',\n",
    "        'char_sentiment_positive_reddit_t', 'char_sentiment_positive_twitter_t',\n",
    "        'char_sentiment_volume_consumed_total_t', 'char_social_dominance_total_t',\n",
    "        'char_social_volume_reddit_t', 'char_social_volume_twitter_t',\n",
    "        'char_unique_social_volume_total_1h_t'], axis=1)\n",
    "\n",
    "    return s_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formValueCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # form new df with relevant columns\n",
    "    val_df = panel_df[['date', 'asset', \n",
    "        'char_percent_of_total_supply_in_profit_t',\n",
    "        'char_mvrv_long_short_diff_usd_t', 'char_mvrv_usd_t',\n",
    "        'char_realized_value_usd_t']].copy()\n",
    "\n",
    "    # Form RHS\n",
    "    val_df = val_df.rename(columns={'char_percent_of_total_supply_in_profit_t': 'char_prct_supply_in_profit_t',\n",
    "                                    'char_mvrv_long_short_diff_usd_t': 'char_mvrv_long_short_diff_t',\n",
    "                                    'char_mvrv_usd_t': 'char_mvrv_t',\n",
    "                                    'char_realized_value_usd_t': 'char_size_realized_t'})\n",
    "\n",
    "    return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formFlowCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Form new dataframe to build\n",
    "    bal_df = panel_df[['date', 'asset',\n",
    "        'char_exchange_inflow_usd_t', 'char_exchange_outflow_usd_t',\n",
    "        'char_supply_circ_t',\n",
    "        'char_traders_balance_t',\n",
    "        'char_exchange_balance_t', 'char_dex_balance_t',\n",
    "        'char_cex_balance_t', 'char_amount_in_top_holders_t',\n",
    "        'char_defi_balance_t',\n",
    "        'char_holders_distribution_over_100_t',\n",
    "        'char_holders_distribution_over_100k_t',\n",
    "        'char_holders_distribution_over_10_t',\n",
    "        'char_holders_distribution_over_10k_t',\n",
    "        'char_holders_distribution_over_1M_t',\n",
    "        'char_holders_distribution_over_1_t',\n",
    "        'char_holders_distribution_over_1k_t',\n",
    "        'char_cexes_to_defi_flow_t',\n",
    "        'char_cexes_to_dex_flow_t',\n",
    "        'char_cexes_to_dex_traders_flow_t',\n",
    "        'char_cexes_to_traders_flow_t',\n",
    "        'char_defi_to_cexes_flow_t',\n",
    "        'char_defi_to_dex_traders_flow_t',\n",
    "        'char_defi_to_dexes_flow_t',\n",
    "        'char_defi_to_exchanges_flow_t',\n",
    "        'char_defi_to_traders_flow_t',\n",
    "        'char_dex_to_cexes_flow_t',\n",
    "        'char_dex_traders_to_cexes_flow_t',\n",
    "        'char_dex_traders_to_defi_flow_t',\n",
    "        'char_dex_traders_to_dexes_flow_t',\n",
    "        'char_dex_traders_to_exchanges_flow_t',\n",
    "        'char_dexes_to_defi_flow_t',\n",
    "        'char_dexes_to_dex_traders_flow_t',\n",
    "        'char_dexes_to_traders_flow_t',\n",
    "        'char_exchanges_to_defi_flow_t',\n",
    "        'char_exchanges_to_dex_traders_flow_t',\n",
    "        'char_exchanges_to_genesis_flow_t',\n",
    "        'char_exchanges_to_traders_flow_t',\n",
    "        'char_traders_to_cexes_flow_t',\n",
    "        'char_traders_to_defi_flow_t',\n",
    "        'char_traders_to_dexes_flow_t',\n",
    "        'char_traders_to_exchanges_flow_t']]\n",
    "        \n",
    "    # Form asset characteristics\n",
    "    temp_df = formNewColumnByAsset(bal_df, target_col='char_exchange_inflow_usd_t', new_col='char_exchange_inflow_tm7', range_hours=168, func=np.sum)\n",
    "    bal_df = bal_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    temp_df = formNewColumnByAsset(bal_df, target_col='char_exchange_outflow_usd_t', new_col='char_exchange_outflow_tm7', range_hours=168, func=np.sum)\n",
    "    bal_df = bal_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    bal_df = bal_df.rename(columns={'char_exchange_inflow_usd_t': 'char_exchange_inflow_tm1h',\n",
    "                                    'char_exchange_outflow_usd_t': 'char_exchange_outflow_tm1h'})\n",
    "                                    \n",
    "    bal_df['char_traders_prct_circ_supply_t'] = bal_df['char_traders_balance_t'] / bal_df['char_supply_circ_t']\n",
    "    bal_df['char_exchange_prct_circ_supply_t'] = bal_df['char_exchange_balance_t'] / bal_df['char_supply_circ_t']\n",
    "    bal_df['char_dex_prct_circ_supply_t'] = bal_df['char_dex_balance_t'] / bal_df['char_supply_circ_t']\n",
    "    bal_df['char_cex_prct_circ_supply_t'] = bal_df['char_cex_balance_t'] / bal_df['char_supply_circ_t']\n",
    "    bal_df['char_hodlers_top_prct_circ_supply_t'] = bal_df['char_amount_in_top_holders_t'] / bal_df['char_supply_circ_t']\n",
    "    bal_df['char_defi_prct_circ_supply_t'] = bal_df['char_defi_balance_t'] / bal_df['char_supply_circ_t']\n",
    "\n",
    "    hodlers_cols = ['char_holders_distribution_over_100_t',\n",
    "        'char_holders_distribution_over_100k_t',\n",
    "        'char_holders_distribution_over_10_t',\n",
    "        'char_holders_distribution_over_10k_t',\n",
    "        'char_holders_distribution_over_1M_t',\n",
    "        'char_holders_distribution_over_1_t',\n",
    "        'char_holders_distribution_over_1k_t']\n",
    "\n",
    "    temp_cols_to_drop = []\n",
    "    for col in hodlers_cols:\n",
    "        temp_df = formNewColumnByAsset(bal_df, target_col=col, new_col=col+'m7', range_hours=168, func=np.mean)\n",
    "        bal_df = bal_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "        bal_df[col+'m7_diff_168h'] = bal_df.groupby('asset')[col+'m7'].diff(periods=168)\n",
    "        bal_df[col+'m7_diff_1h'] = bal_df.groupby('asset')[col].diff(periods=1)\n",
    "        temp_cols_to_drop.append(col+'m7')\n",
    "\n",
    "    diff_cols_168h = [col for col in bal_df.columns if 'diff_168h' in col]\n",
    "    diff_cols_1h = [col for col in bal_df.columns if 'diff_1h' in col]\n",
    "    bal_df['char_delta_holders_dist_tm7'] = bal_df[diff_cols_168h].abs().sum(axis=1) / bal_df[hodlers_cols].sum(axis=1)\n",
    "    bal_df['char_delta_holders_dist_tm1h'] = bal_df[diff_cols_1h].abs().sum(axis=1) / bal_df[hodlers_cols].sum(axis=1)\n",
    "\n",
    "    bal_df = bal_df.drop(temp_cols_to_drop+diff_cols_168h+diff_cols_1h+hodlers_cols, axis=1)\n",
    "\n",
    "    flow_cols = ['char_cexes_to_defi_flow_t',\n",
    "        'char_cexes_to_dex_flow_t',\n",
    "        'char_cexes_to_dex_traders_flow_t',\n",
    "        'char_cexes_to_traders_flow_t',\n",
    "        'char_defi_to_cexes_flow_t',\n",
    "        'char_defi_to_dex_traders_flow_t',\n",
    "        'char_defi_to_dexes_flow_t',\n",
    "        'char_defi_to_exchanges_flow_t',\n",
    "        'char_defi_to_traders_flow_t',\n",
    "        'char_dex_to_cexes_flow_t',\n",
    "        'char_dex_traders_to_cexes_flow_t',\n",
    "        'char_dex_traders_to_defi_flow_t',\n",
    "        'char_dex_traders_to_dexes_flow_t',\n",
    "        'char_dex_traders_to_exchanges_flow_t',\n",
    "        'char_dexes_to_defi_flow_t',\n",
    "        'char_dexes_to_dex_traders_flow_t',\n",
    "        'char_dexes_to_traders_flow_t',\n",
    "        'char_exchanges_to_defi_flow_t',\n",
    "        'char_exchanges_to_dex_traders_flow_t',\n",
    "        'char_exchanges_to_genesis_flow_t',\n",
    "        'char_exchanges_to_traders_flow_t',\n",
    "        'char_traders_to_cexes_flow_t',\n",
    "        'char_traders_to_defi_flow_t',\n",
    "        'char_traders_to_dexes_flow_t',\n",
    "        'char_traders_to_exchanges_flow_t']\n",
    "\n",
    "    temp_cols_to_drop = []\n",
    "    for col in flow_cols:\n",
    "        temp_df = formNewColumnByAsset(bal_df, target_col=col, new_col=col+'m7', range_hours=168, func=np.mean)\n",
    "        bal_df = bal_df.merge(temp_df, on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "        bal_df[col+'m7_diff_168h'] = bal_df.groupby('asset')[col+'m7'].diff(periods=168)\n",
    "        bal_df[col+'m7_diff_1h'] = bal_df.groupby('asset')[col].diff(periods=1)\n",
    "        temp_cols_to_drop.append(col+'m7')\n",
    "\n",
    "    diff_cols_168h = [col for col in bal_df.columns if 'diff_168h' in col]\n",
    "    diff_cols_1h = [col for col in bal_df.columns if 'diff_1h' in col]\n",
    "    bal_df['char_delta_flow_dist_tm7'] = bal_df[diff_cols_168h].sum(axis=1) / bal_df[flow_cols].sum(axis=1)\n",
    "    bal_df['char_delta_flow_dist_tm1h'] = bal_df[diff_cols_1h].sum(axis=1) / bal_df[flow_cols].sum(axis=1)\n",
    "\n",
    "    bal_df = bal_df.drop(temp_cols_to_drop+diff_cols_168h+diff_cols_1h+flow_cols, axis=1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    bal_df = bal_df.drop(columns=['char_supply_circ_t', 'char_traders_balance_t',\n",
    "        'char_exchange_balance_t', 'char_dex_balance_t',  'char_cex_balance_t', \n",
    "        'char_amount_in_top_holders_t', 'char_defi_balance_t'], axis=1)\n",
    "\n",
    "    return bal_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formMacroCovariates(\n",
    "    macro_df: pd.DataFrame, cmkt_df: pd.DataFrame, cmkt_1h_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Merge on market returns\n",
    "    macro_df = macro_df.merge(cmkt_df, on='date', how='outer', validate='one_to_one')\n",
    "    macro_df = macro_df.merge(cmkt_1h_df, on='date', how='outer', validate='one_to_one')\n",
    "\n",
    "    # Form rolling sum columns\n",
    "    sum_cols = ['macro_ico_count_t',\n",
    "        'macro_dex_volume_t',\n",
    "        'macro_ex_usd_volume_24h_cex_t',\n",
    "        'macro_ex_volume_future_usd_t',\n",
    "        'macro_ex_volume_t',\n",
    "        'macro_total_nft_retail_trades_t',\n",
    "        'macro_total_nft_retail_volume_t',\n",
    "        'macro_total_nft_trades_t',\n",
    "        'macro_total_nft_volume_t',\n",
    "        'macro_total_nft_whale_trades_t',\n",
    "        'macro_total_nft_whale_volume_t',\n",
    "        'macro_us_ex_volume_future_usd_t',\n",
    "        'macro_us_ex_volume_spot_usd_t',\n",
    "        'macro_total_usd_ask_size_t',\n",
    "        'macro_total_usd_bid_size_t']\n",
    "    for col in sum_cols:\n",
    "        macro_df[col[:-2]+'_sum_tm7'] = macro_df[col].rolling(168).sum()\n",
    "\n",
    "    # Form rolling avg cols\n",
    "    macro_df['macro_avg_bidask_tm7'] = macro_df['macro_avg_bidask_t'].rolling(168).mean()\n",
    "\n",
    "    return macro_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formRHS(df: pd.DataFrame) -> Tuple:\n",
    "    \"\"\" Form RHS covariates, including asset characteristics and macro covariates. \"\"\"\n",
    "    # Form panel and macro dataframes\n",
    "    macro_cols = [col for col in df.columns if 'macro_' in col]\n",
    "    panel_df = df.drop(macro_cols, axis=1)\n",
    "    macro_df = df[['date']+macro_cols]\n",
    "    macro_df = macro_df.drop_duplicates()\n",
    "    assert macro_df.date.is_unique\n",
    "\n",
    "    # add on cols needed for panel data\n",
    "    panel_df = panel_df.merge(macro_df[['date', 'macro_dgs1mo_t']], on=['date'], how='left', validate='many_to_one')\n",
    "\n",
    "    # Form all RHS asset characteristics\n",
    "    static_df = formStaticCols(panel_df)\n",
    "    static_cols = [col for col in static_df.columns if col not in ['date', 'asset']]\n",
    "    panel_df = panel_df.drop(static_cols, axis=1)\n",
    "    desc_stat_df = formDescStatCols(panel_df)\n",
    "    desc_stat_cols = [col for col in desc_stat_df.columns if col not in ['date', 'asset']]\n",
    "    panel_df = panel_df.drop(desc_stat_cols, axis=1)\n",
    "    mom_df = formMomentumCols(panel_df, static_df)\n",
    "    cmkt_df, cmkt_1h_df = formCmktCol(panel_df, asset_universe_dict)\n",
    "    panel_df = panel_df.drop('macro_dgs1mo_t', axis=1)\n",
    "    fin_df  = formFinancialCols(panel_df, cmkt_df, cmkt_1h_df)\n",
    "    mic_df = formMicrostructureCols(panel_df)\n",
    "    mic_cols = [col for col in mic_df.columns if col not in ['date', 'asset']]\n",
    "    drop_cols = set(mic_cols).intersection(set(panel_df.columns))\n",
    "    panel_df = panel_df.drop(drop_cols, axis=1)\n",
    "    oc_df  = formOnchainCols(panel_df)\n",
    "    oc_cols = [col for col in oc_df.columns if col not in ['date', 'asset']]\n",
    "    oc_cols.remove('char_supply_circ_t')\n",
    "    drop_cols = set(oc_cols).intersection(set(panel_df.columns))\n",
    "    panel_df = panel_df.drop(drop_cols, axis=1)\n",
    "    dev_df = formDevCols(panel_df)\n",
    "    panel_df = panel_df.drop('char_dev_activity_t', axis=1)\n",
    "    s_df = formSocialCols(panel_df)\n",
    "    social_cols = [col for col in s_df.columns if col not in ['date', 'asset']]\n",
    "    drop_cols = set(social_cols).intersection(set(panel_df.columns))\n",
    "    panel_df = panel_df.drop(drop_cols, axis=1)\n",
    "    val_df = formValueCols(panel_df)\n",
    "    bal_df = formFlowCols(panel_df)\n",
    "    del panel_df\n",
    "    gc.collect()\n",
    "\n",
    "    # Merge all characteristics together and clean up memory\n",
    "    char_df = static_df.merge(desc_stat_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    del static_df, desc_stat_df\n",
    "    gc.collect()\n",
    "    char_df = char_df.merge(mom_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    del mom_df\n",
    "    gc.collect()\n",
    "    char_df = char_df.merge(fin_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    del fin_df\n",
    "    gc.collect()\n",
    "    char_df = char_df.merge(mic_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    char_df = char_df.merge(oc_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    char_df = char_df.merge(dev_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    del mic_df, oc_df, dev_df\n",
    "    gc.collect()\n",
    "    char_df = char_df.merge(s_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    char_df = char_df.merge(val_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    char_df = char_df.merge(bal_df, on=['date', 'asset'], how='outer', validate='one_to_one')\n",
    "    del s_df, val_df, bal_df\n",
    "    gc.collect()\n",
    "\n",
    "    # Add cmkt return to macro and form the rest of the macro covariates\n",
    "    macro_df = formMacroCovariates(macro_df, cmkt_df, cmkt_1h_df)\n",
    "\n",
    "    # Return data\n",
    "    return char_df, macro_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formWeeklyPanel(panel_df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    # Drop to weekly frequency\n",
    "    weekly_df = subsetToWeeklyFreq(panel_df)\n",
    "    weekly_df = weekly_df.copy()\n",
    "\n",
    "    # Drop other lhs variable\n",
    "    weekly_df = weekly_df.drop(columns=['r_ex_tp1'], axis=1)\n",
    "\n",
    "    # Rename weekly LHS to match naming convention for weekly panel\n",
    "    weekly_df = weekly_df.rename(columns={'r_ex_tp168': 'r_ex_tp7'})\n",
    "\n",
    "    # Form list of columns\n",
    "    first_cols = ['date', 'asset', 'r_ex_tp7']\n",
    "    industry_cols = [col for col in weekly_df.columns if ('industry' in col) & ('industry_tm' not in col)]\n",
    "    usage_cols = [col for col in weekly_df.columns if 'asset_usage' in col]\n",
    "    static_cols = (industry_cols + usage_cols + \n",
    "        ['char_pow', 'char_pos', 'char_ico_price', 'char_ico'])\n",
    "    char_cols = [col for col in weekly_df.columns if 'char_' in col]\n",
    "    char_cols = list(set(char_cols).difference(static_cols))\n",
    "    char_cols = [col for col in char_cols if col[-1:] != 'h']\n",
    "    char_cols.sort()\n",
    "    macro_tm7_cols = [col for col in weekly_df.columns if ('macro_' in col) & (col[-3:] == 'tm7')]\n",
    "    macro_cols_to_keep_for_weekly = ['macro_snp500_t',\n",
    "        'macro_snp_div_yield_t',\n",
    "        'macro_snp_pe_t',\n",
    "        'macro_vixclsx_t',\n",
    "        'macro_fedfunds_t',\n",
    "        'macro_dgs1mo_t',\n",
    "        'macro_gs1_t',\n",
    "        'macro_gs5_t',\n",
    "        'macro_gs10_t',\n",
    "        'macro_cp3mx_t',\n",
    "        'macro_aaa_t',\n",
    "        'macro_baa_t',\n",
    "        'macro_m1sl_t',\n",
    "        'macro_m2real_t',\n",
    "        'macro_m2sl_t',\n",
    "        'macro_expinf10yr_t',\n",
    "        'macro_expinf1yr_t',\n",
    "        'macro_expinf20yr_t',\n",
    "        'macro_expinf2yr_t',\n",
    "        'macro_expinf30yr_t',\n",
    "        'macro_expinf3yr_t',\n",
    "        'macro_expinf5yr_t',\n",
    "        'macro_t10yie_t',\n",
    "        'macro_t20yiem_t',\n",
    "        'macro_t30yiem_t',\n",
    "        'macro_t5yie_t',\n",
    "        'macro_tb3ms_t',\n",
    "        'macro_tb6ms_t',\n",
    "        'macro_stablecoin_dev_t',\n",
    "        'macro_active_cryptos_t',\n",
    "        'macro_active_exchanges_t',\n",
    "        'macro_active_market_pairs_t',\n",
    "        'macro_ex_num_pairs_cex_t',\n",
    "        'macro_ex_num_pairs_dex_t',\n",
    "        'macro_ex_open_interest_future_usd_t',\n",
    "        'macro_aave_med_borrow_apy_t',\n",
    "        'macro_aave_med_supply_apy_t',\n",
    "        'macro_mcd_avg_liq_t',\n",
    "        'macro_mcd_med_collat_ratio_t',\n",
    "        'macro_btc_diff_mean_t',\n",
    "        'macro_btc_fee_med_usd_t',\n",
    "        'macro_btc_fee_tot_usd_t',\n",
    "        'macro_btc_hash_rate_t',\n",
    "        'macro_btc_sply_cur_t',\n",
    "        'macro_btc_sply_ff_t',\n",
    "        'macro_btc_sply_utxo_loss_t',\n",
    "        'macro_btc_sply_utxo_prof_t',\n",
    "        'macro_btc_tx_tfr_cnt_t',\n",
    "        'macro_btc_tx_tfr_val_adj_usd_t',\n",
    "        'macro_btc_tx_tfr_val_med_usd_t',\n",
    "        'macro_btc_utxo_age_med_t',\n",
    "        'macro_eth_fee_med_t',\n",
    "        'macro_eth_sply_cur_t',\n",
    "        'macro_eth_sply_ff_t',\n",
    "        'macro_eth_stakers_count_t',\n",
    "        'macro_eth_total_fee_t',\n",
    "        'macro_eth_tx_tfr_cnt_t',\n",
    "        'macro_eth_tx_tfr_val_adj_usd_t',\n",
    "        'macro_eth_tx_tfr_val_med_usd_t',\n",
    "        'macro_total_aave_borrowed_t',\n",
    "        'macro_total_aave_deposits_t',\n",
    "        'macro_total_aave_liq_t',\n",
    "        'macro_total_aave_new_debt_t',\n",
    "        'macro_total_aave_supply_t',\n",
    "        'macro_total_compound_borrowed_t',\n",
    "        'macro_total_compound_deposits_t',\n",
    "        'macro_total_compound_liq_t',\n",
    "        'macro_total_compound_new_debt_t',\n",
    "        'macro_total_compound_supply_t',\n",
    "        'macro_total_maker_borrowed_t',\n",
    "        'macro_total_maker_deposits_t',\n",
    "        'macro_total_maker_supply_t',\n",
    "        'macro_total_uni_claims_t',\n",
    "        'macro_total_usd_mcap_t',\n",
    "        'macro_ico_count_t',\n",
    "        'macro_dex_volume_t',\n",
    "        'macro_ex_usd_volume_24h_cex_t',\n",
    "        'macro_ex_volume_future_usd_t',\n",
    "        'macro_ex_volume_t',\n",
    "        'macro_total_nft_retail_trades_t',\n",
    "        'macro_total_nft_retail_volume_t',\n",
    "        'macro_total_nft_trades_t',\n",
    "        'macro_total_nft_volume_t',\n",
    "        'macro_total_nft_whale_trades_t',\n",
    "        'macro_total_nft_whale_volume_t',\n",
    "        'macro_us_ex_volume_future_usd_t',\n",
    "        'macro_us_ex_volume_spot_usd_t']\n",
    "    macro_cols = macro_cols_to_keep_for_weekly+macro_tm7_cols\n",
    "    macro_cols.sort()\n",
    "    weekly_df = weekly_df[first_cols+static_cols+char_cols+macro_cols]\n",
    "\n",
    "    # Drop rows missing LHS variable\n",
    "    weekly_df = weekly_df[weekly_df.r_ex_tp7.notnull()]\n",
    "\n",
    "    # Cut to study period\n",
    "    weekly_df = weekly_df[weekly_df.date.dt.year >= 2018]\n",
    "    weekly_df = weekly_df[weekly_df.date.dt.year < 2023]\n",
    "\n",
    "    # Cut down to only tradable date-assets\n",
    "    weekly_df = subsetByMonthToAssetUniverse(weekly_df, asset_universe_dict)\n",
    "\n",
    "    # Fix inf values to missing\n",
    "    weekly_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Forward fill missing values in RHS columns\n",
    "    weekly_df = weekly_df.groupby('asset').apply(lambda group: group.fillna(method='ffill'))\n",
    "\n",
    "    # Fill remaining missings\n",
    "    weekly_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Replace negative values with zero in known columns\n",
    "    circ_cols = [col for col in weekly_df.columns if 'char_circulation_' in col]\n",
    "    drop_neg_cols = (circ_cols \n",
    "        + ['char_age_mean_dollar_t', 'char_exchange_prct_circ_supply_t', 'char_prct_supply_in_profit_t'])\n",
    "    for col in drop_neg_cols:\n",
    "        weekly_df.loc[weekly_df[col] < 0, col] = 0\n",
    "\n",
    "    # Trim max values in known columns\n",
    "    weekly_df.loc[weekly_df.char_cex_prct_circ_supply_t > 1, 'char_cex_prct_circ_supply_t'] = 1\n",
    "    weekly_df.loc[weekly_df.char_defi_prct_circ_supply_t > 1, 'char_cex_prct_circ_supply_t'] = 1\n",
    "    weekly_df.loc[weekly_df.char_traders_prct_circ_supply_t > 1, 'char_traders_prct_circ_supply_t'] = 1\n",
    "\n",
    "    weekly_df.loc[weekly_df.char_prct_supply_in_profit_t > 100, 'char_prct_supply_in_profit_t'] = 100\n",
    "\n",
    "    weekly_df.loc[weekly_df.char_alpha_tm30 > 0.3, 'char_alpha_tm30'] = 0.3\n",
    "    weekly_df.loc[weekly_df.char_alpha_tm7 > 0.3, 'char_alpha_tm7'] = 0.3\n",
    "\n",
    "    weekly_df.loc[weekly_df.char_beta_downside_tm30 > 0.5, 'char_beta_downside_tm30'] = 0.5\n",
    "\n",
    "    weekly_df.loc[weekly_df.char_beta_tm30 > 1, 'char_beta_tm30'] = 1\n",
    "    weekly_df.loc[weekly_df.char_beta_tm7 > 1, 'char_beta_tm7'] = 1\n",
    "\n",
    "    weekly_df.loc[weekly_df.char_coskew_tm30 > 10, 'char_coskew_tm30'] = 10\n",
    "\n",
    "    weekly_df.loc[weekly_df.char_delta_flow_dist_tm7 > 100, 'char_delta_flow_dist_tm7'] = 100\n",
    "    weekly_df.loc[weekly_df.char_delta_flow_dist_tm7 < -100, 'char_delta_flow_dist_tm7'] = -100\n",
    "\n",
    "    weekly_df.loc[weekly_df.char_ivol_tm7 > 10, 'char_ivol_tm7'] = 10\n",
    "    weekly_df.loc[weekly_df.char_ivol_tm90 > 2, 'char_ivol_tm90'] = 2\n",
    "\n",
    "    weekly_df.loc[weekly_df.char_vol_tm1 > 20, 'char_vol_tm1'] = 20\n",
    "    weekly_df.loc[weekly_df.char_vol_tm30 > 3, 'char_vol_tm30'] = 3\n",
    "    weekly_df.loc[weekly_df.char_vol_tm7 > 5, 'char_vol_tm7'] = 5\n",
    "    weekly_df.loc[weekly_df.char_vol_tm90 > 2, 'char_vol_tm90'] = 2\n",
    "\n",
    "    # Fix known return errors\n",
    "    weekly_df.loc[(weekly_df.date=='2022-05-22') & (weekly_df.asset=='luna'), 'r_ex_tp7'] = 0\n",
    "\n",
    "    # Ensure the weekly_df dataframe contains all dates\n",
    "    earliest_date = weekly_df['date'].min()\n",
    "    end_date = pd.to_datetime('2022-12-31')\n",
    "    all_sundays = pd.date_range(earliest_date, end_date, freq='W-SUN')\n",
    "    assert len(all_sundays) == len(weekly_df.date.unique())\n",
    "\n",
    "    # Ensure no missing values\n",
    "    assert 0 == weekly_df.isnull().sum().sum()\n",
    "\n",
    "    # Sort rows and columns and reset index\n",
    "    weekly_df = weekly_df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "\n",
    "    # Ensure no duplicates by date and asset\n",
    "    assert not weekly_df.duplicated(subset=['date', 'asset']).any()\n",
    "\n",
    "    # Convert all float64 columns to float32\n",
    "    for col in weekly_df.columns:\n",
    "        if weekly_df[col].dtype == np.float64:\n",
    "            weekly_df[col] = weekly_df[col].astype(np.float32)\n",
    "\n",
    "    return weekly_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formHourlyPanels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Drop other lhs variable\n",
    "    df = df.drop(columns=['r_ex_tp168'], axis=1)\n",
    "\n",
    "    # Let's get rid of the char ico price column for now\n",
    "    df = df.drop('char_ico_price', axis=1)\n",
    "\n",
    "    # Form list of columns\n",
    "    first_cols = ['date', 'asset', 'r_ex_tp1']\n",
    "    industry_cols = [col for col in df.columns if ('industry' in col) & ('industry_tm' not in col)]\n",
    "    usage_cols = [col for col in df.columns if 'asset_usage' in col]\n",
    "    static_cols = (industry_cols + usage_cols + \n",
    "        ['char_pow', 'char_pos', 'char_ico'])\n",
    "    char_cols = [col for col in df.columns if 'char_' in col]\n",
    "    char_cols = list(set(char_cols).difference(static_cols))\n",
    "    char_cols.sort()\n",
    "    macro_tm7_cols = [col for col in df.columns if ('macro_' in col) & (col[-3:] == 'tm7')]\n",
    "    macro_cols = [col for col in df.columns if ('macro_' in col)]\n",
    "    macro_cols = list(set(macro_cols).difference(set(macro_tm7_cols)))\n",
    "    macro_cols.sort()\n",
    "\n",
    "    # Keep relevant columns\n",
    "    df = df[first_cols+static_cols+char_cols+macro_cols]\n",
    "\n",
    "    # Drop rows missing LHS variable\n",
    "    df = df[df.r_ex_tp1.notnull()]\n",
    "\n",
    "    # Cut to study period\n",
    "    df = df[df.date.dt.year >= 2018]\n",
    "    df = df[df.date.dt.year < 2023]\n",
    "\n",
    "    # Fix inf values to missing\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Forward fill missing values in RHS columns\n",
    "    df = df.groupby('asset').apply(lambda group: group.fillna(method='ffill'))\n",
    "\n",
    "    # Fill remaining missing with cross-sectional median\n",
    "    missing = df.isnull().sum().T\n",
    "    missing_cols = list(missing[missing > 0].index)\n",
    "    for col in missing_cols:\n",
    "        df[col].fillna(df.groupby('date')[col].transform('median'), inplace=True)\n",
    "\n",
    "    # Ensure the df dataframe contains all dates\n",
    "    first_date = df['date'].min()\n",
    "    last_date = df['date'].max()\n",
    "    all_hours = pd.date_range(first_date, last_date, freq='H')\n",
    "    assert len(all_hours) == len(df.date.unique())\n",
    "\n",
    "    # Assert range (sans char and macro cols which i handle in relevant script)\n",
    "    assert 0 == df[static_cols].min().min()\n",
    "    assert 1 == df[static_cols].max().max()\n",
    "    assert -1 < df.r_ex_tp1.min()\n",
    "    assert 2 >= df.r_ex_tp1.max()\n",
    "\n",
    "    # Ensure no missing values\n",
    "    assert 0 == df.isnull().sum().sum()\n",
    "\n",
    "    # Sort rows and columns and reset index\n",
    "    df = df.sort_values(by=['date', 'asset'], ignore_index=True)\n",
    "\n",
    "    # Ensure no duplicates by date and asset\n",
    "    assert not df.duplicated(subset=['date', 'asset']).any()\n",
    "\n",
    "    # Convert all float64 columns to float32\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    # Break training and test data\n",
    "    train_df = df[~((df.date.dt.year == 2022) & (df.date.dt.month >= 7))].copy()\n",
    "    test_df = df[((df.date.dt.year == 2022) & (df.date.dt.month >= 7))].copy()\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_423731/859128671.py:27: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for _, row in df[industry_columns].sum(axis=1).iteritems():\n",
      "/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/adam/miniconda3/envs/tf/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_423731/2532325951.py:128: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  weekly_df = weekly_df.groupby('asset').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_423731/188426855.py:36: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df = df.groupby('asset').apply(lambda group: group.fillna(method='ffill'))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    ASSET_IN_FP         = '../data/clean/asset_universe_dict.pickle'\n",
    "    PANEL_IN_FP         = '../data/clean/panel.pkl'\n",
    "    WEEKLY_PANEL_OUT_FP  = '../data/clean/panel_weekly.pkl' \n",
    "    HOURLY_PANEL_TRAIN_OUT_FP = '../data/clean/panel_train.pkl'\n",
    "    HOURLY_PANEL_TEST_OUT_FP = '../data/clean/panel_test.pkl'\n",
    "\n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # drop unncessary col\n",
    "    df = df.drop(columns=['r_ex_tp24'], axis=1)\n",
    "\n",
    "    # form rhs covars\n",
    "    char_df, macro_df = formRHS(df)\n",
    "    panel_df = char_df.merge(\n",
    "        df[['date', 'asset', 'r_ex_tp1', 'r_ex_tp168']], on=['date', 'asset'], how='right', validate='one_to_one')\n",
    "    panel_df = panel_df.merge(macro_df, on='date', how='left', validate='many_to_one')\n",
    "    del char_df, macro_df, df\n",
    "    gc.collect()\n",
    "\n",
    "    # Form weekly panel\n",
    "    weekly_df = formWeeklyPanel(panel_df, asset_universe_dict)\n",
    "    weekly_df.to_pickle(WEEKLY_PANEL_OUT_FP)\n",
    "\n",
    "    # Form hourly panels\n",
    "    train_df, test_df = formHourlyPanels(panel_df)\n",
    "    train_df.to_pickle(HOURLY_PANEL_TRAIN_OUT_FP)\n",
    "    test_df.to_pickle(HOURLY_PANEL_TEST_OUT_FP)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
