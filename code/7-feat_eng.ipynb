{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetByMonthToAssetUniverse(panel_df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subset the panel data to the assets in each month (key) of asset_universe_dict.\n",
    "\n",
    "    Args:\n",
    "        panel_df: A Pandas DataFrame containing panel data at the asset-hour level\n",
    "                    with ID columns 'date' and 'asset'.\n",
    "        asset_universe_dict: A dictionary with keys as dates in the format YYYY-MM-DD \n",
    "                                and values as lists of asset strings.\n",
    "    \n",
    "    Returns: A Pandas DataFrame without the rows not included in the study.\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to store the subsetted data\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through the keys (dates) in the asset_universe_dict\n",
    "    for date_key, asset_list in asset_universe_dict.items():\n",
    "        # Convert the date_key string to a datetime object to work with pandas\n",
    "        date_key_dt = pd.to_datetime(date_key)\n",
    "        \n",
    "        # Create a date mask for the month\n",
    "        date_mask = (panel_df['date'].dt.year == date_key_dt.year) & (panel_df['date'].dt.month == date_key_dt.month)\n",
    "        \n",
    "        # Subset the panel_df DataFrame based on the date mask and the asset list\n",
    "        subset = panel_df[date_mask & panel_df['asset'].isin(asset_list)]\n",
    "        \n",
    "        # Append the subset to the new_df DataFrame\n",
    "        new_df = new_df.append(subset, ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def subsetToWeeklyFreq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Takes DataFrame with datetime column \"date\" to subset\n",
    "        it to observations on Sunday at midnight. \"\"\"\n",
    "    return df[(df.date.dt.day_name() == 'Sunday') \n",
    "            & (df.date.dt.time == pd.Timestamp('00:00:00').time())]\n",
    "            \n",
    "def setMissingIfIncomplete(panel_df: pd.DataFrame, return_col: str, hours_to_check: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Set the specified return column to missing (np.nan) if the DataFrame is missing any of the previous hours\n",
    "    specified by hours_to_check for each asset.\n",
    "\n",
    "    Args:\n",
    "        panel_df (pd.DataFrame): The input DataFrame\n",
    "        return_col (str): The name of the column to set to missing (np.nan) if any of the previous hours are missing\n",
    "        hours_to_check (int): The number of previous hours to check for.\n",
    "\n",
    "    Returns: The DataFrame with the return column set to missing if any of the previous hours are missing\n",
    "    \"\"\"\n",
    "    # Shift the date column by the specified hours_to_check\n",
    "    panel_df['prev_date'] = panel_df.groupby('asset')['date'].shift(hours_to_check)\n",
    "    \n",
    "    # Calculate the rolling sum of hour differences over a window of size hours_to_check\n",
    "    panel_df['hours_present'] = panel_df.groupby('asset')['date'].transform(\n",
    "        lambda x: x.diff().dt.total_seconds().rolling(window=hours_to_check).sum() / 3600\n",
    "    )\n",
    "\n",
    "    # Set the return column value to missing (None) if the total number of hours present is not equal to hours_to_check\n",
    "    panel_df.loc[panel_df['hours_present'] != hours_to_check, return_col] = np.nan\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    panel_df.drop(columns=['prev_date', 'hours_present'], inplace=True)\n",
    "\n",
    "    return panel_df\n",
    "\n",
    "def formNewColumnByAsset(panel_df: pd.DataFrame, target_col: str, new_col: str, range_hours: int, func) -> pd.DataFrame:\n",
    "    \"\"\" Adds a new column to a Pandas DataFrame containing panel data at the asset-hour level.\n",
    "    The new column is calculated by applying a function to a range of previous values for each asset.\n",
    "    Any values that do not have the previous range_hours are reset to missing (np.nan).\n",
    "\n",
    "    Args:\n",
    "        panel_df: Pandas DataFrame containing the panel data.\n",
    "        target_col: Name of the column to apply the given function to.\n",
    "        new_col: Name of the new column to add.\n",
    "        range_hours: Number of previous hours to consider for each asset.\n",
    "        func: Function to apply to the range of values for each asset.\n",
    "    \n",
    "    Returns: The modified panel_df with the new column added.\n",
    "    \"\"\"\n",
    "    # Group the DataFrame by asset\n",
    "    grouped = panel_df.groupby('asset')\n",
    "    \n",
    "    # Apply the function to each asset's previous values and store the result in a new Series\n",
    "    result = grouped.apply(lambda x: x[target_col].rolling(range_hours).apply(func))\n",
    "    \n",
    "    # Add the new column to the DataFrame\n",
    "    panel_df[new_col] = result.values\n",
    "    \n",
    "    # Reset missing values\n",
    "    panel_df = setMissingIfIncomplete(panel_df, new_col, range_hours)\n",
    "    \n",
    "    return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set args\n",
    "    ASSET_IN_FP         = '../data/clean/asset_universe_dict.pickle'\n",
    "    PANEL_IN_FP         = '../data/clean/panel.pkl'\n",
    "    WEEKLY_PANEL_OUT_FP  = '../data/clean/weekly_panel.pkl' \n",
    "    HOURLY_PANEL_OUT_FP = '../data/clean/hourly_panel.pkl' \n",
    "    \n",
    "    # import\n",
    "    with open(ASSET_IN_FP, \"rb\") as f:\n",
    "        asset_universe_dict = pickle.load(f)\n",
    "    panel_df = pd.read_pickle(PANEL_IN_FP)\n",
    "\n",
    "    # form panels\n",
    "    # weekly_df = formWeeklyPanel(panel_df)\n",
    "    \n",
    "    # output\n",
    "    # weekly_df.to_pickle(WEEKLY_PANEL_OUT_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO FEAT ENG FUNCTIONS I WANT:\n",
    "\n",
    "# REPORT CORR BETWEEN TWO COLUMNS\n",
    "# -OVERALL\n",
    "# -BY YEAR\n",
    "# -# SIGN CHANGES ACROSS YEARS\n",
    "\n",
    "# REPORT MI BETWEEN TWO COLUMNS\n",
    "# -OVERALL\n",
    "# -BY YEAR\n",
    "# -# SIGN CHANGES ACROSS YEARS\n",
    "\n",
    "# REPORT AVG DIFF BTWN AVG RET OF TOP AND BOTTOM TERTILE AS SORTED BY COL\n",
    "\n",
    "# REPORT CORR WITH OTHER RHS COLS WHERE I JUST REPORT HTE CoRR IF ABOVE 0.8 in abs\n",
    "\n",
    "series2 = df.price_usd[1:].values\n",
    "for col in df.columns.values[1:-2]:\n",
    "    series1 = df[col][:-1].values\n",
    "\n",
    "    # Find indices where both series have non-missing values\n",
    "    non_missing_indices = np.logical_not(np.isnan(series1) | np.isnan(series2))\n",
    "\n",
    "    # Compute the correlation using non-missing values only\n",
    "    corr_matrix = np.corrcoef(series1[non_missing_indices], series2[non_missing_indices])\n",
    "\n",
    "    # Extract the correlation coefficient (off-diagonal element)\n",
    "    corr_coef = corr_matrix[0, 1]\n",
    "    print(col)\n",
    "    print(f\"Correlation coefficient: {corr_coef}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formStaticCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # set columns to keep\n",
    "    static_cols = ['char_industry_asset_mgmt',\n",
    "                'char_industry_cex',\n",
    "                'char_industry_cloud_compute',\n",
    "                'char_industry_currency',\n",
    "                'char_industry_data_mgmt',\n",
    "                'char_industry_dex',\n",
    "                'char_industry_gaming',\n",
    "                'char_industry_infra',\n",
    "                'char_industry_interop',\n",
    "                'char_industry_lending',\n",
    "                'char_industry_media',\n",
    "                'char_industry_other_defi',\n",
    "                'char_industry_smart_contract',\n",
    "                'char_asset_usage_access',\n",
    "                'char_asset_usage_discount',\n",
    "                'char_asset_usage_dividends',\n",
    "                'char_asset_usage_payments',\n",
    "                'char_asset_usage_vote',\n",
    "                'char_asset_usage_work',\n",
    "                'char_pow',\n",
    "                'char_pos',\n",
    "                'char_ico_price',\n",
    "                'char_ico']\n",
    "    \n",
    "    # form column subset\n",
    "    static_df = panel_df[['date', 'asset']+static_cols]\n",
    "\n",
    "    # subset to weekly freq for Sunday midnight\n",
    "    static_df = subsetToWeeklyFreq(static_df)\n",
    "\n",
    "    return static_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formDescStatCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # identify cols\n",
    "    cols = ['char_price_global_t', 'char_volume_24h_global_t', \n",
    "            'char_ico_days_since_t', 'char_vc_t',\n",
    "            'char_rank_cmc_t', 'char_num_pairs_t']\n",
    "\n",
    "    # subset to cols\n",
    "    desc_stat_df = panel_df[['date', 'asset']+cols]\n",
    "\n",
    "    # subset to weekly freq for Sunday midnight\n",
    "    desc_stat_df = subsetToWeeklyFreq(desc_stat_df)\n",
    "\n",
    "    return desc_stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formMomentumCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Define function to use in the Pandas rolling\n",
    "    def calcReturn(x: pd.Series) -> float:\n",
    "        return (x.iloc[-1] - x.iloc[0]) / x.iloc[0]\n",
    "\n",
    "    # Form relevant data\n",
    "    temp_df = panel_df[['date', 'asset', 'char_price_t']].copy()\n",
    "\n",
    "    # Calculate momentums over various day windows: 1, 7, 14, 30, 60, 90.\n",
    "    mom1h_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm1h', range_hours=1, func=calcReturn)\n",
    "    mom1_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm1', range_hours=24, func=calcReturn)\n",
    "    mom7_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm7', range_hours=168, func=calcReturn)\n",
    "    mom14_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm14', range_hours=336, func=calcReturn)\n",
    "    mom30_14_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm30_tm14', range_hours=384, func=calcReturn)\n",
    "    mom30_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm30', range_hours=720, func=calcReturn)\n",
    "    mom60_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm60', range_hours=1440, func=calcReturn)\n",
    "    mom90_df = formNewColumnByAsset(temp_df, target_col='char_price_t', new_col='char_r_tm90', range_hours=2160, func=calcReturn)\n",
    "\n",
    "    # Form reversals\n",
    "    # TODO CHECK THESE SHIFT THE APPROPRIATE DIRECTION\n",
    "    mom7_df['char_r_tm14_tm7'] = mom7_df.groupby('asset')['char_r_tm7'].shift(-168)\n",
    "    mom30_14_df['char_r_tm30_tm14'] = mom30_14_df.groupby('asset')['char_r_tm30_tm14'].shift(-336)\n",
    "    mom60_df['char_r_tm90_tm30'] = mom60_df.groupby('asset')['char_r_tm90_tm30'].shift(-720)\n",
    "\n",
    "    # Form single momentum df\n",
    "    mom_df = mom1_df.drop('char_price_t', axis=1).copy()\n",
    "    for df in [mom1h_df, mom7_df, mom14_df, mom30_df, mom60_df, mom90_df, mom30_14_df]:\n",
    "        df = df.drop('char_price_t', axis=1)\n",
    "        mom_df = mom_df.merge(df, on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "\n",
    "    # subset to weekly freq for Sunday midnight\n",
    "    mom_df = subsetToWeeklyFreq(mom_df)\n",
    "\n",
    "    return mom_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formCmktCol(panel_df: pd.DataFrame, mom_df: pd.DataFrame, asset_universe_dict: Dict[str, List[str]]) -> pd.DataFrame:\n",
    "    # subset to relevant columns\n",
    "    rel_assets_dt_df = subsetByMonthToAssetUniverse(panel_df[['date', 'asset', 'char_mcap_t']], \n",
    "                                                    asset_universe_dict)\n",
    "    temp_df = mom_df[['date', 'asset', 'char_r_tm7']].copy()\n",
    "    temp_df = temp_df.merge(rel_assets_dt_df,\n",
    "                            on=['date', 'asset'], how='inner', validate='one_to_one')\n",
    "\n",
    "    # subset to relevant dates\n",
    "    temp_df = subsetToWeeklyFreq(temp_df)\n",
    "\n",
    "    # form cmkt-weighted average return by week\n",
    "    cmkt_df = temp_df.groupby('date').apply(lambda x: (x['char_r_tm7']*x['char_mcap_t']).sum() / x['char_mcap_t'].sum())\n",
    "    \n",
    "    # clean up\n",
    "   # TODO\n",
    "    # name it macro_cmkt_t\n",
    "\n",
    "    return cmkt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def formCumRetCols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds two new columns to the input DataFrame containing panel data at the asset-hour level.\n",
    "    The new columns are 'char_r_ath_t' and 'char_r_atl_t', representing the cumulative return since\n",
    "    each asset's historical all-time high price and all-time low price, respectively.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing panel data at the asset-hour level.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with the new columns added.\n",
    "    \"\"\"\n",
    "    # Form group mask\n",
    "    grouped = df.groupby('asset')\n",
    "\n",
    "    # Calculate the cumulative maximum for the 'char_price_t' column within each group\n",
    "    df['cummax_price'] = grouped['char_price_t'].cummax()\n",
    "\n",
    "    # Calculate the return since the all-time high price\n",
    "    df['char_r_ath_t'] = df['char_price_t'] / df['cummax_price'] - 1\n",
    "\n",
    "    # Calculate the cumulative minimum for the 'char_price_t' column within each group\n",
    "    df['cummin_price'] = grouped['char_price_t'].cummin()\n",
    "\n",
    "    # Calculate the return since the all-time low price\n",
    "    df['char_r_atl_t'] = df['char_price_t'] / df['cummin_price'] - 1\n",
    "\n",
    "    # Drop the temporary 'cummax_price' and 'cummin_price' columns\n",
    "    df.drop(columns=['cummax_price', 'cummin_price'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def formFinancialCols(panel_df: pd.DataFrame, \n",
    "                        mom_df: pd.DataFrame, cmkt_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\"\"\"\"\n",
    "    # subset to needed columns\n",
    "    fin_df = panel_df[['date', 'asset', 'char_price_t', 'char_mcap_t']].copy()\n",
    "\n",
    "    # merge on new data\n",
    "    fin_df = fin_df.merge(cmkt_df, on='date', how='left', validate='many_to_one') \n",
    "    # TODO THINK THRU HOW THIS CUTS OUT DATES\n",
    "    # -DO I MB WANT TO FORM MACRO_CMKT_T FOR THE WHOLE \n",
    "    fin_df = fin_df.merge(mom_df[['date', 'asset', 'char_r_tm1h']], on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "    \n",
    "    # form characteristics\n",
    "    fin_df = fin_df.rename(columns={'char_mcap_t': 'char_size_t'})\n",
    "    \n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_r_max_tm1', range_hours=24, func=np.max)\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_r_max_tm7', range_hours=168, func=np.max)\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_r_max_tm30', range_hours=720, func=np.max)\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm1', range_hours=24, func=np.std)\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm7', range_hours=168, func=np.std)\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm30', range_hours=720, func=np.std)\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_vol_tm90', range_hours=2160, func=np.std)\n",
    "\n",
    "    fin_df['char_tradable_t'] = (fin_df['date'] - fin_df.groupby('asset')['date'].transform('min')).dt.total_seconds() / 3600 / 24\n",
    "\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_var5_tm7', range_hours=168, func=lambda x: x.quantile(0.05))\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_var5_tm90', range_hours=2160, func=lambda x: x.quantile(0.05))\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_shortfall5_tm7', range_hours=168, func=lambda x: x[x < x.quantile(0.05)].mean())\n",
    "    fin_df = formNewColumnByAsset(fin_df, target_col='char_r_tm1h', new_col='char_shortfall5_tm90', range_hours=2160, func=lambda x: x[x < x.quantile(0.05)].mean())\n",
    "    \n",
    "    fin_df = formCumRetCols(fin_df)\n",
    "\n",
    "    \n",
    "    # drop the cols from cmkt and 'char_r_tm1h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINANCIAL\n",
    "\n",
    "# TODO EDIT TO PASS WINDOW SIZE TO fitAndPredict\n",
    "# TODO EDIT TO FORM STUFF WITHIN FIT AND PREDICT\n",
    "# TODO EDIT TO DROP APPRORPRIATE COLS\n",
    "\n",
    "# Define the rolling window size\n",
    "window_size = 168\n",
    "\n",
    "def fitAndPredict(group):\n",
    "    # Reset the index for each group\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    # Build LHS and RHS\n",
    "    X = group['macro_cmkt_t']\n",
    "    y = group['char_r_tm1h']\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Define a function to fit the model and predict within the rolling window\n",
    "    def rolling_predict(index):\n",
    "        if index < window_size - 1:\n",
    "            return np.nan\n",
    "        else:\n",
    "            start = index - window_size + 1\n",
    "            end = index + 1\n",
    "            model = sm.OLS(y.iloc[start:end], X.iloc[start:end], missing='drop').fit()\n",
    "            return model.predict(X.iloc[index, :].values.reshape(1, -1))[0]\n",
    "\n",
    "    group['predicted_hourly_returns'] = [rolling_predict(i) for i in range(len(group))]\n",
    "    return group\n",
    "\n",
    "panel_df = panel_df.groupby('asset').apply(fitAndPredict)\n",
    "\n",
    "# Calculate the residuals\n",
    "panel_df['residuals'] = panel_df['hourly_returns'] - panel_df['predicted_hourly_returns']\n",
    "\n",
    "# Calculate the rolling standard deviation of the residuals over the past 168 hours\n",
    "panel_df = formNewColumnByAsset(\n",
    "    panel_df,\n",
    "    'residuals',\n",
    "    'char_ivol_tm7',\n",
    "    168,\n",
    "    lambda x: x.std()\n",
    ")\n",
    "\n",
    "panel_df = panel_df.drop(['residuals', 'predicted_hourly_returns'], axis=1)\n",
    "\n",
    "# char_ivol_tm7 = standard deviation of residuals from regressing hourly returns on cmkt over past seven days\n",
    "# -DROP THIS IF NOT BETTER THAN tm30\n",
    "# char_ivol_tm30 = standard deviation of residuals from regressing hourly returns on cmkt over past thirty days\n",
    "# char_ivol_tm90 = standard deviation of residuals from regressing hourly returns on cmkt over past ninety days\n",
    "# -DROP THIS IF NOT BETTER THAN tm30\n",
    "\n",
    "# CAN ADJUST THE ABOVE TO GET THIS\n",
    "# char_alpha_tm30  = intercept from regressing asset excess hourly returns on market excess return over past thirty days\n",
    "# char_beta_tm30  = slope from regressing asset excess returns on market excess return over past thirty days\n",
    "# char_beta_downside_tm30 = slope from regressing negative asset excess returns on negative market excess return over past thirty days\n",
    "\n",
    "# char_coskew_tm30 = coef on excess market return squared term in bivariate regression of asset excess returns on this and market return over 30 day trailing period\n",
    "# char_iskew_tm30 = same regression, take the skewness of the residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MICROSTRUCTURE\n",
    "\n",
    "\n",
    "# def formMicrostructureCols(panel_df: pd.DataFrame, mom_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "# Subset to needed columns\n",
    "mic_df = panel_df[['date', 'asset', 'char_price_t', 'char_volume_t', 'char_trades_t',\n",
    "    'char_bidask_t', 'char_bid_depth_t', 'char_ask_depth_t', 'char_supply_circ_t']].copy()\n",
    "mic_df = mic_df.merge(mom_df[['date', 'asset', 'char_r_tm1h']], on=['date', 'asset'], how='left', validate='one_to_one')\n",
    "\n",
    "# add temporary columns\n",
    "mic_df['temp_volume_price_t'] = mic_df.char_volume_t * mic_df.char_price_t\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_r_tm1h', new_col='temp_r_1m1h_abs_avg_tm7', range_hours=168, func=lambda x: np.mean(np.abs(x)))\n",
    "\n",
    "# form characteristics\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_volume_t', new_col='char_volume_sum_tm1', range_hours=24, func=np.sum)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_volume_t', new_col='char_volume_sum_tm7', range_hours=168, func=np.sum)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_volume_t', new_col='char_volume_std_tm7', range_hours=168, func=np.std)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_trades_t', new_col='char_trades_sum_tm7', range_hours=168, func=np.sum)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_trades_t', new_col='char_trades_std_tm7', range_hours=168, func=np.std)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='temp_volume_price_t', new_col='char_volume_price_avg_tm7', range_hours=168, func=np.mean)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='temp_volume_price_t', new_col='char_volume_price_std_tm7', range_hours=168, func=np.std)\n",
    "mic_df['char_turnover_t'] = mic_df.char_volume_sum_tm7 / mic_df.char_supply_circ_t\n",
    "mic_df['char_illiq_tm7'] = mic_df.temp_r_1m1h_abs_avg_tm7 / (mic_df.char_volume_sum_tm7/168)\n",
    "\n",
    "# drop unneeded columns\n",
    "mic_df = mic_df.drop(['char_price_t', 'temp_volume_price_t', \n",
    "    'char_supply_circ_t', 'char_r_tm1h', 'temp_r_1m1h_abs_avg_tm7'], axis=1)\n",
    "\n",
    "# char_turnover_std_tm30 = std of residuals from regressing hourly turnover on a constant over last thirty days\n",
    "\n",
    "\n",
    "# FORM THESE IF USEFUL:\n",
    "# char_bidask_avg_tm7 = avg bid ask spread over last week\n",
    "# char_bidask_std_tm7 = std of bid ask spread over last week\n",
    "# char_bid_depth_avg_tm7 = avg bid depth over last week\n",
    "# char_bid_depth_std_tm7 = std bid depth over last week\n",
    "# char_ask_depth_avg_tm7 = avg ask depth over last week\n",
    "# char_ask_depth_std_tm7 = std ask depth over last week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONCHAIN\n",
    "\n",
    "# def formOnchainCols(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "# Subset to the needed columns\n",
    "oc_df = panel_df[['date', 'asset', 'char_network_growth_t',\n",
    "    'char_holders_distribution_total_t', 'char_active_addr_t', \n",
    "    'char_tx_volume_t',\n",
    "    'char_circulation_7d_t', 'char_circulation_30d_t', \n",
    "    'char_circulation_90d_t', 'char_circulation_365d_t', \n",
    "    'char_circulation_3y_t',  'char_dormant_circulation_365d_t', \n",
    "    'char_supply_circ_t', 'char_supply_max_t', \n",
    "    'char_age_mean_dollar_t', 'char_age_destroyed_t']].copy()\n",
    "\n",
    "# Rename columns\n",
    "oc_df = oc_df.rename(columns={'char_holders_distribution_total_t': 'char_addr_total_t',\n",
    "                            'char_active_addr_t': 'char_addr_active_t',\n",
    "                            'char_circulation_7d_t': 'char_circulation_tm7',\n",
    "                            'char_circulation_30d_t': 'char_circulation_tm30',\n",
    "                            'char_circulation_90d_t': 'char_circulation_tm90',\n",
    "                            'char_circulation_365d_t': 'char_circulation_tm365',\n",
    "                            'char_circulation_3y_t': 'char_circulation_tm3y',\n",
    "                            'char_dormant_circulation_365d_t': 'char_circulation_dormant_tm365'})\n",
    "\n",
    "\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_network_growth_t', new_col='char_addr_new_tm7', range_hours=168, func=np.sum)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_addr_active_t', new_col='char_addr_active_tm7', range_hours=168, func=np.sum)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_tx_volume_t', new_col='char_tx_volume_tm7', range_hours=168, func=np.sum)\n",
    "mic_df = formNewColumnByAsset(mic_df, target_col='char_age_destroyed_t', new_col='char_age_destroyed_tm7', range_hours=168, func=np.mean)\n",
    "\n",
    "\n",
    "# \"char_addr_new_log_delta_tm14_tm7\" = change from previous week to current week in LOG of char_add_new_tm24\n",
    "\n",
    "# TODO drop the 90 and 180 one if they dont seem to be good and correlated with other stuff\n",
    "'char_dormant_circulation_180d_t',\n",
    "'char_dormant_circulation_90d_t'\n",
    "\n",
    "# TODO DROP IF NO ETTER THAN char_tx_volume_t\n",
    " 'char_tx_deposit_t',\n",
    " 'char_tx_withdraw_t',\n",
    "\n",
    "# TODO DROP UNLESS WAY USEFULL\n",
    "'char_circulation_1d_t',\n",
    "\n",
    "# TODO LEAVE AS IS BUT MAKE SURE NOT TOO CORRELATED WITH ANYTHING ELSE OTHERWISE CONSIDER DROPPING\n",
    "\n",
    " 'char_supply_circ_t',\n",
    " 'char_supply_max_t',\n",
    "\n",
    "'char_age_mean_dollar_t' # LEAVE AS IS IF USEFUL. OTHERWISE DROP.\n",
    "\n",
    "# Drop columns we do not need\n",
    "mic_df = mic_df.drop(columns=['char_network_growth_t', 'char_addr_active_t', 'char_age_destroyed_t'], axis=1)\n",
    "\n",
    "# return mic_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO FUNCTION FOR FORMING WEEKLY PANEL\n",
    "\n",
    "# def formWeeklyPanel(panel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "# Drop rows missing the LHS variable\n",
    "panel_df = panel_df[panel_df.r_ex_tp168.notnull()].reset_index(drop=True)\n",
    "\n",
    "# Rename weekly LHS to match naming convention for weekly panel\n",
    "panel_df = panel_df.rename(columns={'r_ex_tp168': 'r_ex_tp7'})\n",
    "\n",
    "# Form all RHS characteristics\n",
    "static_df = formStaticCols(panel_df)\n",
    "desc_stat_df = formDescStatCols(panel_df)\n",
    "# TODO CONFIRM THIS GIVES ACTUALLY SUNDAY MIDNIGHT AND NOT MONDAY MIDNIGHT\n",
    "mom_df = formMomentumCols(panel_df)\n",
    "cmkt_df = formCmktCol(panel_df, mom_df, asset_universe_dict)\n",
    "fin_df  = formFinancialCols(panel_df, mom_df, cmkt_df)\n",
    "mic_df = formMicrostructureCols(panel_df, mom_df)\n",
    "oc_df  = formOnchainCols(panel_df)\n",
    "# -FUNCTION TO FORM ONCHAIN CHARS\n",
    "# -FUNCTION TO FORM BALANCES CHARS\n",
    "# -FUNCTION TO FORM DEV CHARS\n",
    "# -FUNCTION TO FORM SOCIAL CHARS\n",
    "# -FUNCTION TO FORM VALUATION CHARS\n",
    "\n",
    "\n",
    "    # drop hourly ret\n",
    "    \n",
    "\n",
    "# Form RHS macro covariates\n",
    "\n",
    "# -FUNCTION TO FINISH PROCESSING ALL MACRO COLUMNS TO GIVE RAW STUFF FOR DESC STAT BY JUST TAKING AVG/SUMS\n",
    "# -split out the macro variables to merge back on so i work at timeseries level and not panel?\n",
    "# -form snp momentum\n",
    "# -form snp volatility\n",
    "# -form squared market return\n",
    "# -Summarize macro columns as there are just too damn many; use Goyal 8 as guide\n",
    "\n",
    "# TODO scope distribution of eahc column to confirm they look OK\n",
    "# -OVERALL, OVER TIME, BY ASSET, AND OVER TIME ASSET\n",
    "\n",
    "# Merge and clean the finaly weekly panel\n",
    "# def finalClean()\n",
    "# weekly_df = .merge()\n",
    "# -merge panel and macro back together\n",
    "# -cut down to study period\n",
    "# -ensure all dates are present\n",
    "# -report out assets that don't ahve consecutive days to eyeball when they enter and leave\n",
    "# -ensure no missing\n",
    "# -ensure all cols have appropriate range\n",
    "# -ensure rows and cols sorted\n",
    "\n",
    "# return weekly_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO EDIT THE WEEKLY TEMPLATE BELOW TO DO HOURLY; EDIT IN LIGHT OF WHAT I ACTUALLY DID FOR WEEKLY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO FUNCTION TO FORM HOURLY PANEL FOLLOWING TEMPLATE OF WEEKLY\n",
    "\n",
    "# -FUNCTION TO KEEP STATIC CHARS\n",
    "# -FUNCTION TO KEEP DESC STAT COLS\n",
    "# -FUNCTION TO FORM ALL MOMENTUMS\n",
    "# -FUNC TO FORM CMKT RETURN\n",
    "# -FUNCTION TO FORM FINANCIAL CHARS\n",
    "# -FUNCTION TO FORM MICROSTRUCTURE CHARS\n",
    "# -FUNCTION TO FORM ONCHAIN CHARS\n",
    "# -FUNCTION TO FORM BALANCES CHARS\n",
    "# -FUNCTION TO FORM DEV CHARS\n",
    "# -FUNCTION TO FORM SOCIAL CHARS\n",
    "# -FUNCTION TO FORM VALUATION CHARS\n",
    "# -FUNCTION TO FORM ALL GU 2019 CHARS\n",
    "# -FUNCTION TO FINISH PROCESSING ALL MACRO COLUMNS TO GIVE RAW STUFF FOR DESC STAT BY JUST TAKING AVG/SUMS\n",
    "\n",
    "# scope distribution of eahc column to confirm they look OK\n",
    "# -OVERALL, OVER TIME, BY ASSET, AND OVER TIME ASSET\n",
    "\n",
    "# NORMALIZE COLS\n",
    "# do transformations of characteirstics to cross sectional -1 to 1\n",
    "# for macro, do transformation to make it stationary and take whatever form of it gives stationary\n",
    "# -follow Pelger on this for the options and then come up with metric for most stationary and above some threshold\n",
    "# -do it programatically\n",
    "# to normalize macro, shift to -1 to 1 from end of validation and back; then for all test do it recursively for each new ob\n",
    "# -make sure this doesn't break correlation that much\n",
    "\n",
    "\n",
    "# def finalClean()\n",
    "# -merge panel and macro back together\n",
    "# -cut down to study period\n",
    "# -ensure all dates are present\n",
    "# -report out assets that don't ahve consecutive days to eyeball when they enter and leave\n",
    "# -ensure no missing\n",
    "# -ensure all cols have appropriate range\n",
    "# -ensure rows and cols sorted\n",
    "\n",
    "# ensure all columns are -1 to 1?\n",
    "\n",
    "# save train_val_df and test_df separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# REMINDER: LEAVE TEMPLATED CODE AS IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO LEAVE ALL OF THESE AS IS; I CAN JUST TAKE THE SUNDAY MIDNIGHT VALUE\n",
    "\n",
    "['char_industry_asset_mgmt',\n",
    "'char_industry_cex',\n",
    "'char_industry_cloud_compute',\n",
    "'char_industry_currency',\n",
    "'char_industry_data_mgmt',\n",
    "'char_industry_dex',\n",
    "'char_industry_gaming',\n",
    "'char_industry_infra',\n",
    "'char_industry_interop',\n",
    "'char_industry_lending',\n",
    "'char_industry_media',\n",
    "'char_industry_other_defi',\n",
    "'char_industry_smart_contract',\n",
    "'char_pow',\n",
    "'char_pos',\n",
    "'char_asset_usage_access',\n",
    "'char_asset_usage_discount',\n",
    "'char_asset_usage_dividends',\n",
    "'char_asset_usage_payments',\n",
    "'char_asset_usage_vote',\n",
    "'char_asset_usage_work',\n",
    "'char_ico_price',\n",
    "'char_ico']\n",
    "\n",
    "['char_ico_days_since_t',\n",
    "'char_rank_cmc_t',\n",
    "'char_vc_t',\n",
    "'char_num_pairs_t',\n",
    "'char_price_global_t',\n",
    "'char_volume_24h_global_t']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MOMENTUMS\n",
    "\n",
    "# char_r_tm1\n",
    "# char_r_tm7\n",
    "# char_r_tm14\n",
    "# char_r_tm30\n",
    "# char_r_tm60\n",
    "# char_r_tm90\n",
    "# char_r_tm14_tm7\n",
    "# char_r_tm30_tm14\n",
    "# char_r_tm90_tm30\n",
    "\n",
    "'char_price_t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINANCIAL\n",
    "\n",
    "# create hourly return and create line to drop at end\n",
    "# 'char_r_tm1h'\n",
    "\n",
    "# char_r_max_tm1 = max hourly return over past day\n",
    "# char_r_max_tm7 = max hourly return over past week\n",
    "# char_r_max_tm30 = max hourly return over past thirty days\n",
    "\n",
    "# char_price_t = price\n",
    "# char_price_log_max_tm1 = log of maximum price in past day \n",
    "# char_price_log_max_tm7 = log of maximum price in past seven days\n",
    "\n",
    "# char_size_t = market cap\n",
    "\n",
    "# char_vol_tm1 = standard deviation of hourly returns in past day\n",
    "# char_vol_tm7 = standard deviation of hourly returns in past seven days\n",
    "# char_vol_tm30 = standard deviation of hourly returns in past thirty days\n",
    "# char_vol_tm90 = standard deviation of hourly returns in past ninety days\n",
    "\n",
    "# r_ath_t = Cumulative return since ath \n",
    "# r_atl_t = Cumulative return since atl \n",
    "\n",
    "# char_var5_tm7 = 5th percentile of past seven days of hourly returns\n",
    "# char_var5_tm90 = 5th percentile of past 90 days of hourly returns\n",
    "# char_shortfall5_tm7 = avg hourly ret below 5th percentile over past seven days\n",
    "# char_shortfall5_tm90 = avg hourly ret below 5th percentile over past ninty days\n",
    "\n",
    "# char_ivol_tm7 = standard deviation of residuals from regressing hourly returns on cmkt over past seven days\n",
    "# -DROP THIS IF NOT BETTER THAN tm30\n",
    "# char_ivol_tm30 = standard deviation of residuals from regressing hourly returns on cmkt over past thirty days\n",
    "# char_ivol_tm90 = standard deviation of residuals from regressing hourly returns on cmkt over past ninety days\n",
    "# -DROP THIS IF NOT BETTER THAN tm30\n",
    "# char_alpha_tm30  = intercept from regressing asset excess hourly returns on market excess return over past thirty days\n",
    "# char_beta_tm30  = slope from regressing asset excess returns on market excess return over past thirty days\n",
    "# char_beta_downside_tm30 = slope from regressing negative asset excess returns on negative market excess return over past thirty days\n",
    "\n",
    "# char_coskew_tm30 = coef on excess market return squared term in bivariate regression of asset excess returns on this and market return over 30 day trailing period\n",
    "# char_iskew_tm30 = same regression, take the skewness of the residuals\n",
    "\n",
    "# char_tradable_t = number of days since first tradable\n",
    "\n",
    "['char_price_t',\n",
    "'char_mcap_t',\n",
    "'char_r_tm1h']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MICROSTRUCTURE\n",
    "\n",
    "# char_volume_t = dollar volume over past week\n",
    "# char_volume_log_avg_tm24 = average log dollar volume over past day\n",
    "# char_volume_log_avg_tm24 = std of log dollar volume over past day\n",
    "# char_trades_t = number of trades over past week\n",
    "# char_volume_price_log_avg_tm7 = average of log of hourly dollar volume times price over past seven days\n",
    "# char_volume_price_log_std_tm7 = std of log of price times volume over past seven day at hourly freq\n",
    "\n",
    "# char_turnover_t = trading volume over past week divided by circulating supply\n",
    "\n",
    "# char_turnover_std_tm30 = std of residuals from regressing hourly turnover on a constant over last thirty days\n",
    "\n",
    "# char_illiq_tm7 = avg abs value of hourly returns divided by avg hourly trading volume over past week\n",
    "\n",
    "['char_volume_t',\n",
    "'char_trades_t',\n",
    "'char_bidask_t',\n",
    "'char_bid_depth_t',\n",
    "'char_ask_depth_t']\n",
    "\n",
    "# THESE AS IS\n",
    "# char_bidask_t = bid ask spread\n",
    "# char_bid_depth_t = depth of first bid\n",
    "# char_ask_depth_t = depth of first ask\n",
    "\n",
    "# FORM THESE IF USEFUL:\n",
    "# char_bidask_avg_tm7 = avg bid ask spread over last week\n",
    "# char_bidask_std_tm7 = std of bid ask spread over last week\n",
    "# char_bid_depth_avg_tm7 = avg bid depth over last week\n",
    "# char_bid_depth_std_tm7 = std bid depth over last week\n",
    "# char_ask_depth_avg_tm7 = avg ask depth over last week\n",
    "# char_ask_depth_std_tm7 = std ask depth over last week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONCHAIN\n",
    "\n",
    "# \"char_addr_new_tm7\" = char_network_growth_t = number of new addresses that transfer token in last week\n",
    "# \"char_addr_new_log_delta_tm14_tm7\" = change from previous week to current week in LOG of char_add_new_tm24\n",
    "# char_addr_total_t =  'char_holders_distribution_total_t',\n",
    "# \"char_addr_active_t\" = 'char_active_addr_t' = number of unique addresses active over last hour\n",
    "# \"char_addr_active_tm7\" = sum of char_addr_active_t over last 7 days\n",
    "# \"char_tx_volume_t\" = sum of transaction volume in usd  'char_tx_volume_t' over past seven days\n",
    "\n",
    "# char_circulation_tm7 = number of unique tokens transfered over last 7 days 'char_circulation_7d_t',\n",
    "# char_circulation_tm30 = number of unique tokens transfered over last 30 days 'char_circulation_30d_t',\n",
    "# char_circulation_tm90 = number of unique tokens transfered over last 90 days 'char_circulation_90d_t',\n",
    "# char_circulation_tm365 = number of unique tokens transfered over last one year 'char_circulation_365d_t',\n",
    "# char_circulation_tm3y = number of unique tokens transfered over last three years 'char_circulation_3y_t'\n",
    "\n",
    "# TODO drop the 2 year one\n",
    "\n",
    "# char_circulation_dormant_tm365 = number of tokens transfered in last day that havent moved for over 365 days\n",
    " 'char_dormant_circulation_365d_t',\n",
    "\n",
    "# TODO drop the 90 and 180 one if they dont seem to be good and correlated with other stuff\n",
    "'char_dormant_circulation_180d_t',\n",
    "'char_dormant_circulation_90d_t'\n",
    "\n",
    "# TODO DROP IF NO ETTER THAN char_tx_volume_t\n",
    " 'char_tx_deposit_t',\n",
    " 'char_tx_withdraw_t',\n",
    "\n",
    "# TODO DROP UNLESS WAY USEFULL\n",
    "'char_circulation_1d_t',\n",
    "\n",
    "# TODO LEAVE AS IS BUT MAKE SURE NOT TOO CORRELATED WITH ANYTHING ELSE OTHERWISE CONSIDER DROPPING\n",
    "\n",
    " 'char_supply_circ_t',\n",
    " 'char_supply_max_t',\n",
    "\n",
    "'char_age_mean_dollar_t' # LEAVE AS IS IF USEFUL. OTHERWISE DROP.\n",
    "\n",
    "'char_age_destroyed_t' # TAKE MEAN OVER LAST WEEK to form 'char_age_destroyed_tm7'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BALANCES\n",
    "\n",
    "# TODO form exchange inflow and outflow in usd\n",
    "# confirm useful predictor\n",
    "# confirm not correlated iwth other stuff at super high degree where that other stuff i def keep\n",
    "# form optimal avg over past hour, day, or week for both\n",
    "\n",
    " 'char_exchange_inflow_usd_t',\n",
    " 'char_exchange_outflow_usd_t',\n",
    "\n",
    "# DROP THESE CONFIRMING THEY DONT OFFER MUCH BEHIND USD VERSIONS\n",
    "\n",
    " 'char_exchange_inflow_t',\n",
    " 'char_exchange_outflow_t',\n",
    "\n",
    "\n",
    "# FORM ALL THE BELOW BY DIVIDING BY CIRC SUPPLY \n",
    "# TODO KEEP IF AT ALL USEFUL AND NOT TOTALLY CORR WITH SOMETHING I DEF KEEP\n",
    "\n",
    "# 'char_traders_balance_t',\n",
    "# 'char_whale_balance_t',\n",
    "# 'char_exchange_balance_t',\n",
    "# 'char_dex_balance_t',\n",
    "# 'char_cex_balance_t',\n",
    "# 'char_amount_in_top_holders_t',\n",
    "# 'char_defi_balance_t',\n",
    "\n",
    "# TODO DROP ALL OF THE BELOW IF DONT ADD MUCH BEYOND WHAT I FORM ABOVE\n",
    "\n",
    "['char_supply_on_exchanges_t',\n",
    "'char_supply_outside_exchanges_t',\n",
    "\n",
    "'char_defi_cex_balance_t',\n",
    "'char_defi_dex_balance_t',\n",
    "'char_defi_exchange_balance_t',\n",
    "'char_dex_cex_balance_t',\n",
    "\n",
    "'char_traders_cex_balance_t',\n",
    "'char_traders_defi_balance_t',\n",
    "'char_traders_dex_balance_t',\n",
    "'char_traders_exchange_balance_t',\n",
    "\n",
    "'char_traders_whale_balance_t',\n",
    "\n",
    "\n",
    "'char_whale_cex_balance_t',\n",
    "'char_whale_defi_balance_t',\n",
    "'char_whale_dex_balance_t',\n",
    "\n",
    "'char_percent_of_total_supply_on_exchanges_t',\n",
    "]\n",
    "\n",
    "# TODO CREATE SOME AGG CHANGE IN HOURLY CHANGE OVER PAST WEEK\n",
    "# -vector diff from previous hour values and then avg that?\n",
    "# -play to get tasty that is CONFIRM ALSO MY METRIC IS USEFUL ON ITS OWN AND NOT TOTALLY CORR WITH SOMETHING ELSE \n",
    "'char_holders_distribution_over_100_t',\n",
    " 'char_holders_distribution_over_100k_t',\n",
    " 'char_holders_distribution_over_10_t',\n",
    " 'char_holders_distribution_over_10k_t',\n",
    " 'char_holders_distribution_over_1M_t',\n",
    " 'char_holders_distribution_over_1_t',\n",
    " 'char_holders_distribution_over_1k_t',\n",
    "\n",
    "# TODO CONFIRM MORE USEFUL THAN ANY OF THES EON THEIR OWN OR THEIR FIRST DIFFS OR MEAN OF FIRST DIFFS\n",
    "\n",
    "\n",
    "# TODO REPEAT ALL OF THE ABOVE FOR THE BELOW COLUMSN TO GET A FEEL OF FLOW\n",
    "['char_cexes_to_defi_flow_t',\n",
    " 'char_cexes_to_dex_flow_t',\n",
    " 'char_cexes_to_dex_traders_flow_t',\n",
    " 'char_cexes_to_traders_flow_t',\n",
    " 'char_cexes_to_whale_flow_t',\n",
    " 'char_defi_to_cexes_flow_t',\n",
    " 'char_defi_to_dex_traders_flow_t',\n",
    " 'char_defi_to_dexes_flow_t',\n",
    " 'char_defi_to_exchanges_flow_t',\n",
    " 'char_defi_to_traders_flow_t',\n",
    " 'char_defi_to_whale_flow_t',\n",
    " 'char_dex_to_cexes_flow_t',\n",
    " 'char_dex_traders_to_cexes_flow_t',\n",
    " 'char_dex_traders_to_defi_flow_t',\n",
    " 'char_dex_traders_to_dexes_flow_t',\n",
    " 'char_dex_traders_to_exchanges_flow_t',\n",
    " 'char_dex_traders_to_whale_flow_t',\n",
    " 'char_dexes_to_defi_flow_t',\n",
    " 'char_dexes_to_dex_traders_flow_t',\n",
    " 'char_dexes_to_traders_flow_t',\n",
    " 'char_dexes_to_whale_flow_t',\n",
    " 'char_exchanges_to_defi_flow_t',\n",
    " 'char_exchanges_to_dex_traders_flow_t',\n",
    " 'char_exchanges_to_genesis_flow_t',\n",
    " 'char_exchanges_to_traders_flow_t',\n",
    " 'char_exchanges_to_whales_flow_t',\n",
    " 'char_traders_to_cexes_flow_t',\n",
    " 'char_traders_to_defi_flow_t',\n",
    " 'char_traders_to_dexes_flow_t',\n",
    " 'char_traders_to_exchanges_flow_t',\n",
    " 'char_traders_to_whale_flow_t',\n",
    " 'char_whale_to_cexes_flow_t',\n",
    " 'char_whale_to_defi_flow_t',\n",
    " 'char_whale_to_dex_traders_flow_t',\n",
    " 'char_whale_to_dexes_flow_t',\n",
    " 'char_whale_to_traders_flow_t',\n",
    " 'char_whales_to_exchanges_flow_t',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV\n",
    "\n",
    "# AS IS OR MAYBE SOME AVG OVER PAST WEEK\n",
    "['char_dev_activity_t']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SOCIAL\n",
    "\n",
    "# CONFIRM BALANCE DOESNT OFFER MUCH BEFORE THE NEG AND POS ONES\n",
    "# THEN JUST KEEP ALL\n",
    "['char_sentiment_balance_reddit_t',\n",
    "'char_sentiment_balance_twitter_t',\n",
    "'char_sentiment_negative_reddit_t',\n",
    "'char_sentiment_negative_twitter_t',\n",
    "'char_sentiment_positive_reddit_t',\n",
    "'char_sentiment_positive_twitter_t',\n",
    "\n",
    "'char_sentiment_volume_consumed_total_t', # TOTAL OVER LAST WEK\n",
    "\n",
    "'char_social_dominance_total_t',\n",
    "\n",
    "'char_social_volume_reddit_t', # TOTAL OVER LAST WEK\n",
    "'char_social_volume_twitter_t', # TOTAL OVER LAST WEK\n",
    "\n",
    "'char_unique_social_volume_total_1h_t'] # TOTAL OVER LAST WEEK IF BETTER THAN RAW TOTALS\n",
    "\n",
    "\n",
    "# VALUATION\n",
    "\n",
    "# TODO JUST USE AS IS IF USEFUL AND NOT TOTALLY CORRELATED WITH OTHER STUFF\n",
    "\n",
    "'char_stock_to_flow_t',\n",
    "'char_percent_of_total_supply_in_profit_t'\n",
    "\n",
    "# TODO FIGURE OUT WHICH ONE TO USE OR BOTH\n",
    "'char_mvrv_long_short_diff_usd_t',\n",
    "'char_mvrv_usd_t',\n",
    "\n",
    "# TODO FIGURE OUT WHICH ONE TO USE\n",
    "'char_mean_realized_price_usd_t',\n",
    "'char_realized_value_usd_t',\n",
    "\n",
    "# TODO FIGURE OUT WHICH ONE TO USE\n",
    "'char_nvt_t',\n",
    "'char_nvt_transaction_volume_t',\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TASKS FOR LATER\n",
    "# -RUN UNI FACTOR IMPORTANCE FOR BOTH WEEKLY AND HOURLY\n",
    "# --WEEKLY IS WHAT IS IN PAPER\n",
    "# --HOURLY INFORMS HOW TO BUILD AUTOENCODER / DL MODEL\n",
    "# -WE WILL FIT AUTOENCODER AND MY STUFF WITH THE RAW CHARACTERISTICS AND MACRO TO START TO SHOW MY TRANSFORMER CAN LEARN TEMPLORAL\n",
    "# --BUT THEN GIVE THEM THE BEST STUFF FROM PRIOR TO SHOW AUTO STILL IS CRUSHED\n",
    "# -when he imports for uni factor: cut down to asset univser, drop macro, and drop global_price\n",
    "# -when for low-dim: drop global_price, normalize characteristics, just drop macro, or drop all for just returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR HOURLY, \n",
    "# -maybe just take stuff mostly as is\n",
    "# -but maybe try to form most stationary version for all?\n",
    "# -like keeps one that i know i just want that verison of\n",
    "# -then write a func to loop over the rest to form all the pelger transformations to then take the most stationary one\n",
    "# --as long as it is above some level of stationary?\n",
    "\n",
    " 'macro_aaa_t',\n",
    " 'macro_aave_med_borrow_apy_t',\n",
    " 'macro_aave_med_supply_apy_t',\n",
    " 'macro_aave_med_variable_borrow_apy_t',\n",
    " 'macro_acogno_t',\n",
    " 'macro_active_cryptos_t',\n",
    " 'macro_active_exchanges_t',\n",
    " 'macro_active_market_pairs_t',\n",
    " 'macro_amdmnox_t',\n",
    " 'macro_andenox_t',\n",
    " 'macro_avg_fee_mean_usd_t',\n",
    " 'macro_avg_fee_med_usd_t',\n",
    " 'macro_avg_fee_rev_pct_t',\n",
    " 'macro_avg_ndf_t',\n",
    " 'macro_avg_nvt_adj_ff_t',\n",
    " 'macro_avg_rvt_adj_t',\n",
    " 'macro_avg_ser_t',\n",
    " 'macro_avg_sopr_t',\n",
    " 'macro_avg_sply_act_pct_1yr_t',\n",
    " 'macro_avg_vel_act_1yr_t',\n",
    " 'macro_avg_vel_cur_1yr_t',\n",
    " 'macro_awhman_t',\n",
    " 'macro_awotman_t',\n",
    " 'macro_baa_t',\n",
    " 'macro_bogmbase_t',\n",
    " 'macro_btc_adr_act_cnt_t',\n",
    " 'macro_btc_adr_bal_cnt_t',\n",
    " 'macro_btc_cap_act_1yr_usd_t',\n",
    " 'macro_btc_cap_mrkt_ffusd_t',\n",
    " 'macro_btc_cap_real_usd_t',\n",
    " 'macro_btc_diff_mean_t',\n",
    " 'macro_btc_fee_med_usd_t',\n",
    " 'macro_btc_fee_tot_usd_t',\n",
    " 'macro_btc_flow_in_ex_usd_t',\n",
    " 'macro_btc_flow_miner_net_0hop_all_usd_t',\n",
    " 'macro_btc_flow_out_ex_usd_t',\n",
    " 'macro_btc_hash_rate_t',\n",
    " 'macro_btc_iss_tot_usd_t',\n",
    " 'macro_btc_mcrc_t',\n",
    " 'macro_btc_mctc_t',\n",
    " 'macro_btc_momr_t',\n",
    " 'macro_btc_mri_0hop_all30d_t',\n",
    " 'macro_btc_mvrv_t',\n",
    " 'macro_btc_ndf_t',\n",
    " 'macro_btc_nvt_adj_ff_t',\n",
    " 'macro_btc_puell_mul_rev_t',\n",
    " 'macro_btc_rev_hash_usd_t',\n",
    " 'macro_btc_rev_usd_t',\n",
    " 'macro_btc_rvt_adj_t',\n",
    " 'macro_btc_ser_t',\n",
    " 'macro_btc_sopr_t',\n",
    " 'macro_btc_sply_act_10yr_t',\n",
    " 'macro_btc_sply_act_180d_t',\n",
    " 'macro_btc_sply_act_1d_t',\n",
    " 'macro_btc_sply_act_1yr_t',\n",
    " 'macro_btc_sply_act_30d_t',\n",
    " 'macro_btc_sply_act_5yr_t',\n",
    " 'macro_btc_sply_act_7d_t',\n",
    " 'macro_btc_sply_act_ever_t',\n",
    " 'macro_btc_sply_act_pct_1yr_t',\n",
    " 'macro_btc_sply_adr_bal_usd_100_t',\n",
    " 'macro_btc_sply_adr_bal_usd_10k_t',\n",
    " 'macro_btc_sply_adr_bal_usd_1_t',\n",
    " 'macro_btc_sply_adr_bal_usd_1m_t',\n",
    " 'macro_btc_sply_adr_top_100_t',\n",
    " 'macro_btc_sply_adr_top_1pct_t',\n",
    " 'macro_btc_sply_cur_t',\n",
    " 'macro_btc_sply_ex_usd_t',\n",
    " 'macro_btc_sply_exp_fut_10yr_t',\n",
    " 'macro_btc_sply_ff_t',\n",
    " 'macro_btc_sply_miner_0hop_all_usd_t',\n",
    " 'macro_btc_sply_rvv_180d_t',\n",
    " 'macro_btc_sply_rvv_1yr_t',\n",
    " 'macro_btc_sply_rvv_30d_t',\n",
    " 'macro_btc_sply_rvv_5yr_t',\n",
    " 'macro_btc_sply_rvv_7d_t',\n",
    " 'macro_btc_sply_utxo_loss_t',\n",
    " 'macro_btc_sply_utxo_prof_t',\n",
    " 'macro_btc_tx_tfr_cnt_t',\n",
    " 'macro_btc_tx_tfr_val_adj_usd_t',\n",
    " 'macro_btc_tx_tfr_val_day_dst_t',\n",
    " 'macro_btc_tx_tfr_val_med_usd_t',\n",
    " 'macro_btc_tx_tfr_val_usd_t',\n",
    " 'macro_btc_utxo_age_med_t',\n",
    " 'macro_btc_utxo_loss_unreal_usd_t',\n",
    " 'macro_btc_utxo_prof_unreal_usd_t',\n",
    " 'macro_btc_vel_act_1yr_t',\n",
    " 'macro_btc_vel_cur_1yr_t',\n",
    " 'macro_businvx_t',\n",
    " 'macro_busloans_t',\n",
    " 'macro_ce16ov_t',\n",
    " 'macro_ces06_t',\n",
    " 'macro_ces20_t',\n",
    " 'macro_ces30_t',\n",
    " 'macro_claimsx_t',\n",
    " 'macro_clf16ov_t',\n",
    " 'macro_cmrmtsplx_t',\n",
    " 'macro_compapffx_t',\n",
    " 'macro_conspi_t',\n",
    " 'macro_cp3mx_t',\n",
    " 'macro_cpiaucsl_t',\n",
    " 'macro_cusr0000sac_t',\n",
    " 'macro_cusr0000sad_t',\n",
    " 'macro_cusr0000sas_t',\n",
    " 'macro_ddurrg3m086sbea_t',\n",
    " 'macro_dex_volume_t',\n",
    " 'macro_dgs1mo_t',\n",
    " 'macro_dndgrg3m086sbea_t',\n",
    " 'macro_dpcera3m086sbea_t',\n",
    " 'macro_dserrg3m086sbea_t',\n",
    " 'macro_dtcthfnm_t',\n",
    " 'macro_emv_inflation_t',\n",
    " 'macro_emv_t',\n",
    " 'macro_eth_adr_act_cnt_t',\n",
    " 'macro_eth_adr_act_cont_cnt_t',\n",
    " 'macro_eth_adr_act_rec_cnt_t',\n",
    " 'macro_eth_adr_act_sent_cnt_t',\n",
    " 'macro_eth_adr_bal_cnt_t',\n",
    " 'macro_eth_cap_act_1yr_usd_t',\n",
    " 'macro_eth_cap_mrkt_ffusd_t',\n",
    " 'macro_eth_cap_real_usd_t',\n",
    " 'macro_eth_cont_erc_20_cnt_t',\n",
    " 'macro_eth_fee_med_t',\n",
    " 'macro_eth_flow_in_ex_usd_t',\n",
    " 'macro_eth_flow_out_ex_usd_t',\n",
    " 'macro_eth_gas_used_tx_t',\n",
    " 'macro_eth_iss_tot_usd_t',\n",
    " 'macro_eth_mvrv_t',\n",
    " 'macro_eth_ndf_t',\n",
    " 'macro_eth_nvt_adj_ff_t',\n",
    " 'macro_eth_puell_mul_rev_t',\n",
    " 'macro_eth_rev_hash_usd_t',\n",
    " 'macro_eth_rev_usd_t',\n",
    " 'macro_eth_roi_t',\n",
    " 'macro_eth_rvt_adj_t',\n",
    " 'macro_eth_ser_t',\n",
    " 'macro_eth_sply_act_10yr_t',\n",
    " 'macro_eth_sply_act_180d_t',\n",
    " 'macro_eth_sply_act_1d_t',\n",
    " 'macro_eth_sply_act_1yr_t',\n",
    " 'macro_eth_sply_act_30d_t',\n",
    " 'macro_eth_sply_act_5yr_t',\n",
    " 'macro_eth_sply_act_7d_t',\n",
    " 'macro_eth_sply_act_ever_t',\n",
    " 'macro_eth_sply_act_pct_1yr_t',\n",
    " 'macro_eth_sply_adr_bal_usd_100_t',\n",
    " 'macro_eth_sply_adr_bal_usd_10k_t',\n",
    " 'macro_eth_sply_adr_bal_usd_1_t',\n",
    " 'macro_eth_sply_adr_bal_usd_1m_t',\n",
    " 'macro_eth_sply_adr_top_100_t',\n",
    " 'macro_eth_sply_adr_top_1pct_t',\n",
    " 'macro_eth_sply_burnt_usd_t',\n",
    " 'macro_eth_sply_cur_t',\n",
    " 'macro_eth_sply_ex_usd_t',\n",
    " 'macro_eth_sply_exp_fut_10yr_t',\n",
    " 'macro_eth_sply_ff_t',\n",
    " 'macro_eth_stakers_count_t',\n",
    " 'macro_eth_total_fee_t',\n",
    " 'macro_eth_tx_tfr_cnt_t',\n",
    " 'macro_eth_tx_tfr_val_adj_usd_t',\n",
    " 'macro_eth_tx_tfr_val_med_usd_t',\n",
    " 'macro_eth_tx_tfr_val_usd_t',\n",
    " 'macro_eth_vel_act_1yr_t',\n",
    " 'macro_eth_vel_cur_1yr_t',\n",
    " 'macro_ex_num_pairs_cex_t',\n",
    " 'macro_ex_num_pairs_dex_t',\n",
    " 'macro_ex_open_interest_future_usd_t',\n",
    " 'macro_ex_usd_volume_24h_cex_t',\n",
    " 'macro_ex_volume_future_usd_t',\n",
    " 'macro_ex_volume_t',\n",
    " 'macro_excausx_t',\n",
    " 'macro_exjpusx_t',\n",
    " 'macro_expinf10yr_t',\n",
    " 'macro_expinf1yr_t',\n",
    " 'macro_expinf20yr_t',\n",
    " 'macro_expinf2yr_t',\n",
    " 'macro_expinf30yr_t',\n",
    " 'macro_expinf3yr_t',\n",
    " 'macro_expinf5yr_t',\n",
    " 'macro_exszusx_t',\n",
    " 'macro_exusukx_t',\n",
    " 'macro_fedfunds_t',\n",
    " 'macro_funding_rate_med_usdt_binance_t',\n",
    " 'macro_gepu_t',\n",
    " 'macro_gs10_t',\n",
    " 'macro_gs1_t',\n",
    " 'macro_gs5_t',\n",
    " 'macro_houst_t',\n",
    " 'macro_hwiuratio_t',\n",
    " 'macro_ico_count_t',\n",
    " 'macro_indpro_t',\n",
    " 'macro_invest_t',\n",
    " 'macro_m1sl_t',\n",
    " 'macro_m2real_t',\n",
    " 'macro_m2sl_t',\n",
    " 'macro_manemp_t',\n",
    " 'macro_mcd_avg_liq_t',\n",
    " 'macro_mcd_med_collat_ratio_t',\n",
    " 'macro_mvrv_med_t',\n",
    " 'macro_nonrevsl_t',\n",
    " 'macro_oilpricex_t',\n",
    " 'macro_payems_t',\n",
    " 'macro_pcepi_t',\n",
    " 'macro_permit_t',\n",
    " 'macro_realln_t',\n",
    " 'macro_rpi_t',\n",
    " 'macro_snp500_t',\n",
    " 'macro_snp_div_yield_t',\n",
    " 'macro_snp_indust_t',\n",
    " 'macro_snp_pe_t',\n",
    " 'macro_stablecoin_dev_t',\n",
    " 'macro_t10yie_t',\n",
    " 'macro_t20yiem_t',\n",
    " 'macro_t30yiem_t',\n",
    " 'macro_t5yie_t',\n",
    " 'macro_tb3ms_t',\n",
    " 'macro_tb6ms_t',\n",
    " 'macro_teu_sca_t',\n",
    " 'macro_tmu_sca_t',\n",
    " 'macro_total_aave_borrowed_t',\n",
    " 'macro_total_aave_deposits_t',\n",
    " 'macro_total_aave_liq_t',\n",
    " 'macro_total_aave_new_debt_t',\n",
    " 'macro_total_aave_supply_t',\n",
    " 'macro_total_adr_act_cnt_t',\n",
    " 'macro_total_adr_bal_cnt_t',\n",
    " 'macro_total_adr_bal_usd_100_cnt_t',\n",
    " 'macro_total_adr_bal_usd_100k_cnt_t',\n",
    " 'macro_total_adr_bal_usd_10k_cnt_t',\n",
    " 'macro_total_adr_bal_usd_1m_cnt_t',\n",
    " 'macro_total_cap_act_1yr_usd_t',\n",
    " 'macro_total_cap_fut_exp_10yr_usd_t',\n",
    " 'macro_total_cap_mrkt_cur_usd_t',\n",
    " 'macro_total_cap_mrkt_ffusd_t',\n",
    " 'macro_total_cap_mvrv_cur_t',\n",
    " 'macro_total_cap_real_usd_t',\n",
    " 'macro_total_compound_borrowed_t',\n",
    " 'macro_total_compound_deposits_t',\n",
    " 'macro_total_compound_liq_t',\n",
    " 'macro_total_compound_new_debt_t',\n",
    " 'macro_total_compound_supply_t',\n",
    " 'macro_total_dai_created_t',\n",
    " 'macro_total_dai_repaid_t',\n",
    " 'macro_total_fee_tot_usd_t',\n",
    " 'macro_total_iss_tot_usd_t',\n",
    " 'macro_total_maker_borrowed_t',\n",
    " 'macro_total_maker_deposits_t',\n",
    " 'macro_total_maker_supply_t',\n",
    " 'macro_total_nft_retail_trades_t',\n",
    " 'macro_total_nft_retail_volume_t',\n",
    " 'macro_total_nft_trades_t',\n",
    " 'macro_total_nft_volume_t',\n",
    " 'macro_total_nft_whale_trades_t',\n",
    " 'macro_total_nft_whale_volume_t',\n",
    " 'macro_total_open_interest_usdt_binance_t',\n",
    " 'macro_total_open_value_usdt_binance_t',\n",
    " 'macro_total_rev_usd_t',\n",
    " 'macro_total_sply_act_10yr_t',\n",
    " 'macro_total_sply_act_180d_t',\n",
    " 'macro_total_sply_act_1d_t',\n",
    " 'macro_total_sply_act_1yr_t',\n",
    " 'macro_total_sply_act_30d_t',\n",
    " 'macro_total_sply_act_5yr_t',\n",
    " 'macro_total_sply_act_7d_t',\n",
    " 'macro_total_sply_act_ever_t',\n",
    " 'macro_total_sply_adr_bal_usd_100_t',\n",
    " 'macro_total_sply_adr_bal_usd_10k_t',\n",
    " 'macro_total_sply_adr_bal_usd_1_t',\n",
    " 'macro_total_sply_adr_bal_usd_1m_t',\n",
    " 'macro_total_sply_adr_top_100_t',\n",
    " 'macro_total_sply_adr_top_1pct_t',\n",
    " 'macro_total_sply_ff_t',\n",
    " 'macro_total_tx_tfr_cnt_t',\n",
    " 'macro_total_tx_tfr_val_adj_usd_t',\n",
    " 'macro_total_tx_tfr_val_usd_t',\n",
    " 'macro_total_uni_claims_t',\n",
    " 'macro_total_usd_mcap_t',\n",
    " 'macro_totresns_t',\n",
    " 'macro_twexafegsmthx_t',\n",
    " 'macro_uempmean_t',\n",
    " 'macro_umcsentx_t',\n",
    " 'macro_unrate_t',\n",
    " 'macro_us_ex_open_interest_future_usd_t',\n",
    " 'macro_us_ex_volume_future_usd_t',\n",
    " 'macro_us_ex_volume_spot_usd_t',\n",
    " 'macro_us_mpu_t',\n",
    " 'macro_uscons_t',\n",
    " 'macro_usfire_t',\n",
    " 'macro_usgood_t',\n",
    " 'macro_vixclsx_t'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
